{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Theory\n",
    "\n",
    "\n",
    "## Notations and Rules:  \n",
    " \n",
    "#### Notations  \n",
    " >$X,Y$, random variable, can be discrete or continuous.  \n",
    " >$x_i, y_i$, a specific value that X,Y take.  \n",
    " >**PDF**, probability density function, distribution for continuous random variable.  \n",
    " >**PMF**, probability mass function, distribution for discrete random variable.  \n",
    " >**CDF**, cumulative distribution function, the probability that $x$ lies in the interval $(-\\infty, x)$.  \n",
    " >$P(X)$, the CDF of random variable $X$. \n",
    " >$p(X)$, the PDF of random variable $X$.  \n",
    " >$p(x_i)=p(X=x_i)$, the probability that $X$ equals to $x_i$.   \n",
    " >$p(X, Y)$, joint PDF of random variable $X$ and $Y$.  \n",
    " >$p(X=x_i, Y=y_i)$, the probability of $X=x_i$ as well as $Y=y_i$.  \n",
    " >$p(X|Y=y_i)$, when $Y=y_i$, the PDF of $X$.  \n",
    " >$p(X=x_i|Y=y_i)$, when $Y=y_i$, the probability of $X=x_i$.  \n",
    " \n",
    " \n",
    "#### <font color='red'>The Rules of Probability</font>\n",
    " >sum rule $$\\displaystyle{p(X) = \\sum_Y p(X, Y)}$$\n",
    " >product rule $$\\displaystyle{p(X, Y)=p(Y|X)p(X)}$$\n",
    " \n",
    "#### <font color='red'>Bayes' theorem</font>\n",
    " $$\\displaystyle{p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}=\\frac{p(X|Y)p(Y)}{\\sum_Y p(X|Y)p(Y)}}$$\n",
    " >where $p(Y)$ is a **prior probability**, because it is the probability before we observe the identity of $X$.  \n",
    " >$p(Y|X)$ is a **posterior probability**, because it is the probability after we have observed $X$.\n",
    " \n",
    "#### Independent \n",
    " >If $X$ and $Y$ are independent  \n",
    " $$p(X, Y) = p(X)p(Y)$$\n",
    " \n",
    "#### PDF\n",
    " >For PDF $p(x)$\n",
    " $$\\begin{align*}&p(x\\in(a,b))=\\int_a^b p(x)dx\\\\\n",
    " &p(x) \\geq 0\\\\\n",
    " &p(x\\in(-\\infty, \\infty))=\\int_{-\\infty}^{\\infty} p(x)dx=1\\end{align*}$$\n",
    " \n",
    "#### PDF Transform\n",
    " >PDF transform  \n",
    " >If $x = g(y)$, then a function $f(x)$ becomes \\tilde{f}(y)=f(g(y)). Now consider a probability density $p_x(x)$ that corresponds to density $p_y(y)$ with respect to the new variable $y$, where the suffices denote the fact that $p_x(x)$ and $p_y(y)$ are different densities. Observations falling in the range $(x, x+\\Delta x)$ will, for small value of $\\Delta x$, be transformed into the range $(y, y+\\Delta y)$ where $p_x(x)\\Delta x \\approx p_y(y)\\Delta y$, and hence  \n",
    " $$\\displaystyle{p_y(y) = p_x(x)\\left|\\frac{\\Delta x}{\\Delta y}\\right|= p_x(x)\\left|\\frac{ dx}{dy}\\right|=p_x(x)|g'(y)|}$$\n",
    " \n",
    "#### CDF  \n",
    " $$\\displaystyle{P(z)=\\int_{-\\infty}^{z}p(x)dx}$$\n",
    " \n",
    "#### <font color='red'>The Rules of Probability for Continuous Random Variables</font>\n",
    " >sum rule $$\\displaystyle{p(x) = \\int p(x, y)dy}$$\n",
    " >product rule $$\\displaystyle{p(x, y)=p(y|x)p(x)}$$\n",
    " \n",
    "-----------\n",
    "## Expectations and Covariances\n",
    "\n",
    "\n",
    "### Expectations and Covariance of function\n",
    "\n",
    "#### Expectation of function  \n",
    ">One of the most important operations involving probabilities is that of finding weighted averages of functions. The **<font color='red'>average value</font>** of some function $f(x)$ under a probability distribution $p(x)$ is called the expectation of $f(x)$ and will be denoted by $\\mathbb{E}[f]$ *(this is a value)*.  \n",
    ">Discrete $$\\displaystyle{\\mathbb{E}[f]=\\sum_x p(x)f(x)}$$\n",
    ">Continuous $$\\displaystyle{\\mathbb{E}[f]=\\int p(x)f(x)dx}$$\n",
    "\n",
    ">Generally, we have finite number N of points drawn from the probability distribution.  \n",
    "$$\\displaystyle{\\mathbb{E}[f]\\approx\\frac{1}{N}\\sum_{n=1}^{N} f(x_n)}$$\n",
    "\n",
    ">For a 2-D function $f(x, y)$\n",
    "$$\\mathbb{E}_x[f(x,y)]$$\n",
    ">denotes the average of the function $f(x,y)$ with respect to the distribution of x. The $\\mathbb{E}_x[f(x,y)]$ will be a function of y.  \n",
    "\n",
    "\n",
    "#### Condictional expectation  \n",
    "$$\\displaystyle{\\mathbb{E}_x[f|y]=\\sum_xp(x|y)f(x)}$$\n",
    "\n",
    "#### Variance  \n",
    "$$var[f]=\\mathbb{E}\\big[(f(x)-\\mathbb{E}[f(x)])^2\\big]=\\mathbb{E}[f(x)^2]-\\mathbb{E}[f(x)]^2$$\n",
    "\n",
    "### Variances and Covariance of random variable\n",
    "$$\\begin{align*}var[x]&=\\mathbb{E}[x^2]-\\mathbb{E}[x]^2\\\\cov[x,y]&=\\mathbb{E}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}]=\\mathbb{E}_{x,y}[xy]-\\mathbb{E}[x]\\mathbb{E}[y]\\end{align*}$$\n",
    ">If $x$ and $y$ are independent, then their convariance vanishes.  \n",
    "\n",
    ">For vectors of random variables $\\mathbf{x},\\mathbf{y}$\n",
    "$$cov[\\mathbf{x},\\mathbf{y}]=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\{\\mathbf{x}-\\mathbb{E}[\\mathbf{x}]\\}\\{\\mathbf{y}^T-\\mathbb{E}[\\mathbf{y}^T]\\}]=\\mathbb{E}_{\\mathbf{x},\\mathbf{y}}[\\mathbf{x}\\mathbf{y}^T]-\\mathbb{E}[\\mathbf{x}]\\mathbb{E}[\\mathbf{y}]$$\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "## The Gaussian Distribution\n",
    "\n",
    "### Normal/Gaussian Distribution\n",
    "#### Definition\n",
    "$$\\displaystyle{\\mathcal{N}(x|\\mu,\\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}}$$\n",
    ">where $x$ is a single real-variable.  \n",
    ">$\\mu$ is mean of $x$.  \n",
    ">$\\sigma^2$ is variance of $x$.  \n",
    "\n",
    "#### Expectation and Variance\n",
    ">If there is a function $f(x) = x$, then the expectation of this function is  \n",
    "$$\\displaystyle{ \\mathbb{E}[x]=\\int_{-\\infty}^{\\infty}\\mathcal{N}(x|\\mu,\\sigma^2)xdx=\\mu }$$\n",
    ">This is the **Expectation** of random variable $x$\n",
    ">Similarly, for the second order moment  \n",
    "$$\\displaystyle{ \\mathbb{E}[x^2]=\\int_{-\\infty}^{\\infty}\\mathcal{N}(x|\\mu,\\sigma^2)x^2dx=\\mu^2+\\sigma^2 }$$\n",
    ">The **variance** of $x$ is  \n",
    "$$var[x]=\\mathbb{E}[x^2]-\\mathbb{E}[x]^2=\\sigma^2$$\n",
    "\n",
    "#### D-dim\n",
    ">Gaussian distribution of D-dimensional vector $\\mathbf{x}$\n",
    "$$\\displaystyle{\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu},\\mathbf{\\Sigma})=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\mathbf{\\Sigma}|^{1/2}}exp\\left\\{-\\frac{1}{2}(\\mathbf{x}-\\mathbf{\\mu})^T\\mathbf{\\Sigma}^{-1}(\\mathbf{x}-\\mathbf{\\mu})\\right\\}}$$\n",
    "\n",
    "### <font color='red'>Use Gaussian in Maximum Likelihood</font>\n",
    ">Consider a 1-D data set $\\mathbb{x}=\\{x_1,x_2,\\cdots,x_N\\}$, each element in this set is independent.  \n",
    ">Now we suppose the observasions of these data are drawn independently from a Gaussian distribution whose mean $\\mu$ and variance $\\sigma^2$ are unknown, and we would like to determine these parameters from the data set.  \n",
    ">Because each element is independent, the overall probability of the data set generated from the Gaussian distribution is  \n",
    "$$\\displaystyle{p(\\mathbb{x}|\\mu,\\sigma^2)=\\prod_{n=1}^N \\mathcal{N}(x_n|\\mu,\\sigma^2)}$$\n",
    ">This is the **<font color='red'>likelihood function</font>** for the Gaussian with two unknown parameters $\\mu$ and $\\sigma^2$. By arrangement of the parameters $\\mu$ and $\\sigma^2$, the likelihood (probability) shall be manimum, which means the best Gaussian distribution of generating these data.  \n",
    ">Anyway, we take the logarithm of each side of the equation for convinience.  \n",
    "$$\\displaystyle{\\ln p(\\mathbb{x}|\\mu,\\sigma^2)=-\\frac{1}{2\\sigma^2}\\sum_{n=1}^N(x_n-\\mu)^2-\\frac{N}{2}\\ln\\sigma^2-\\frac{N}{2}\\ln(2\\pi)}$$\n",
    ">For maximizing the likelihood with respect to $\\mu$, the solution is  \n",
    "$$\\mu_{ML}=\\frac{1}{N}\\sum_{n=1}^{N}x_n$$\n",
    ">which is the sample mean. Simmilarly, maximizing with respect to $\\sigma^2$, the solution is  \n",
    "$$\\sigma^2_{ML}=\\frac{1}{N}\\sum_{n=1}^N(x_n-\\mu_{ML})^2$$\n",
    ">which is the sample variance. We can also calibrate the sample variance  \n",
    "$$\\displaystyle{\\tilde{\\sigma}^2=\\frac{N}{N-1}\\sigma^2_{ML}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential estimation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Robbins-Monro Algorithm\n",
    "*<a href=\"https://en.wikipedia.org/wiki/Stochastic_approximation\">Robbins-Monro</a>* algorithm is a method to estimate the solution(root) of a function from ramdom variables <font color='red'>iteratively</font>.  \n",
    "Assume that we have a function $M(\\theta)$, and a constant $\\alpha$, such that the equation $M(\\theta)=\\alpha$ has a unique root at $\\theta^*$. It is assumed that while we cannot directly observe the function $M(\\theta)$, we can instead obtain measurements of the random variable $N(\\theta)$ where $\\mathbb{E}[N(\\theta)]=M(\\theta)$. The structure of the algorithm is to then generate iterates of the form:\n",
    "$$\\theta_{n+1}=\\theta_n - a_n(N(\\theta_n)-\\alpha)$$\n",
    "Here, $a_1,a_2,\\cdots$ is a sequence of positive step sizes. Robbins and Monro proved that $\\theta_n$ conveges in $L^2$ to $\\theta$, and the convergence is actually with probability one, provided that  \n",
    "- $N(\\theta)$ is uniformly bounded.\n",
    "- $M(\\theta)$ is nondecreasing. \n",
    "- $M'(\\theta)$ exists and is positive.\n",
    "- $a_n$ satisfies the following requirements\n",
    "$$\\begin{align*}\n",
    "\\lim_{N\\to\\infty}a_N &= 0\\\\\n",
    "\\sum_{N=1}^{\\infty}a_N&=\\infty\\\\\n",
    "\\sum_{N=1}^{\\infty}a_N^2&<\\infty\n",
    "\\end{align*}$$\n",
    "\n",
    "## Application of Robbins-Monro Algorithm\n",
    "\n",
    "For maximizing the log likelihood function, we derivate the function and let it equal to zero\n",
    "$$\\frac{\\partial}{\\partial \\theta}\\left .\\left\\{\\frac{1}{N}\\sum_{n=1}^N\\ln p(\\mathbf{x}_n|\\theta) \\right\\}\\right|_{\\theta_{ML}}=0$$\n",
    "Exchanging the derivative and the summation, and taking the limit $N\\to\\infty$ we have\n",
    "$$\\lim_{N\\to\\infty}\\frac{1}{N}\\sum_{n=1}^N\\ln p(\\mathbf{x}_n|\\theta)=\\mathbb{E}_x\\left[\\frac{\\partial}{\\partial\\theta}\\ln p(x|\\theta)\\right]$$\n",
    "For each single $x$, its hyperparameter $\\theta$ makes the expectation of $\\frac{\\partial}{\\partial\\theta}\\ln p(x|\\theta)$ to be zero.Compare the form here with that of Robbins-Monro algorithm, we have\n",
    "- $0$, the original result given the input $\\theta$, $M(\\theta)=\\alpha$.\n",
    "- $\\frac{\\partial}{\\partial\\theta}\\ln p(x|\\theta)$, the measurement of the random variable $N(\\theta)$.\n",
    "- $\\mu_{ML}$, the solution we need to figure out, $\\theta$.\n",
    "- $a_N$, the coefficient.\n",
    "\n",
    "Then the sequential form of $\\mu_{ML}$ is given by\n",
    "$$\\begin{align*}\n",
    "\\mu_{ML}^{(N)}&=\\mu_{ML}^{(N-1)}+a_{N-1}\\left(\\frac{\\partial}{\\partial\\theta}\\ln p(x_N|\\mu_{ML}^{(N-1)})-0\\right)\\\\\n",
    "&=\\mu_{ML}^{(N-1)}+a_{N-1}\\left(\\frac{1}{\\sigma^2}(x_N-\\mu_{ML}^{(N-1)})\\right)\\\\\n",
    "&=\\mu_{ML}^{(N-1)}+\\frac{1}{N}(x_N-\\mu_{ML}^{(N-1)})\\qquad let\\ a_N=\\frac{\\sigma^2}{N+1}\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general linear regression model takes the form\n",
    "\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x})$$\n",
    "\n",
    "With the training set, we then need to solve the following equation.\n",
    "\n",
    "$$\\underbrace{\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}}_{\\Phi}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "w_0\\\\ w_1\\\\ \\vdots\\\\ w_{M-1}\\\\\n",
    "\\end{bmatrix}}_{\\mathbf{w}}\n",
    "=\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "t_1\\\\ t_2\\\\ \\vdots\\\\ t_{N}\\\\\n",
    "\\end{bmatrix}}_{\\mathbf{t}}\n",
    "$$\n",
    "\n",
    "Although in most cases, the number of training data set denoted by $N$ is larger than the dimension of the input features denoted by $M$. Thus we can not find the exact solution of these equations. However, before looking into how to use the kernel method to solve these equations, there is a concept that we need to be clear.\n",
    "\n",
    "The vector $\\mathbf{w}$ can be partitioned into two vectors that lie in two complemented spaces.\n",
    "\n",
    "$$\\mathbf{w} = \\hat{\\mathbf{w}} + \\mathbf{z}\\qquad \\text{where } \\hat{\\mathbf{w}}\\in \\Phi_{row},\\ \\mathbf{z}\\in N(\\Phi)$$\n",
    "\n",
    "where $\\hat{\\mathbf{w}}$ is in the row space of $\\Phi$ and $\\mathbf{z}$ is in the null space of $\\Phi$ Then the equation becomes\n",
    "\n",
    "$$\\Phi\\mathbf{w} = \\Phi(\\hat{\\mathbf{w}} +\\mathbf{z}) = \\Phi\\hat{\\mathbf{w}} + 0 = \\Phi\\hat{\\mathbf{w}}$$\n",
    "\n",
    "Thus the best solution of $\\mathbf{w}$ is in the row space of $\\Phi$.\n",
    "\n",
    "$$\\mathbf{w} = \\hat{\\mathbf{w}}$$\n",
    "\n",
    "---------------------\n",
    "\n",
    "# Dual Representations\n",
    "\n",
    "Because the vector $\\mathbf{w}$ is in the row space of $\\Phi$, we can write it in the following form\n",
    "\n",
    "$$\\mathbf{w} = \\Phi^T \\mathbf{a} \\tag{6.3}$$\n",
    "\n",
    "By substituting $\\mathbf{w} = \\Phi^T \\mathbf{a}$ into the common error function, we can obtain a new error function with respect to $\\mathbf{a}$. Our goal then turns out to be finding the value of $\\mathbf{a}$ that minimize the error function.\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{Original Error Function:} &J(\\mathbf{w}) &=\\frac{1}{2}\\sum_{n=1}^N\\{\\mathbf{w}^T\\phi(\\mathbf{x}_n)-t_n\\}^2 + \\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w} \\tag{6.2}\\\\\n",
    "&\\text{New Error Function:} &J(\\mathbf{a}) &=\\frac{1}{2}\\mathbf{a}^T\\Phi\\Phi^T\\Phi\\Phi^T\\mathbf{a}-\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{1}{2}\\mathbf{a}^T\\Phi\\Phi^T\\mathbf{a} \\tag{6.5}\n",
    "\\end{align*}$$\n",
    "\n",
    "We now define the Gram matrix $\\mathbf{K} = \\Phi\\Phi^T$, which is an $N\\times N$ symmetric matrix with elements\n",
    "\n",
    "$$\\bbox[#ffe0f0]{K_{nm} = \\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) = k(\\mathbf{x}_n,\\mathbf{x}_m)} \\tag{6.6}$$\n",
    "\n",
    "which is called <font color='red'>*kernel function*</font>. Then the error function can be simplified to be\n",
    "\n",
    "$$J(\\mathbf{a}) = \\frac{1}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{K}\\mathbf{a}-\\mathbf{a}^T\\mathbf{K}\\mathbf{t}+\\frac{1}{2}\\mathbf{t}^T\\mathbf{t}+\\frac{\\lambda}{2}\\mathbf{a}^T\\mathbf{K}\\mathbf{a} \\tag{6.7}$$\n",
    "\n",
    "Setting the gradient of $J(\\mathbf{a})$ with respect to $\\mathbf{a}$ to zero, we obtain the following solution\n",
    "\n",
    "$$\\mathbf{a} = (\\mathbf{K}+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t} \\tag{6.8}$$\n",
    "\n",
    "If we substitute this back into the linear regresion model, we obtain the following prediction for a new input $\\mathbf{x}$\n",
    "\n",
    "$$\\bbox[#ffe0f0]{y(\\mathbf{x}) = \\mathbf{k}(\\mathbf{x})^T(\\mathbf{K}+\\lambda\\mathbf{I}_N)^{-1}\\mathbf{t}} \\tag{6.9}$$\n",
    "\n",
    "where we have defined the vector $\\mathbf{k}(\\mathbf{x})$ with elements $k_n(\\mathbf{x}) = k(\\mathbf{x}_n,\\mathbf{x})$.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Advantages\n",
    "\n",
    "1. Both in the modeling method and the kernel method, the computational effort are mainly cost in matrix inverting. Modeling method need to invert a $M\\times M$ matrix whereas kernel method is $N\\times N$. Thus, if $N$ is less than $M$, using the kernel method will achive more computational effciency.\n",
    "2. Because the dual formulation is expressed entirely in terms of the kernel function $k(\\mathbf{x},\\mathbf{x}')$, we can therefore work directly in terms of kernels and avoid the explicit introduction of the feature vector $\\phi(\\mathbf{x})$, which allows us implicitly to use feature spaces of high, even infinite, dimensionality.\n",
    "\n",
    "\n",
    "--------------------\n",
    "\n",
    "# About the remaining sections\n",
    "\n",
    "1. The kernel function is not only from the inner product of basis functions, we can also use construction the kernel directly. In the Section 6.2, we will talk about the techniques of constructing the kernel functions.\n",
    "2. In Section 6.3, we shall introduce a specific kernel.\n",
    "3. In Section 6.4, we shall look into the kernel method from the perspective of Gaussian process, which derives the expression of (6.9) from the probability theorem.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

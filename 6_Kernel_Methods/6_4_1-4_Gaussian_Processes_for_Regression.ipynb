{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression revisited\n",
    "\n",
    "From this section, we will learn how we go into the Gaussian processes from the linear regression model.\n",
    "\n",
    "1. <div style='background-color:#e0f0ff'>Here is a general linear regression model.\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}) \\tag{6.49}$$\n",
    "where $\\mathbf{w}$ is a $M$-dimensional weight vector and $\\phi(\\mathbf{x})$ is also a $M$-dimensional basis vector.</div>\n",
    "2. <div style='background-color:#f0e0ff'>Now we consider a prior distribution over $\\mathbf{w}$ that is given by an isotropic Gaussian of the form\n",
    "$$p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}I) \\tag{6.50}$$\n",
    "For specific $\\mathbf{x}$, $y(\\mathbf{x})$ is also a Gaussian distributed variable because it is the linear combination of Gaussian variables.</div>\n",
    "3. <div style='background-color:#fff0e0'>In practice, we are given a training data set at the training data points $\\mathbf{X} = (\\mathbf{x}_1,\\cdots,\\mathbf{x}_N)^T$. We are therefore interested in the joint distribution of the function values $\\mathbf{y} = (y_1,\\cdots, y_N)^T$, where $y_n = y(\\mathbf{x}_n)$. This vector is given by\n",
    "$$\\mathbf{y} = \\Phi \\mathbf{w}\\quad \\text{where}\\quad \\Phi = \\big(\\phi(\\mathbf{x}_1),\\cdots, \\phi(\\mathbf{x})_N\\big)^T\\tag{6.51}$$\n",
    "Here, $\\mathbf{y}$ is a vector whose elements are random variables at different points. This vector is derived by a joint probability density function denoted by $p\\big(y_1, \\cdots, y_N\\big)$. This joint distribution is still Gaussian because its marginal distributions $p(y_n)$ are all Gaussian.</div>\n",
    "4. <div style='background-color:#f0ffe0'>High dimensional Gaussian is determined by the mean and covariance.  \n",
    "$$\\color{red}{\\begin{align*}\n",
    "\\mathbb{E}[\\mathbf{y}] &= \\Phi\\mathbb{E}[\\mathbf{w}] = (0,\\cdots,0)^T \\tag{6.52}\\\\\n",
    "\\text{cov}[\\mathbf{y}] &= \\mathbb{E}\\big[(\\mathbf{y}-\\mathbb{E}[\\mathbf{y}])(\\mathbf{y}-\\mathbb{E}[\\mathbf{y}])^T\\big]=\\mathbb{E}[\\mathbf{y}\\mathbf{y}^T] = \\Phi\\mathbb{E}[\\mathbf{w}\\mathbf{w}^T]\\Phi^T = \\frac{1}{\\alpha}\\Phi\\Phi^T = \\mathbf{K} \\tag{6.53}\n",
    "\\end{align*}}$$\n",
    "where $\\mathbf{K}$ is the Gram matrix with elements\n",
    "$$\\color{red}{K_{nm} = k(\\mathbf{x}_n, \\mathbf{x}_m) = \\frac{1}{\\alpha}\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\tag{6.54}}$$\n",
    "and $k(\\mathbf{x},\\mathbf{x}')$ is the kernel function.</div>\n",
    "5. <div style='background-color:#ffe0f0'>In this case, $y(\\mathbf{x})$ is said to be a Gaussian process, because $y(\\mathbf{x})$ is a Gaussian distributed random variable at arbitrary point of $\\mathbf{x}$.</div>\n",
    "\n",
    "In general, a Gaussian process is defined as a probability distribution over functions $y(\\mathbf{x})$ such that the set of values of $y(\\mathbf{x})$ evaluated at an arbitrary set of points $\\mathbf{x}_1,\\cdots,\\mathbf{x}_N$ jointly have a Gaussian distribution. \n",
    "\n",
    "In most applications, we will not have any prior knowledge about the mean of $y(\\mathbf{x})$ and so by symmetry we take it to zero. This is equivalent to choosing the mean of the prior over weight values $p(\\mathbf{w}|\\alpha)$ to be zero in the basis function viewpoint.\n",
    "\n",
    "For evaluating $y(\\mathbf{x})$, we would rather wish to know the relationship among different points, which are given by $cov[\\mathbf{y}]$. And this relationship will determine the form of $y(\\mathbf{x})$ eventually.\n",
    "\n",
    "------------\n",
    "\n",
    "# Add the noise\n",
    "\n",
    "The vector $\\mathbf{y}$ we discussed above is just about $y(\\mathbf{x})$ itself. Here we need to take account of the noise on the observed target values, which are given by\n",
    "\n",
    "$$t_n = y_n + \\epsilon_n \\tag{6.57}$$\n",
    "\n",
    "Here we shall consider noise at each point have a Gaussian distribution, so that\n",
    "\n",
    "$$p(t_n|y_n) = \\mathcal{N}(t_n|y_n, \\beta^{-1}) \\tag{6.58}$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter representing the precision of the noise. And because the noise is independent for each data point, the noise process is therefore given by an isotropic Gaussian\n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "p(\\mathbf{t}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{t}|\\mathbf{y}, \\beta^{-1}I_n) & (6.59)\\\\\n",
    "p(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|0, \\mathbf{K}) &(6.60)\\\\\n",
    "p(\\mathbf{t}) = \\int p(\\mathbf{t}|\\mathbf{y})p(\\mathbf{y})d\\mathbf{y} &(6.61)\n",
    "\\end{array}\\right\\}\n",
    "\\overset{(2.115)}{\\Rightarrow} p(\\mathbf{t}) = \\mathcal{N}(\\mathbf{t}|0, \\mathbf{C})\\quad \\text{where}\\quad C(\\mathbf{x}_n,\\mathbf{x}_m)=k(\\mathbf{x}_n,\\mathbf{x}_m)+\\beta^{-1}\\delta_{nm} \\tag{6.62}$$\n",
    "\n",
    "where $\\delta_{nm}=\\left\\{\\begin{array}{ll}1 &n=m\\\\ 0 &\\text{otherwise}\\end{array}\\right.$. The covariances are simply added due to independence.\n",
    "\n",
    "---------------\n",
    "\n",
    "# Widely used kernel function\n",
    "We mentioned in Section 6.2 that we can construct kernel with various of techniques. However, here is a common used kernel function.\n",
    "\n",
    "$$k(\\mathbf{x}_n, \\mathbf{x}_m) = \\underbrace{\\theta_0 exp\\left\\{-\\frac{\\theta_1}{2}\\|\\mathbf{x}_n-\\mathbf{x}_m\\|^2\\right\\}}_{non-linear} + \\underbrace{\\theta_2}_{bias} + \\underbrace{\\theta_3\\mathbf{x}_n^T\\mathbf{x}_m}_{linear} \\tag{6.63}$$\n",
    "where $\\mathbf{\\theta} = (\\theta_0,\\theta_1,\\theta_2,\\theta_3)^T$ are the hyperparameters.\n",
    "\n",
    "----------------\n",
    "\n",
    "# Predictive Distribution\n",
    "\n",
    "Now we shall use the obtained Gaussian process to predict the new value $t_{N+1}$ and its probability $p(t_{N+1}|\\mathbf{t})$ at the point $\\mathbf{x}_{N+1}$.\n",
    "\n",
    "For the reason that $t_{N+1}$ is also one point in the Gaussian process, we can write down the joint distribution of $\\mathbf{t}_{N+1} = (t_1,\\cdots, t_N, t_{N+1})^T$ as $p(\\mathbf{t}_{N+1})$ which is a multivariate Gaussian. By making use of the results from Section 2.3.1, we obtain the covariance of the multivariate Gaussian $p(\\mathbf{t}_{N+1})$. \n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "\\mathbf{t} &= (t_1,\\cdots,t_N)^T\\\\\n",
    "\\mathbf{t}_{N+1} &= (t_1,\\cdots, t_N,t_{N+1})^T\\\\\n",
    "p(\\mathbf{t}) &= \\mathcal{N}(\\mathbf{t}|0,\\mathbf{C}_N)\\\\\n",
    "p(\\mathbf{t}_{N+1}) &= \\mathcal{N}(\\mathbf{t}_{N+1}|0, \\mathbf{C}_{N+1})\\\\\n",
    "\\end{array}\\right\\}\\overset{Section\\ 2.3.1 }{\\Rightarrow}\n",
    "\\mathbf{C}_{N+1} = \\begin{bmatrix}\\mathbf{C}_N & \\mathbf{k}\\\\ \\mathbf{k}^T & c\\end{bmatrix} \\tag{6.65}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{C}_{N+1}$ has the same form as $\\mathbf{C}_N$. Thus\n",
    "\n",
    "$$\\mathbf{C}_{N+1}(\\mathbf{x}_n,\\mathbf{x}_m) = k(\\mathbf{x}_n,\\mathbf{x}_m)+\\beta^{-1}\\delta_{nm}\n",
    "\\Rightarrow\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\mathbf{k} = (k_1,\\cdots,k_{N})^T\\quad\\text{where}\\quad k_n = k(\\mathbf{x}_n, \\mathbf{x}_{N+1})\\\\\n",
    "c = k(\\mathbf{x}_{N+1},\\mathbf{x}_{N+1})+\\beta^{-1}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "With this result, we can use the equations (2.81) and (2.82) to derive the conditional distribution\n",
    "\n",
    "$$\\bbox[#e0f0ff]{p(t_{N+1}|\\mathbf{t}) = \\mathcal{N}(t_{N+1}|m, \\sigma^2)\\quad \\text{where}\\quad \n",
    "\\left\\{\\begin{array}{ll}\n",
    "m &= \\mathbf{k}^T\\mathbf{C}_N^{-1}\\mathbf{t} & (6.66)\\\\\n",
    "\\sigma^2 &= c-\\mathbf{k}^T\\mathbf{C}_{N}^{-1}\\mathbf{k} &(6.67)\n",
    "\\end{array}\\right.}$$\n",
    "\n",
    "Recall that, in Section 6.2, we discussed that the kernel function $k(\\mathbf{x},\\mathbf{x}')$ can be constructed using various of techniques. The only restriction is that the covariance matrix $\\mathbf{C}_N$ given by (6.62) must be positive definite.\n",
    "\n",
    "\n",
    "<font color='#bbbbbb'>\n",
    "Note that the mean (6.66) of the predictive distribution can be written, as a function of $\\mathbf{x}_{N+1}$, in the form\n",
    "\n",
    "$$m(\\mathbf{x}_{N+1}) = \\sum_{n=1}^N a_n k(\\mathbf{x}_n, \\mathbf{x}_{N+1}) \\tag{6.68}$$\n",
    "\n",
    "where $a_n$ is the $n^{th}$ component of $\\mathbf{C}_N^{-1}\\mathbf{t}$. Thus, if the kernel function $k(\\mathbf{x}_n,\\mathbf{x}_m)$ depends only on the distance $\\|\\mathbf{x}_n-\\mathbf{x}_m\\|$, then we obtain an **expansion** in radial basis functions.\n",
    "</font>\n",
    "\n",
    "---------------\n",
    "\n",
    "# Performance\n",
    "\n",
    "The central computational operation in using Gaussian processes will involve the inversion of a matrix $\\mathbf{C}_N$ of size $N\\times N$, for which stardard methods require $O(N^3)$ computations. By contrast, in the basis function model we have to invert a matrix $S_{N}$ of size $M\\times M$, which is $O(M^3)$ computational complexity. If the number $M$ of basis functions is smaller than the number $N$ of data points, it will be computational more efficient to work in the basis function frame work.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "# Learning the hyperparameters\n",
    "\n",
    "The prediction of Gaussian process model will depend, in part, on the choice of covariance function. In practice, rather than fixing the \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---------------\n",
    "\n",
    "# Automatic relevance determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = [56.42952261 59.59849514 36.5428274  35.54260546]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# convert warnings to error\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "sigma = 0.1\n",
    "noise_sigma = 0.2\n",
    "\n",
    "# avoid over-fitting\n",
    "# add noise\n",
    "BETA = (noise_sigma**2)**(-1)\n",
    "# BETA = 0\n",
    "# early stopping\n",
    "ITERATE_COUNTS = 1000\n",
    "\n",
    "N = 10\n",
    "\n",
    "kernel = lambda theta, xn, xm: theta[0]*np.exp(-theta[1]/2 * (xn-xm)**2) + theta[2] + theta[3]*xn*xm\n",
    "\n",
    "A = None\n",
    "F = None\n",
    "def Adam(derivatives):\n",
    "    a0 = 0.1\n",
    "    rf = 0.9\n",
    "    r = 0.999\n",
    "    global A, F\n",
    "    if A is None:\n",
    "        A = np.full(derivatives.shape, 0)\n",
    "        F = np.full(derivatives.shape, 0)\n",
    "    A = r*A + (1-r)*derivatives**2\n",
    "    F = rf*F + (1-rf)*derivatives\n",
    "    at = a0 * (np.sqrt(1-r)/(1-rf))\n",
    "    return -at/(np.sqrt(A)+1e-8)*F\n",
    "\n",
    "def gen_convariance(X, theta):\n",
    "    l = len(X)\n",
    "    CN = np.zeros((l, l))\n",
    "    for i in range(l):\n",
    "        for j in range(l):\n",
    "            CN[i][j] = kernel(theta, X[i], X[j])\n",
    "            if i==j:\n",
    "                CN[i][j] += BETA\n",
    "    return CN\n",
    "\n",
    "def partial_derivatives(X, theta, i):\n",
    "    l = len(X)\n",
    "    DCN = np.zeros((l, l))\n",
    "    if i==0:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = np.exp(-theta[1]/2 * (X[i]-X[j])**2)\n",
    "    elif i==1:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = theta[0] * (-1/2 * (X[i]-X[j])**2) * np.exp(-theta[1]/2 * (X[i]-X[j])**2)\n",
    "    elif i==2:\n",
    "        DCN = np.ones((l, l))\n",
    "    elif i==3:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = X[i] * X[j]\n",
    "    return DCN\n",
    "\n",
    "# Learning the hyperparameters\n",
    "def hyperparameter_optmization(X, T):\n",
    "    theta = np.random.normal(0, 1, 4)   \n",
    "    derivatives = np.zeros(4)\n",
    "    for j in range(ITERATE_COUNTS):\n",
    "        CN = gen_convariance(X, theta)\n",
    "        ICN = np.linalg.inv(CN)\n",
    "        for i in range(4):\n",
    "            DCN = partial_derivatives(X, theta, i)\n",
    "            derivatives[i] = -1/2*np.trace(ICN @ DCN) + 1/2* T @ ICN @ DCN @ ICN @ T\n",
    "        if np.allclose(derivatives, 0):\n",
    "            print(\"stationary point found!!!\")\n",
    "            break\n",
    "        theta += Adam(derivatives)\n",
    "    return theta\n",
    "\n",
    "def main():\n",
    "    fig = plt.figure(figsize=(6,4), dpi=50)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_ylim(-1.6, 1.6)\n",
    "    \n",
    "    #xn = np.random.uniform(0, 1, N)\n",
    "    xn = np.linspace(0, 1, N)\n",
    "    noise = np.random.normal(0, noise_sigma, N)\n",
    "    tn = np.sin(2*np.pi*xn)+noise\n",
    "    lx = np.linspace(0, 1, 30)\n",
    "    ly = np.sin(2*np.pi*lx)\n",
    "    \n",
    "    ax.plot(lx, ly, 'g')\n",
    "    ax.scatter(xn, tn, color='b')\n",
    "    \n",
    "    theta = hyperparameter_optmization(xn, tn)\n",
    "    print(\"theta =\", theta)\n",
    "    CN = gen_convariance(xn, theta)\n",
    "    ICN = np.linalg.inv(CN)\n",
    "    \n",
    "    y = np.zeros(ly.shape)\n",
    "    std = np.zeros(ly.shape)\n",
    "    for i in range(len(lx)):\n",
    "        k = kernel(theta, lx[i], xn)\n",
    "        y[i] = k@ICN@tn\n",
    "        \n",
    "        c = kernel(theta, lx[i], lx[i])\n",
    "        var = c - k@ICN@k\n",
    "        if var < 0:\n",
    "            print(var)\n",
    "            var = 0\n",
    "        std[i] = np.sqrt(var)\n",
    "    #print(std)\n",
    "    ax.plot(lx, y, 'r')\n",
    "    ax.fill_between(lx, y-std, y+std, color='C3', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

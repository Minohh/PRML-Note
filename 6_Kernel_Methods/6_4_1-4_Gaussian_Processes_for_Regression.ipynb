{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression revisited\n",
    "\n",
    "From this section, we will learn how we go into the Gaussian processes from the linear regression model.\n",
    "\n",
    "1. <div style='background-color:#e0f0ff'>Here is a general linear regression model.\n",
    "$$y(\\mathbf{x}) = \\mathbf{w}^T\\phi(\\mathbf{x}) \\tag{6.49}$$\n",
    "where $\\mathbf{w}$ is a $M$-dimensional weight vector and $\\phi(\\mathbf{x})$ is also a $M$-dimensional basis vector.</div>\n",
    "2. <div style='background-color:#f0e0ff'>Now we consider a prior distribution over $\\mathbf{w}$ that is given by an isotropic Gaussian of the form\n",
    "$$p(\\mathbf{w}) = \\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}I) \\tag{6.50}$$\n",
    "For specific $\\mathbf{x}$, $y(\\mathbf{x})$ is also a Gaussian distributed variable because it is the linear combination of Gaussian variables.</div>\n",
    "3. <div style='background-color:#fff0e0'>In practice, we are given a training data set at the training data points $\\mathbf{X} = (\\mathbf{x}_1,\\cdots,\\mathbf{x}_N)^T$. We are therefore interested in the joint distribution of the function values $\\mathbf{y} = (y_1,\\cdots, y_N)^T$, where $y_n = y(\\mathbf{x}_n)$. This vector is given by\n",
    "$$\\mathbf{y} = \\Phi \\mathbf{w}\\quad \\text{where}\\quad \\Phi = \\big(\\phi(\\mathbf{x}_1),\\cdots, \\phi(\\mathbf{x})_N\\big)^T\\tag{6.51}$$\n",
    "Here, $\\mathbf{y}$ is a vector whose elements are random variables at different points. This vector is derived by a joint probability density function denoted by $p\\big(y_1, \\cdots, y_N\\big)$. This joint distribution is still Gaussian because its marginal distributions $p(y_n)$ are all Gaussian.</div>\n",
    "4. <div style='background-color:#f0ffe0'>High dimensional Gaussian is determined by the mean and covariance.  \n",
    "$$\\color{red}{\\begin{align*}\n",
    "\\mathbb{E}[\\mathbf{y}] &= \\Phi\\mathbb{E}[\\mathbf{w}] = (0,\\cdots,0)^T \\tag{6.52}\\\\\n",
    "\\text{cov}[\\mathbf{y}] &= \\mathbb{E}\\big[(\\mathbf{y}-\\mathbb{E}[\\mathbf{y}])(\\mathbf{y}-\\mathbb{E}[\\mathbf{y}])^T\\big]=\\mathbb{E}[\\mathbf{y}\\mathbf{y}^T] = \\Phi\\mathbb{E}[\\mathbf{w}\\mathbf{w}^T]\\Phi^T = \\frac{1}{\\alpha}\\Phi\\Phi^T = \\mathbf{K} \\tag{6.53}\n",
    "\\end{align*}}$$\n",
    "where $\\mathbf{K}$ is the Gram matrix with elements\n",
    "$$\\color{red}{K_{nm} = k(\\mathbf{x}_n, \\mathbf{x}_m) = \\frac{1}{\\alpha}\\phi(\\mathbf{x}_n)^T\\phi(\\mathbf{x}_m) \\tag{6.54}}$$\n",
    "and $k(\\mathbf{x},\\mathbf{x}')$ is the kernel function.</div>\n",
    "5. <div style='background-color:#ffe0f0'>In this case, $y(\\mathbf{x})$ is said to be a Gaussian process, because $y(\\mathbf{x})$ is a Gaussian distributed random variable at arbitrary point of $\\mathbf{x}$.</div>\n",
    "\n",
    "In general, a Gaussian process is defined as a probability distribution over functions $y(\\mathbf{x})$ such that the set of values of $y(\\mathbf{x})$ evaluated at an arbitrary set of points $\\mathbf{x}_1,\\cdots,\\mathbf{x}_N$ jointly have a Gaussian distribution. \n",
    "\n",
    "In most applications, we will not have any prior knowledge about the mean of $y(\\mathbf{x})$ and so by symmetry we take it to zero. This is equivalent to choosing the mean of the prior over weight values $p(\\mathbf{w}|\\alpha)$ to be zero in the basis function viewpoint.\n",
    "\n",
    "For evaluating $y(\\mathbf{x})$, we would rather wish to know the relationship among different points, which are given by $cov[\\mathbf{y}]$. And this relationship will determine the form of $y(\\mathbf{x})$ eventually.\n",
    "\n",
    "------------\n",
    "\n",
    "# Add the noise\n",
    "\n",
    "The vector $\\mathbf{y}$ we discussed above is just about $y(\\mathbf{x})$ itself. Here we need to take account of the noise on the observed target values, which are given by\n",
    "\n",
    "$$t_n = y_n + \\epsilon_n \\tag{6.57}$$\n",
    "\n",
    "Here we shall consider noise at each point have a Gaussian distribution, so that\n",
    "\n",
    "$$p(t_n|y_n) = \\mathcal{N}(t_n|y_n, \\beta^{-1}) \\tag{6.58}$$\n",
    "\n",
    "where $\\beta$ is a hyperparameter representing the precision of the noise. And because the noise is independent for each data point, the noise process is therefore given by an isotropic Gaussian\n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "p(\\mathbf{t}|\\mathbf{y}) = \\mathcal{N}(\\mathbf{t}|\\mathbf{y}, \\beta^{-1}I_n) & (6.59)\\\\\n",
    "p(\\mathbf{y}) = \\mathcal{N}(\\mathbf{y}|0, \\mathbf{K}) &(6.60)\\\\\n",
    "p(\\mathbf{t}) = \\int p(\\mathbf{t}|\\mathbf{y})p(\\mathbf{y})d\\mathbf{y} &(6.61)\n",
    "\\end{array}\\right\\}\n",
    "\\overset{(2.115)}{\\Rightarrow} p(\\mathbf{t}) = \\mathcal{N}(\\mathbf{t}|0, \\mathbf{C})\\quad \\text{where}\\quad C(\\mathbf{x}_n,\\mathbf{x}_m)=k(\\mathbf{x}_n,\\mathbf{x}_m)+\\beta^{-1}\\delta_{nm} \\tag{6.62}$$\n",
    "\n",
    "where $\\delta_{nm}=\\left\\{\\begin{array}{ll}1 &n=m\\\\ 0 &\\text{otherwise}\\end{array}\\right.$. The covariances are simply added due to independence.\n",
    "\n",
    "---------------\n",
    "\n",
    "# Widely used kernel function\n",
    "We mentioned in Section 6.2 that we can construct kernel with various of techniques. However, here is a common used kernel function.\n",
    "\n",
    "$$k(\\mathbf{x}_n, \\mathbf{x}_m) = \\underbrace{\\theta_0 exp\\left\\{-\\frac{\\theta_1}{2}\\|\\mathbf{x}_n-\\mathbf{x}_m\\|^2\\right\\}}_{non-linear} + \\underbrace{\\theta_2}_{bias} + \\underbrace{\\theta_3\\mathbf{x}_n^T\\mathbf{x}_m}_{linear} \\tag{6.63}$$\n",
    "where $\\mathbf{\\theta} = (\\theta_0,\\theta_1,\\theta_2,\\theta_3)^T$ are the hyperparameters.\n",
    "\n",
    "----------------\n",
    "\n",
    "# Predictive Distribution\n",
    "\n",
    "Now we shall use the obtained Gaussian process to predict the new value $t_{N+1}$ and its probability $p(t_{N+1}|\\mathbf{t})$ at the point $\\mathbf{x}_{N+1}$.\n",
    "\n",
    "For the reason that $t_{N+1}$ is also one point in the Gaussian process, we can write down the joint distribution of $\\mathbf{t}_{N+1} = (t_1,\\cdots, t_N, t_{N+1})^T$ as $p(\\mathbf{t}_{N+1})$ which is a multivariate Gaussian. By making use of the results from Section 2.3.1, we obtain the covariance of the multivariate Gaussian $p(\\mathbf{t}_{N+1})$. \n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "\\mathbf{t} &= (t_1,\\cdots,t_N)^T\\\\\n",
    "\\mathbf{t}_{N+1} &= (t_1,\\cdots, t_N,t_{N+1})^T\\\\\n",
    "p(\\mathbf{t}) &= \\mathcal{N}(\\mathbf{t}|0,\\mathbf{C}_N)\\\\\n",
    "p(\\mathbf{t}_{N+1}) &= \\mathcal{N}(\\mathbf{t}_{N+1}|0, \\mathbf{C}_{N+1})\\\\\n",
    "\\end{array}\\right\\}\\overset{Section\\ 2.3.1 }{\\Rightarrow}\n",
    "\\mathbf{C}_{N+1} = \\begin{bmatrix}\\mathbf{C}_N & \\mathbf{k}\\\\ \\mathbf{k}^T & c\\end{bmatrix} \\tag{6.65}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{C}_{N+1}$ has the same form as $\\mathbf{C}_N$. Thus\n",
    "\n",
    "$$\\mathbf{C}_{N+1}(\\mathbf{x}_n,\\mathbf{x}_m) = k(\\mathbf{x}_n,\\mathbf{x}_m)+\\beta^{-1}\\delta_{nm}\n",
    "\\Rightarrow\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\mathbf{k} = (k_1,\\cdots,k_{N})^T\\quad\\text{where}\\quad k_n = k(\\mathbf{x}_n, \\mathbf{x}_{N+1})\\\\\n",
    "c = k(\\mathbf{x}_{N+1},\\mathbf{x}_{N+1})+\\beta^{-1}\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "With this result, we can use the equations (2.81) and (2.82) to derive the conditional distribution\n",
    "\n",
    "$$\\bbox[#e0f0ff]{p(t_{N+1}|\\mathbf{t}) = \\mathcal{N}(t_{N+1}|m, \\sigma^2)\\quad \\text{where}\\quad \n",
    "\\left\\{\\begin{array}{ll}\n",
    "m &= \\mathbf{k}^T\\mathbf{C}_N^{-1}\\mathbf{t} & (6.66)\\\\\n",
    "\\sigma^2 &= c-\\mathbf{k}^T\\mathbf{C}_{N}^{-1}\\mathbf{k} &(6.67)\n",
    "\\end{array}\\right.}$$\n",
    "\n",
    "Recall that, in Section 6.2, we discussed that the kernel function $k(\\mathbf{x},\\mathbf{x}')$ can be constructed using various of techniques. The only restriction is that the covariance matrix $\\mathbf{C}_N$ given by (6.62) must be positive definite.\n",
    "\n",
    "\n",
    "<font color='#bbbbbb'>\n",
    "Note that the mean (6.66) of the predictive distribution can be written, as a function of $\\mathbf{x}_{N+1}$, in the form\n",
    "\n",
    "$$m(\\mathbf{x}_{N+1}) = \\sum_{n=1}^N a_n k(\\mathbf{x}_n, \\mathbf{x}_{N+1}) \\tag{6.68}$$\n",
    "\n",
    "where $a_n$ is the $n^{th}$ component of $\\mathbf{C}_N^{-1}\\mathbf{t}$. Thus, if the kernel function $k(\\mathbf{x}_n,\\mathbf{x}_m)$ depends only on the distance $\\|\\mathbf{x}_n-\\mathbf{x}_m\\|$, then we obtain an **expansion** in radial basis functions.\n",
    "</font>\n",
    "\n",
    "---------------\n",
    "\n",
    "# Performance\n",
    "\n",
    "The central computational operation in using Gaussian processes will involve the inversion of a matrix $\\mathbf{C}_N$ of size $N\\times N$, for which stardard methods require $O(N^3)$ computations. By contrast, in the basis function model we have to invert a matrix $S_{N}$ of size $M\\times M$, which is $O(M^3)$ computational complexity. If the number $M$ of basis functions is smaller than the number $N$ of data points, it will be computational more efficient to work in the basis function frame work.\n",
    "\n",
    "\n",
    "-----------------\n",
    "\n",
    "# Learning the hyperparameters\n",
    "\n",
    "Before going into the training procedure, we should determine the values of the hyperparameters such that the kernel of the Gaussian process is able to provide nice effect of prediction in the subsequent training step.\n",
    "\n",
    "The predictions of Gaussian process model depends on the covariance matrix $\\mathbf{C}_{N}$ directly. But fundamentally, The predictions depend on the hyperparameter $\\mathbf{\\theta}$, because the elements of the covariance matrix is given by the kernel function that is constructed by these hyperparameters. In (6.62), we have obtained the joint probability of $p(\\mathbf{t})$. If we take the hyperparameters into account, the probability can be rewritten in the following form.\n",
    "\n",
    "$$p(\\mathbf{t}|\\mathbf{\\theta}) = \\mathcal{N}\\big(\\mathbf{t}|0, \\mathbf{C}_N(\\mathbf{\\theta})\\big)$$\n",
    "\n",
    "which is a likelihood function. It log likelihood function is easily evaluated using the standard form for a multivariate Gaussian distribution, giving\n",
    "\n",
    "$$\\ln p(\\mathbf{t}|\\mathbf{\\theta}) = -\\frac{1}{2}\\ln|\\mathbf{C}_N| - \\frac{1}{2}\\mathbf{t}^T\\mathbf{C}_N^{-1}\\mathbf{t}-\\frac{N}{2}\\ln(2\\pi) \\tag{6.69}$$\n",
    "\n",
    "The partial derivatives with respect to $\\theta_i$ is given by\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_i}\\ln p (\\mathbf{t}|\\mathbf{\\theta}) = -\\frac{1}{2}Tr\\left(\\mathbf{C}_N^{-1} \\frac{\\partial \\mathbf{C}_N}{\\partial \\theta_i}\\right) + \\frac{1}{2}\\mathbf{t}^T \\mathbf{C}_N^{-1}\\frac{\\partial \\mathbf{C}_N}{\\partial \\theta_i} \\mathbf{C}_N^{-1} \\mathbf{t} \\tag{6.70}$$\n",
    "\n",
    "Because $\\ln p (\\mathbf{t}|\\mathbf{\\theta})$ will in general be a nonconvex function, it can have multiple maxima. We can use the optimization technique of Deep Learning such as Adam to find the stationary point of these hyperparameters.\n",
    "\n",
    "---------------\n",
    "\n",
    "# Automatic relevance determination\n",
    "\n",
    "The main idea of ARD (automatic relevance determination) is to allow the relative importance of different inputs to be inferred from the data. In other words, in the kernel function, each input of feature is weighted by an individual hyperparameter. These hyperparameters represents the importance of the input features. And they will be evaluated using the nonconvex optimization techniques. Here is an example of the ARD kernel functions\n",
    "\n",
    "$$k(\\mathbf{x}, \\mathbf{x}') = \\theta_0 exp\\left\\{-\\frac{1}{2}\\sum_{i=1}^2 \\eta_i (x_i - x_i')^2\\right\\} \\tag{6.71}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta = [10.36638358  5.06959314 -3.89336954  0.04031136]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 300x200 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# convert warnings to error\n",
    "warnings.filterwarnings(\"error\")\n",
    "\n",
    "noise_sigma = 0.2\n",
    "\n",
    "# avoid over-fitting\n",
    "# add noise\n",
    "IBETA = noise_sigma**2\n",
    "# IBETA = 0\n",
    "# early stopping\n",
    "ITERATE_COUNTS = 100\n",
    "\n",
    "N = 10\n",
    "\n",
    "kernel = lambda theta, xn, xm: theta[0]*np.exp(-theta[1]/2 * (xn-xm)**2) + theta[2] + theta[3]*xn*xm\n",
    "\n",
    "A = None\n",
    "F = None\n",
    "def Adam(derivatives):\n",
    "    a0 = 0.1\n",
    "    rf = 0.9\n",
    "    r = 0.999\n",
    "    global A, F\n",
    "    if A is None:\n",
    "        A = np.full(derivatives.shape, 0)\n",
    "        F = np.full(derivatives.shape, 0)\n",
    "    A = r*A + (1-r)*derivatives**2\n",
    "    F = rf*F + (1-rf)*derivatives\n",
    "    at = a0 * (np.sqrt(1-r)/(1-rf))\n",
    "    return -at/(np.sqrt(A)+1e-8)*F\n",
    "\n",
    "def gen_convariance(X, theta):\n",
    "    l = len(X)\n",
    "    CN = np.zeros((l, l))\n",
    "    for i in range(l):\n",
    "        for j in range(l):\n",
    "            CN[i][j] = kernel(theta, X[i], X[j])\n",
    "            if i==j:\n",
    "                CN[i][j] += IBETA\n",
    "    return CN\n",
    "\n",
    "def partial_derivatives(X, theta, i):\n",
    "    l = len(X)\n",
    "    DCN = np.zeros((l, l))\n",
    "    if i==0:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = np.exp(-theta[1]/2 * (X[i]-X[j])**2)\n",
    "    elif i==1:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = theta[0] * (-1/2 * (X[i]-X[j])**2) * np.exp(-theta[1]/2 * (X[i]-X[j])**2)\n",
    "    elif i==2:\n",
    "        DCN = np.ones((l, l))\n",
    "    elif i==3:\n",
    "        for i in range(l):\n",
    "            for j in range(l):\n",
    "                DCN[i][j] = X[i] * X[j]\n",
    "    return DCN\n",
    "\n",
    "# Learning the hyperparameters\n",
    "def hyperparameter_optmization(X, T):\n",
    "    theta = np.random.normal(0, 4, 4)   \n",
    "    derivatives = np.zeros(4)\n",
    "    for j in range(ITERATE_COUNTS):\n",
    "        CN = gen_convariance(X, theta)\n",
    "        ICN = np.linalg.inv(CN)\n",
    "        for i in range(4):\n",
    "            DCN = partial_derivatives(X, theta, i)\n",
    "            derivatives[i] = -(-1/2*np.trace(ICN @ DCN) + 1/2* T @ ICN @ DCN @ ICN @ T)\n",
    "        if np.allclose(derivatives, 0):\n",
    "            print(\"stationary point found!!!\")\n",
    "            break\n",
    "        theta += Adam(derivatives)\n",
    "    return theta\n",
    "\n",
    "def main():\n",
    "    fig = plt.figure(figsize=(6,4), dpi=50)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_ylim(-1.6, 1.6)\n",
    "    \n",
    "    #xn = np.random.uniform(0, 1, N)\n",
    "    xn = np.linspace(0, 1, N)\n",
    "    noise = np.random.normal(0, noise_sigma, N)\n",
    "    tn = np.sin(2*np.pi*xn)+noise\n",
    "    lx = np.linspace(0, 1, 30)\n",
    "    ly = np.sin(2*np.pi*lx)\n",
    "    \n",
    "    ax.plot(lx, ly, 'g')\n",
    "    ax.scatter(xn, tn, color='b')\n",
    "    \n",
    "    theta = hyperparameter_optmization(xn, tn)\n",
    "    print(\"theta =\", theta)\n",
    "    CN = gen_convariance(xn, theta)\n",
    "    ICN = np.linalg.inv(CN)\n",
    "    \n",
    "    y = np.zeros(ly.shape)\n",
    "    std = np.zeros(ly.shape)\n",
    "    for i in range(len(lx)):\n",
    "        k = kernel(theta, lx[i], xn)\n",
    "        y[i] = k@ICN@tn\n",
    "        \n",
    "        c = kernel(theta, lx[i], lx[i]) + IBETA\n",
    "        var = c - k@ICN@k\n",
    "        if var < 0:\n",
    "            print(var)\n",
    "            var = 0\n",
    "        std[i] = np.sqrt(var)\n",
    "    #print(std)\n",
    "    ax.plot(lx, y, 'r')\n",
    "    ax.fill_between(lx, y-std, y+std, color='C3', alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

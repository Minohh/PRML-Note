{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n",
    " \n",
    "$$\\begin{align*}\n",
    "\\text{quadratic error function:} &\\qquad E_{q}(y_n - t_n) = \\frac{1}{2}\\sum_{n=1}^N\\big\\{y_n - t_n\\big\\}^2 \\tag{7.50}\\\\\n",
    "\\epsilon\\text{-insensitive error function:} &\\qquad E_{\\epsilon}(y_n - t_n) = \n",
    "\\left\\{\\begin{array}{ll}\n",
    "0, &\\text{if }|y_n - t_n|<\\epsilon\\\\\n",
    "|y_n - t_n|-\\epsilon, &\\text{otherwise}\n",
    "\\end{array}\\right. \\tag{7.51}\n",
    "\\end{align*}$$\n",
    "\n",
    "The region $(y-\\epsilon, y+\\epsilon)$ is a tube with width $\\epsilon$ centers on $y$. As the $\\epsilon$-insensitive error function shows, the samples whose $t_n$ lie inside the tube contribute nothing to the error function for sparseness, and the errors of the samples that lie on the boundary or outside the tube are equal to the distance from the sample points to the boundary.\n",
    "\n",
    "# Mathmatic representation\n",
    "\n",
    "## Problem\n",
    "Our goal is to minimize a regularized error function given by\n",
    "\n",
    "$$C\\sum_{n=1}^N E_{\\epsilon}\\big(y_n - t_n\\big) + \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\tag{7.52}$$\n",
    "\n",
    "\n",
    "\n",
    "## Constraints\n",
    "\n",
    "We need to transform the $\\epsilon$-insensitive error function to a computationally easier form.\n",
    "\n",
    "$$E_{\\epsilon}(y_n - t_n) = \n",
    "\\left\\{\\begin{array}{ll}\n",
    "0, &\\text{if }|y_n - t_n|<\\epsilon\\\\\n",
    "|y_n - t_n|-\\epsilon, &\\text{otherwise}\n",
    "\\end{array}\\right.\n",
    "= \\left\\{\\begin{array}{ll}\n",
    "0\\geqslant t_n - y_n -\\epsilon, &\\text{if }0\\leqslant t_n - y_n<\\epsilon\\\\\n",
    "0\\geqslant y_n - t_n -\\epsilon, &\\text{if }-\\epsilon< t_n - y_n\\leqslant 0\\\\\n",
    "t_n - y_n -\\epsilon, &\\text{if }t_n - y_n\\geqslant \\epsilon\\\\\n",
    "y_n - t_n -\\epsilon, &\\text{if }t_n - y_n\\leqslant -\\epsilon\\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "For further processing with lagrange multiplier, we also need to eliminate the conditions of these inequal equations. Here, we introduce two variables\n",
    "\n",
    "$$E_{\\epsilon}(y_n - t_n) = \n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\xi_n, &\\text{if } t_n - y_n \\geqslant 0\\\\\n",
    "\\hat{\\xi}_n, &\\text{if } t_n - y_n \\leqslant 0\\\\\n",
    "\\end{array}\\right.\n",
    "\\qquad\n",
    "\\text{where }\n",
    "\\xi_n\\text{ and }\\hat{\\xi}_n\\text{ satisfy }\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\xi_n\\geqslant t_n - y_n -\\epsilon\\\\\n",
    "\\xi_n\\geqslant 0\\\\\n",
    "\\hat{\\xi}_n\\geqslant y_n - t_n -\\epsilon\\\\\n",
    "\\hat{\\xi}_n\\geqslant 0\\\\\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where $\\xi_n$ denotes the error of the sample that lie in the upside of the tube, and $\\hat{\\xi}_n$ denotes the error of the sample that lie in the downside of the tube. And because $\\xi_n$ is zero if the sample is not in the upside as well as $\\hat{\\xi}_n$ is zero if the sample is not in the downside, error of an individual sample can be written in the form\n",
    "\n",
    "$$E_{\\epsilon}(y_n - t_n) = \\xi_n + \\hat{\\xi}_n$$\n",
    "\n",
    "where the conditions $\\xi_n\\geqslant t_n - y_n -\\epsilon$ and $\\hat{\\xi}_n\\geqslant y_n - t_n -\\epsilon$ are mutually exclusive.\n",
    "\n",
    "# Solution \n",
    "\n",
    "## Introduce Lagrange multiplier\n",
    "\n",
    "According to the Lagrange multiplier theorey, we can solve the SVM regression problem by finding the solution of \n",
    "\n",
    "$$\\underset{\\mathbf{a}\\geqslant 0,\\mathbf{\\hat{a}}\\geqslant 0,\\mathbf{\\mu}\\geqslant 0,\\mathbf{\\hat{\\mu}}\\geqslant 0}{\\quad max\\quad }\\underset{\\mathbf{w},b,\\mathbf{\\xi}, \\mathbf{\\hat{\\xi}}}{\\quad min\\quad }L(\\mathbf{w},b,\\mathbf{\\xi},\\mathbf{\\hat{\\xi}},\\mathbf{a},\\mathbf{\\hat{a}},\\mathbf{\\mu},\\mathbf{\\hat{\\mu}})$$\n",
    "\n",
    "where the Lagrangian function, or say objective function, is given by\n",
    "\n",
    "$$\n",
    "\\left.\\begin{array}{ll}\n",
    "\\text{Problem:} & \\displaystyle{\\underset{\\mathbf{w},b,\\mathbf{\\xi}, \\mathbf{\\hat{\\xi}}}{\\ min\\ }C\\sum_{n=1}^N(\\xi_n+\\hat{\\xi}_n) + \\frac{1}{2}\\|\\mathbf{w}\\|^2 } \\\\\n",
    "\\text{Constraint 1:} &y_n-t_n+\\epsilon+\\xi_n\\geqslant 0 \\\\\n",
    "\\text{Constraint 2:} &-y_n+t_n+\\epsilon+\\hat{\\xi}_n\\geqslant 0 \\\\\n",
    "\\text{Constraint 3:} &\\xi_n\\geqslant 0 \\\\\n",
    "\\text{Constraint 4:} &\\hat{\\xi}_n\\geqslant 0 \n",
    "\\end{array}\\right\\}\n",
    "\\Rightarrow\n",
    "\\begin{align*}\n",
    "L = &C\\sum_{n=1}^N(\\xi_n+\\hat{\\xi}_n) + \\frac{1}{2}\\|\\mathbf{w}\\|^2 -\\sum_{n=1}^N(\\mu_n\\xi_n+\\hat{\\mu}_n\\hat{\\xi}_n) \\\\\n",
    "&- \\sum_{n=1}^N a_n(y_n-t_n+\\epsilon+\\xi_n) - \\sum_{n=1}^N \\hat{a}_n(-y_n+t_n+\\epsilon+\\hat{\\xi}_n)\n",
    "\\end{align*}\\tag{7.56}$$\n",
    "\n",
    "For the reason that the conditions $\\xi_n\\geqslant t_n - y_n -\\epsilon$ and $\\hat{\\xi}_n\\geqslant y_n - t_n -\\epsilon$ are mutually exclusive, the lagrange multipliers $a_n$ and $\\hat{a}_n$ satisfy\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{if } a_n\\neq 0, &\\hat{a}_n = 0\\\\\n",
    "\\text{if } \\hat{a}_n\\neq 0, &a_n = 0\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Finding the solution of $\\underset{\\mathbf{w},b,\\mathbf{\\xi},\\mathbf{\\hat{\\xi}}}{\\ min\\ }L$ is equivalent to finding the partial derivatives with respect to $\\mathbf{w}$, $b$, $\\mathbf{\\xi}$, $\\mathbf{\\hat{\\xi}}$ equal to zero.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial\\mathbf{w}} = 0 &\\quad\\Rightarrow\\quad \\mathbf{w} = \\sum_{n=1}^N (a_n - \\hat{a}_n) \\phi(\\mathbf{x}_n) \\tag{7.57}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = 0 &\\quad\\Rightarrow\\quad \\sum_{n=1}^N (a_n - \\hat{a}_n) = 0 \\tag{7.58}\\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_n} = 0 &\\quad\\Rightarrow\\quad a_n = C-\\mu_n \\tag{7.59}\\\\\n",
    "\\frac{\\partial L}{\\partial \\hat{\\xi}_n} = 0 &\\quad\\Rightarrow\\quad \\hat{a}_n = C-\\hat{\\mu}_n \\tag{7.60}\n",
    "\\end{align*}$$\n",
    "\n",
    "where \n",
    "- (7.57) indicates that the weight vector $\\mathbf{w}$ changes along the difference between the multiplier $\\mathbf{a}$ and $\\mathbf{\\hat{a}}$.\n",
    "- (7.58) is the linear equality constraint that the multipliers shall satisfy.\n",
    "- (7.59) limits the range of the multipliers $\\mathbf{a}$. The Lagrange multiplier theorey requires $\\mu_n\\geqslant 0$ such that each multiplier in $\\mathbf{a}$ should satisfy $a_n\\leqslant C$.\n",
    "- (7.60) limits the range of the multipliers $\\mathbf{\\hat{a}}$. The Lagrange multiplier theorey requires $\\hat{\\mu}_n\\geqslant 0$ such that each multiplier in $\\mathbf{\\hat{a}}$ should satisfy $\\hat{a}_n\\leqslant C$.\n",
    "\n",
    "Substitute these conditions into the Lagragian function, we obtain\n",
    "\n",
    "$$\\bbox[#ffe0f0]{L(\\mathbf{a},\\mathbf{\\hat{a}}) = -\\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N (a_n - \\hat{a}_n)(a_m - \\hat{a}_m) k(\\mathbf{x}_n,\\mathbf{x}_m) - \\epsilon\\sum_{n=1}^N (a_n+\\hat{a}_n) + \\sum_{n=1}^N(a_n-\\hat{a}_n)t_n} \\tag{7.61}$$\n",
    "\n",
    "which is an equation that is only related to the multipliers $\\mathbf{a}$ and $\\mathbf{\\hat{a}}$. As a result, our goal turns out to be solving the quadratic problem with respect to $\\mathbf{a}$ and $\\mathbf{\\hat{a}}$ subject to the linear equality constraint as well as the constraints on $\\mathbf{a}$ and $\\mathbf{\\hat{a}}$, which is denoted by.\n",
    "\n",
    "$$\\bbox[#e0f0ff]{\\underset{0\\leqslant\\mathbf{a}\\leqslant C, 0\\leqslant\\mathbf{\\hat{a}}\\leqslant C}{\\quad max\\quad } L \\quad s.t.\\ \\sum_{n=1}^N (a_n - \\hat{a}_n) = 0 \\quad\\text{and}\\quad \n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\text{if } a_n\\neq 0, &\\hat{a}_n = 0\\\\\n",
    "\\text{if } \\hat{a}_n\\neq 0, &a_n = 0\\\\\n",
    "\\end{array}\\right.}$$\n",
    "\n",
    "which can be solved using the SMO algorithom.\n",
    "\n",
    "\n",
    "\n",
    "## Solution of $\\mathbf{w}$\n",
    "\n",
    "As we said before, the weight vector $\\mathbf{w}$ changes along the difference between the multiplier $\\mathbf{a}$ and $\\mathbf{\\hat{a}}$, and we just got the difference with the algorithm SMO, thus we can obtain the solution of $\\mathbf{w}$ using the equation\n",
    "\n",
    "$$\\mathbf{w}^\\star = \\sum_{n=1}^N (a_n - \\hat{a}_n) \\phi(\\mathbf{x}_n)$$\n",
    "\n",
    "\n",
    "## Solution of $b$\n",
    "\n",
    "<font color='grey'>*SMO also compute the value of $b$.*</font>\n",
    "\n",
    "<font color='#aaaaaa'>\n",
    "\n",
    "The Lagrange multiplier theorey suggests that the solution of $\\mathbf{a}$ satisfies the KKT condition that takes the form\n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "a_n\\geqslant 0 \\\\\n",
    "y_n-t_n+\\epsilon+\\xi_n\\geqslant 0 \\\\\n",
    "a_n (y_n-t_n+\\epsilon+\\xi_n) = 0 \\\\\n",
    "\\hat{a}_n\\geqslant 0 \\\\\n",
    "-y_n+t_n+\\epsilon+\\hat{\\xi}_n\\geqslant 0 \\\\\n",
    "\\hat{a}_n (-y_n+t_n+\\epsilon+\\hat{\\xi}_n) = 0 \\\\\n",
    "\\mu_n \\geqslant 0 \\\\\n",
    "\\xi_n \\geqslant 0 \\\\\n",
    "\\mu_n\\xi_n = 0 \\\\\n",
    "\\hat{\\mu}_n \\geqslant 0 \\\\\n",
    "\\hat{\\xi}_n \\geqslant 0 \\\\\n",
    "\\hat{\\mu}_n\\hat{\\xi}_n = 0 \\\\\n",
    "------------\\\\\n",
    "a_n = C-\\mu_n \\\\\n",
    "\\hat{a}_n = C-\\hat{\\mu}_n \\\\\n",
    "\\text{if } a_n\\neq 0, \\hat{a}_n = 0\\\\\n",
    "\\text{if } \\hat{a}_n\\neq 0, a_n = 0\n",
    "\\end{array}\\right\\}\n",
    "\\Rightarrow\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\text{if }a_n = 0\\text{ and }\\hat{a}_n=0, & |y_n - t_n|<\\epsilon &(\\mathbf{x}_n \\text{ inside the tube})\\\\\n",
    "\\text{if }0< (a_n \\text{ or } \\hat{a}_n)< C, & |y_n - t_n|=\\epsilon &(\\mathbf{x}_n \\text{ on the boundary of the tube})\\\\\n",
    "\\text{if }(a_n \\text{ or } \\hat{a}_n) = C, & |y_n - t_n|>\\epsilon &(\\mathbf{x}_n \\text{ outside the tube})\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "where we have defined $y_n = y(\\mathbf{x}_n) = \\mathbf{w}^T\\phi(\\mathbf{x}_n) + b$. <font color='green'>The data points that lie on the boundary of the tube are callded *support vectors*. </font>\n",
    "\n",
    "\n",
    "Hence, for any $\\mathbf{x}_n$ that lies on the boundary of the tube, the following equation holds\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\text{if }0<a_n < C: & t_n - y_n = \\epsilon\\\\\n",
    "\\text{if }0<\\hat{a}_n < C: & y_n - t_n = \\epsilon\\\\\n",
    "\\end{array}$$\n",
    "\n",
    "Thus we can solve $b$ by the following equations\n",
    "\n",
    "$$\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\text{if }0<a_n < C: & \\displaystyle{b = t_n - \\epsilon - \\sum_{m=1}^N(a_m-\\hat{a}_m)k(\\mathbf{x}_n, \\mathbf{x}_m)}\\\\\n",
    "\\text{if }0<\\hat{a}_n < C: & \\displaystyle{b = t_n + \\epsilon - \\sum_{m=1}^N(a_m-\\hat{a}_m)k(\\mathbf{x}_n, \\mathbf{x}_m)}\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "In practice, it is better to average over all such estimates of $b$.\n",
    "</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

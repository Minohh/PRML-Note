{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition\n",
    "\n",
    "In practice, the class-conditional distributions may overlap, in which case exact separation of the training data can lead to poor generalization. We therefore need a way to modify the support vector machine so as to allow some of the training points to be misclassified. We start with modifying the error function\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{Separable distributions:} &\\displaystyle{\\sum_{n=1}^N E_{\\infty}(y_nt_n - 1) + \\lambda \\|\\mathbf{w}\\|^2} \\tag{7.19}\\\\\n",
    "&\\text{Overlapped distributions:} &\\displaystyle{C\\sum_{n=1}^N \\xi_n + \\frac{1}{2}\\|\\mathbf{w}\\|^2\\quad\\qquad} \\tag{7.21}\n",
    "\\end{align*}$$\n",
    "\n",
    "We define the data points that are correctly classified as well as not lie in the margin as lying on the 'correct side' of the margin boundary, and the others as 'wrong side' of the margin boundary. <font color='red'>Then $\\xi_n$, which are called *slack variables*, represent the penalty of the samples that lie on the 'wrong side',</font> and is definded by\n",
    "\n",
    "$$\\xi_n = \\left\\{\\begin{array}{ll}\n",
    "0 &\\text{if } \\mathbf{x}_n \\text{ lie on the correct side}\\\\\n",
    "|t_n - y_n| &\\text{if } \\mathbf{x}_n \\text{ lie on the wrong side}\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "\n",
    "\n",
    "# Mathmatic representation\n",
    "\n",
    "## Problem\n",
    "\n",
    "Now our goal is to minimize the new error function.\n",
    "\n",
    "$$\\underset{\\mathbf{w},b,\\mathbf{\\xi}}{min\\ }C\\sum_{n=1}^N \\xi_n + \\frac{1}{2}\\|\\mathbf{w}\\|^2$$\n",
    "\n",
    "\n",
    "## Constraints\n",
    "\n",
    "From the definition of $\\mathbf{\\xi}$, we can easily find that the constraints are now on the slack variables $\\mathbf{\\xi}$. For further computation, it is necessary to eliminate the absolute operation from the form of these slack variables.\n",
    "\n",
    "$$\\xi_n = \\left\\{\\begin{array}{ll}\n",
    "0 &\\text{if }y_n \\geqslant t_n = 1 & (\\mathbf{x}_n \\text{ lie on the correct side})\\\\\n",
    "0 &\\text{if }y_n \\leqslant t_n = -1 & (\\mathbf{x}_n \\text{ lie on the correct side})\\\\\n",
    "t_n - y_n &\\text{if }y_n \\leqslant t_n = 1 &(\\mathbf{x}_n \\text{ lie on the wrong side})\\\\\n",
    "y_n - t_n &\\text{if }y_n \\geqslant t_n = -1 &(\\mathbf{x}_n \\text{ lie on the wrong side})\\\\\n",
    "\\end{array}\\right.\n",
    "= \\left\\{\\begin{array}{ll}\n",
    "0 \\geqslant 1-t_ny_n &\\text{if }y_n \\geqslant t_n = 1 \\\\\n",
    "0 \\geqslant 1-t_ny_n &\\text{if }y_n \\leqslant t_n = -1  \\\\\n",
    "1 - t_ny_n &\\text{if }y_n \\leqslant t_n = 1  \\\\\n",
    "1 - t_ny_n &\\text{if }y_n \\geqslant t_n = -1 \n",
    "\\end{array}\\right.$$\n",
    "\n",
    "Thus the constrinats on $\\xi_n$ is given by\n",
    "\n",
    "$$\\xi_n\\geqslant 1-t_ny_n \\qquad \\text{and} \\qquad \\xi\\geqslant 0$$\n",
    "\n",
    "# Solution \n",
    "\n",
    "## Introduce Lagrange multiplier\n",
    "\n",
    "According to the Lagrange multiplier theorey, we can solve the unseparable SVM problem by finding the solution of \n",
    "\n",
    "$$\\underset{\\mathbf{a}\\geqslant 0,\\mathbf{\\mu}\\geqslant 0}{\\ max\\ }\\underset{\\mathbf{w},b,\\mathbf{\\xi}}{min\\ }L(\\mathbf{w},b,\\mathbf{\\xi},\\mathbf{a},\\mathbf{\\mu})$$\n",
    "\n",
    "where the Lagrangian function, or say objective function, is given by\n",
    "\n",
    "$$\n",
    "\\left.\\begin{array}{ll}\n",
    "\\text{Problem:} & \\displaystyle{\\underset{\\mathbf{w},b,\\mathbf{\\xi}}{min\\ }\\frac{1}{2}\\|\\mathbf{w}\\|^2  + C\\sum_{n=1}^N \\xi_n} \\\\\n",
    "\\text{Constraint 1:} &t_ny_n - 1 + \\xi_n\\geqslant 0 \\\\\n",
    "\\text{Constraint 2:} &\\xi_n \\geqslant 0\n",
    "\\end{array}\\right\\}\n",
    "\\Rightarrow\n",
    "L(\\mathbf{w},b,\\mathbf{\\xi},\\mathbf{a},\\mathbf{\\mu}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{n=1}^N \\xi_n - \\sum_{n=1}^N a_n\\{t_n y_n-1+\\xi_n\\} - \\sum_{n=1}^N \\mu_n\\xi_n \\tag{7.22}$$\n",
    "\n",
    "Finding the solution of $\\underset{\\mathbf{w},b,\\mathbf{\\xi}}{min\\ }L$ is equivalent to finding the partial derivatives with respect to $\\mathbf{w}$, $b$, $\\mathbf{\\xi}$ equal to zero.\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial\\mathbf{w}} = 0 &\\quad\\Rightarrow\\quad \\mathbf{w} = \\sum_{n=1}^N a_n t_n\\phi(\\mathbf{x}_n) \\tag{7.29}\\\\\n",
    "\\frac{\\partial L}{\\partial b} = 0 &\\quad\\Rightarrow\\quad \\sum_{n=1}^N a_n t_n = 0 \\tag{7.30}\\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_n} = 0 &\\quad\\Rightarrow\\quad a_n = C-\\mu_n \\tag{7.31}\n",
    "\\end{align*}$$\n",
    "\n",
    "where \n",
    "- (7.29) indicates that the weight vector $\\mathbf{w}$ changes along the multiplier $\\mathbf{a}$.\n",
    "- (7.30) is the linear equality constraint that the multipliers shall satisfy.\n",
    "- (7.31) limits the range of the multipliers. The Lagrange multiplier theorey requires $\\mu_n\\geqslant 0$ such that each multiplier in $\\mathbf{a}$ should satisfy $a_n\\leqslant C$.\n",
    "\n",
    "Substitute these conditions into the objective function, we obtain\n",
    "\n",
    "$$\\bbox[#ffe0f0]{L(\\mathbf{w}^\\star,b^\\star,\\mathbf{\\xi}^\\star,\\mathbf{a},\\mathbf{\\mu}) = \\sum_{n=1}^N a_n - \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N a_n a_m t_n t_m k(\\mathbf{x}_n, \\mathbf{x}_m)} \\tag{7.32}$$\n",
    "\n",
    "which is an equation that is only related to the multipliers $\\mathbf{a}$. As a result, our goal turns out to be solving the quadratic problem with respect to $\\mathbf{a}$ subject to the linear equality constraint and the constraints on $\\mathbf{a}$, which is denoted by.\n",
    "\n",
    "$$\\bbox[#e0f0ff]{\\underset{0\\leqslant\\mathbf{a}\\leqslant C}{\\ max\\ } L(\\mathbf{w}^\\star,b^\\star,\\mathbf{\\xi}^\\star,\\mathbf{a},\\mathbf{\\mu}) \\quad s.t.\\ \\sum_{n=1}^N a_n t_n = 0}$$\n",
    "\n",
    "which can be solved using the SMO algorithom.\n",
    "\n",
    "Now comparing the quadratic function here with that in the separable SVM, we notice almost everything is the same, except that the constraint on $\\mathbf{a}$ changes form $\\mathbf{a}\\geqslant 0$ to $0\\leqslant\\mathbf{a}\\leqslant C$. Thus the separable SVM can be viewed as a special case with $C\\to\\infty$.\n",
    "\n",
    "\n",
    "## Solution of $\\mathbf{w}$\n",
    "\n",
    "As we said before, the weight vector $\\mathbf{w}$ changes along the Lagrange multipliers $\\mathbf{a}$, and we just got the value of $\\mathbf{a}$, thus we can obtain the solution of $\\mathbf{w}$ using the equation\n",
    "\n",
    "$$\\mathbf{w}^\\star = \\sum_{n=1}^N a_n^\\star t_n \\phi(\\mathbf{x}_n)$$\n",
    "\n",
    "\n",
    "## Solution of $b$\n",
    "\n",
    "<font color='grey'>*SMO also compute the value of $b$.*</font>\n",
    "\n",
    "<font color='#aaaaaa'>\n",
    "\n",
    "The Lagrange multiplier theorey suggests that the solution of $\\mathbf{a}$ satisfies the KKT condition that takes the form\n",
    "\n",
    "$$\\left.\\begin{array}{ll}\n",
    "a_n\\geqslant 0 & (7.23)\\\\\n",
    "t_n y_n - 1 + \\xi_n \\geqslant 0 & (7.24)\\\\\n",
    "a_n (t_n y_n - 1 + \\xi_n) = 0 & (7.25) \\\\\n",
    "\\mu_n \\geqslant 0 & (7.26)\\\\\n",
    "\\xi_n \\geqslant 0 & (7.27)\\\\\n",
    "\\mu_n\\xi_n = 0 & (7.28)\\\\\n",
    "-----------\\\\\n",
    "a_n = C-\\mu_n\n",
    "\\end{array}\\right\\}\n",
    "\\Rightarrow\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\text{if }a_n = 0, & t_n y_n > 1 &(\\mathbf{x}_n \\text{ on the correct side})\\\\\n",
    "\\text{if }0< a_n < C, & t_n y_n = 1 &(\\mathbf{x}_n \\text{ on the boundary of the margin})\\\\\n",
    "\\text{if }a_n = C, & t_n y_n < 1 &(\\mathbf{x}_n \\text{ on the wrong side})\\\\\n",
    "\\end{array}\\right.$$\n",
    "\n",
    "where we have defined $y_n = y(\\mathbf{x}_n) = \\mathbf{w}^T\\phi(\\mathbf{x}_n) + b$. <font color='green'>The data points that lie on the boundary of the margin are callded *support vectors*. </font>\n",
    "\n",
    "\n",
    "Hence, for any $\\mathbf{x}_n$ that lies on the boundary of the margin, the following equation holds\n",
    "\n",
    "$$t_n y_n = t_n\\left(\\sum_{m\\in \\mathcal{S}}a_m t_m k(\\mathbf{x}_n, \\mathbf{x}_m) + b^\\star\\right) = 1 \\tag{7.17}$$\n",
    "\n",
    "where $\\mathcal{S}$ denotes the set of indices of the support vectors. Although we can solve this equation for $b$ using an arbitrarily chosen support vector $\\mathbf{x}_n$, a numerically more stable solution is obtained by first multiplying through by $t_n$, making use of $t_n^2=1$, and then averaging these equations over all support vectors and solving for $b$ to give\n",
    "\n",
    "$$b^\\star = \\frac{1}{N_{\\mathcal{M}}}\\sum_{n\\in\\mathcal{M}}\\left(t_n - \\sum_{m\\in\\mathcal{S}}a_m t_m k(\\mathbf{x}_n, \\mathbf{x}_m)\\right) \\tag{7.18}$$\n",
    "\n",
    "where $\\mathcal{M}$ denotes the set of indices of data points having $0<a_n<C$, which is the set of indices of the data points that lie on the boundary of the margin.\n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

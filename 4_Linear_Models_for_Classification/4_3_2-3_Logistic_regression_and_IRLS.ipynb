{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "\n",
    "## Review\n",
    "In Section 4.2 we saw that under rather general assumptions (class-conditional distribution is shared-variance Gaussian), the posterior probability of class $C_1$ can be writen as a logistic sigmoid acting on a linear function of the feature vector $\\phi$ so that\n",
    "\n",
    "$$p(C_1|\\phi) = y(\\phi) = \\sigma(\\mathbf{w}^T\\phi) \\tag{4.87}$$ \n",
    "\n",
    "where \n",
    "- $\\phi = \\phi(\\mathbf{x})$, is the transformation that can transform an $M'$-dimensional feature $\\mathbf{x}$ to another $M$-dimensional feature $\\phi(\\mathbf{x})$.\n",
    "- $\\displaystyle{\\sigma(a) = \\frac{1}{1+exp(-a)}}$ is the logistic sigmoid function.\n",
    "\n",
    "\n",
    "## From generative to discriminative\n",
    "Under the assumption of shared-varianced Gaussian class-condisional densities, for an $M$-dimensional feature space $\\phi$, we would have used total of $M(M+5)/2+1$ parameters which consist of \n",
    "- $2M$ parameters for the mean.\n",
    "- $M(M+1)/2$ parameters for the shared covariance matrix.\n",
    "- $1$ class prior $p(C_1)$.\n",
    "\n",
    "Whereas the logistic regression model have only $M$ of the number of parameters. For large values of $M$, there is a clear advantage in working with the logistic regression model directly.\n",
    "\n",
    "## Maximum likelihood\n",
    "We now use maximum likelihood to determine the parameters of the logistic regression model. To do this, we shall make use of the derivateive of the logistic sigmoid function, which can conviniently be expressed in terms of the sigmoid function itself\n",
    "\n",
    "$$\\frac{d\\sigma}{da} = \\sigma(1-\\sigma) \\tag{4.88}$$\n",
    "\n",
    "The likelihood function is given by\n",
    "\n",
    "$$p(\\mathbb{t}|\\mathbf{w}) = \\prod_{n=1}^N y_n^{t_n}\\{1-y_n\\}^{1-t_n} \\tag{4.89}$$\n",
    "\n",
    "where\n",
    "- $\\{\\mathbf{X},\\mathbb{t}\\}$ is the data set.\n",
    "- $\\mathbb{t} = (t_1, \\cdots, t_N)^T$ denotes the class of $N$ input features $\\mathbf{X} = (\\mathbf{x}_1,\\cdots,\\mathbf{x}_N)^T$. Here is a two-class problem, so $t_n\\in\\{0,1\\}$.\n",
    "- $y_n = p(C_1|\\phi_n) = \\sigma(a) = \\sigma(\\mathbf{w}^T\\phi_n) = \\sigma(\\mathbf{w}^T\\phi(\\mathbf{x}_n))$ is the probability of classifying to $C_1$ given the data point $\\mathbf{x}_n$.\n",
    "- $\\phi_n = \\phi(\\mathbf{x}_n)$ denotes the transformation from $\\mathbf{x}_n$ to $\\phi_n$.\n",
    "\n",
    "Our goal is to maximize the likelihood function, which is the same as to minimize the negative logarithm of the likelihood, which gives the cross entropy error function in the form\n",
    "\n",
    "$$E(\\mathbf{w}) = -\\ln p(\\mathbb{t}|\\mathbf{w}) = -\\sum_{n=1}^N\\{t_n\\ln y_n+(1-t_n)\\ln (1-y_n)\\} \\tag{4.90}$$\n",
    "\n",
    "Taking the gradient of the error function with respect to $\\mathbf{w}$, we obtain\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla E(\\mathbf{w}) &= \\frac{d}{d\\mathbf{w}} \\left\\{-\\sum_{n=1}^N\\{t_n\\ln y_n+(1-t_n)\\ln (1-y_n)\\}\\right\\}\\\\\n",
    "&= \\frac{d}{d\\mathbf{w}} \\left\\{ -\\sum_{n=1}^N\\{t_n\\ln \\sigma(a)+(1-t_n)\\ln (1-\\sigma(a))\\} \\right\\}\\qquad a=\\mathbf{w}^T\\phi_n\\\\\n",
    "&=\\frac{da}{d\\mathbf{w}} \\frac{d\\sigma}{da}\\frac{d}{d\\sigma}\\left\\{ -\\sum_{n=1}^N\\{t_n\\ln \\sigma+(1-t_n)\\ln (1-\\sigma)\\} \\right\\}\\\\\n",
    "&=\\frac{da}{d\\mathbf{w}} \\frac{d\\sigma}{da}\\left\\{ -\\sum_{n=1}^N\\left\\{\\frac{t_n}{\\sigma}-\\frac{(1-t_n)}{1-\\sigma}\\right\\} \\right\\}\\\\\n",
    "&=\\frac{da}{d\\mathbf{w}} \\frac{d\\sigma}{da}\\left\\{ -\\sum_{n=1}^N\\left\\{\\frac{t_n-t_n\\sigma-\\sigma+t_n\\sigma}{\\sigma(1-\\sigma)}\\right\\} \\right\\}\\\\\n",
    "&=\\frac{da}{d\\mathbf{w}} \\frac{d\\sigma}{da}\\left\\{ \\sum_{n=1}^N\\left\\{\\frac{\\sigma-t_n}{\\sigma(1-\\sigma)}\\right\\} \\right\\}\\\\\n",
    "&=\\phi_n\\cdot \\sigma(1-\\sigma)\\cdot\\left\\{ \\sum_{n=1}^N\\left\\{\\frac{\\sigma-t_n}{\\sigma(1-\\sigma)}\\right\\} \\right\\}\\qquad make\\ use\\ of\\ (4.88)\\\\\n",
    "&=\\sum_{n=1}^N(\\sigma-t_n)\\phi_n\\\\\n",
    "\\color{red}{\\nabla E(\\mathbf{w})}&=\\color{red}{\\sum_{n=1}^N(y_n-t_n)\\phi_n} \\tag{4.91}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "## Over-fitting\n",
    "\n",
    "If the transformed training data set $\\phi_n$ is linearly separable, the decision boundary that seperate two classes will have the property\n",
    "\n",
    "$$\\mathbf{w}^T\\phi_n \\left\\{\n",
    "\\begin{array}{ll}\n",
    "\\geqslant 0 & if\\ t_n = 1\\\\\n",
    "<0 & if\\ t_n = 0\n",
    "\\end{array}\n",
    "\\right .$$\n",
    "\n",
    "Moreover, from (4.90) we see that the error $E(\\mathbf{w})$ will have the minimum if $y_n=t_n$ for all $n$. And the sigmoid function $y_n=\\sigma(\\mathbf{w}^T\\phi_n)$ will generate the result that is close to $0$ or $1$ for the value $\\mathbf{w}^T\\phi_n$ that have large magnitude. This suggests that the magnitude of the parameters goes to infinity.\n",
    "\n",
    "In this case, everything seems ok in $\\phi$ space. But in feature space, every training piont from each class $k$ is assigned a posterior probability $p(C_k|\\mathbf{x}) = 1$, which leads to infinitely steep decision boundary. And this is the tipical presentation of over-fitting.\n",
    "\n",
    "-------------------\n",
    "\n",
    "\n",
    "# IRLS (Iterative reweighted least squares)\n",
    "\n",
    "<font color='red'>For logistic regression, there is no longer a closed-form solution, due to the nonlinearity of the logistic sigmoid function.</font> However, the error function is concave, and hence has a unique minimum. We can therefore use use a technique based on the *Newton-Raphson* iterative optimization scheme to obtain the approximation of the parameters corresponding to the minimum of the error function.\n",
    "\n",
    "We want to find the parameters that satisfy\n",
    "\n",
    "$$\\nabla E(\\mathbf{w}) = 0$$\n",
    "\n",
    "The Newton-Raphson update takes the form\n",
    "\n",
    "$$\\mathbf{w}^{(new)} = \\mathbf{w}^{(old)}-\\mathbf{H}^{-1}\\nabla E(\\mathbf{w})\\tag{4.92}$$\n",
    "\n",
    "\n",
    "## Newton-Raphson for Linear regression\n",
    "For linear regression model (3.3) with the sum-of-squares error function (3.12),\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla E(\\mathbf{w}) &= \\sum_{n=1}^N(y_n-t_n)\\phi_n = \\sum_{n=1}^N(\\mathbf{w}^T\\phi_n-t_n)\\phi_n = \\Phi^T\\Phi\\mathbf{w}-\\Phi^T\\mathbb{t} \\tag{4.93}\\\\\n",
    "\\mathbf{H} &= \\nabla\\nabla E(\\mathbf{w}) = \\sum_{n=1}^N\\phi_n\\phi_n^T = \\Phi^T\\Phi \\tag{4.94}\\\\\n",
    "\\Phi &=\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "The Newton-Raphson update then takes the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbf{w}^{(new)} &= \\mathbf{w}^{(old)} - (\\Phi^T\\Phi)^{-1}\\{ \\Phi^T\\Phi\\mathbf{w}^{(old)} - \\Phi^T\\mathbb{t} \\}\\\\\n",
    "&=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbb{t} \\tag{4.95}\n",
    "\\end{align*}$$\n",
    "\n",
    "The Newton-Raphson formula gives the exact solution in one step.\n",
    "\n",
    "## Newton-Raphson for Logistic regression\n",
    "\n",
    "From (4.91), we see that the gradient and Hessian of the cross-entropy error function are given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\nabla E(\\mathbf{w}) &= \\sum_{n=1}^N(y_n-t_n)\\phi_n = \\Phi^T(\\mathbb{y}-\\mathbb{t}) \\tag{4.96}\\\\\n",
    "\\mathbf{H} &= \\nabla\\nabla E(\\mathbf{w}) = \\sum_{n=1}^N y_n(1-y_n)\\phi_n\\phi_n^T = \\Phi^T\\mathbf{R}\\Phi \\tag{4.97}\\\\\n",
    "\\mathbf{R} &= \\begin{bmatrix}\n",
    "R_{11} &0 &\\cdots &0\\\\\n",
    "0 &R_{22} &\\cdots &0\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots \\\\\n",
    "0 &0 &\\cdots &R_{NN}\n",
    "\\end{bmatrix} \\qquad R_{nn} = y_n(1-y_n)\\tag{4.98}\n",
    "\\end{align*}$$\n",
    "\n",
    "The Hessian matrix $\\mathbf{H}$ is positive difinite because it has the form $\\Phi^T\\mathbf{R}\\Phi$ and the elements of the diagonal matrix $\\mathbf{R}$ have the property $0<y_n<1$. It follows that the error function is a concave function of $\\mathbf{w}$ and hence has a unique minimum.\n",
    "\n",
    "The Newton-Raphson update formula for the logistic regression model then becomes\n",
    "\n",
    "<font color='red'>$$\\begin{align*}\n",
    "\\mathbf{w}^{(new)} &= \\mathbf{w}^{(old)} - (\\Phi^T\\mathbf{R}\\Phi)^{-1}\\Phi^T(\\mathbb{y}-\\mathbb{t})\\\\\n",
    "&=(\\Phi^T\\mathbf{R}\\Phi)^{-1}\\big\\{\\Phi^T\\mathbf{R}\\Phi\\mathbf{w}^{(old)}-\\Phi^T(\\mathbb{y}-\\mathbb{t})\\big\\}\\\\\n",
    " &= (\\Phi^T\\mathbf{R}\\Phi)^{-1}\\Phi^T\\mathbf{R}\\mathbb{z} \\tag{4.99}\n",
    "\\end{align*}$$</font>\n",
    "\n",
    "where $\\mathbb{z}$ is an $N$-dimensional vector with elements\n",
    "\n",
    "<font color='red'>$$\\mathbb{z} = \\Phi\\mathbf{w}^{(old)}-\\mathbf{R}^{-1}(\\mathbb{y}-\\mathbb{t}) \\tag{4.100}$$</font>\n",
    "\n",
    "<font color='red'>Because the weighting matrix $\\mathbf{R}$ is not constant but depends on the parameter vector $\\mathbf{w}$, we must apply the normal equations iteratively, each time using the new weight vector $\\mathbf{w}$ to compute a revised weighting matrix $\\mathbf{R}$. For this reason, the algorithm is known as *iterative reweighted least squares*, or *IRLS*.</font>\n",
    "\n",
    "## Interpretation of $\\mathbf{R}$ and $\\mathbb{z}$\n",
    "\n",
    "As in the weighted least-squares problem, the elements of the diagonal weighting matrix $\\mathbf{R}$ can be interpreted as variances because the means (the decision boundary) and variance of $t$ in the logistic regression model are given by\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{E}[t] &= \\sigma(\\mathbf{x}) = y \\tag{4.101}\\\\\n",
    "var[t] &= \\mathbb{E}[t^2]-\\mathbb{E}[t]^2 = \\sigma(\\mathbf{x}) - \\sigma(\\mathbf{x})^2=y(1-y) \\tag{4.102}\n",
    "\\end{align*}$$\n",
    "\n",
    "where we have used the property $t^2=t$ for $t\\in\\{0,1\\}$.\n",
    "\n",
    "The quantity $z_n$, which corresponds to the $n^{th}$ element of $\\mathbb{z}$, can be given a simple interpretation as an effective target value in the space of the variable $a = \\mathbf{w}^T\\phi$ obtained by making alocal linear approximation to the logistic sigmoid function around the current operating point $\\mathbf{w}^{(old)}$.\n",
    "\n",
    "$$\\begin{align*}\n",
    "y_n = \\sigma(a_n) &\\Rightarrow a_n = \\sigma^{-}(y_n)\\qquad \\sigma^{-}\\ is\\ the\\ inverse\\ function\\ of\\ \\sigma\\\\\n",
    "a_n(\\mathbf{w}) &\\simeq a_n(\\mathbf{w}^{(old)})+\\left.\\frac{da_n}{dy_n}\\right|_{\\mathbf{w}^{(old)}}(t_n-y_n)\\\\\n",
    "&=\\phi_n^T\\mathbf{w}^{(old)} - \\frac{y_n-t_n}{y_n(1-y_n)}\\\\\n",
    "&=z_n \\tag{4.103}\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterate times 100\n",
      "[0.42348314 4.33480415 1.4266894 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "limX = [-2.0, 2.0]\n",
    "limY = [-2.0, 2.0]\n",
    "\n",
    "N=10\n",
    "S1 = np.array([[0.5, 0.0],\n",
    "               [0.0, 0.7]])\n",
    "S2 = np.array([[0.4, 0.0],\n",
    "               [0.0, 0.7]])\n",
    "M1 = np.array([1, 1])\n",
    "M2 = np.array([-1, -1])\n",
    "\n",
    "Phi = lambda x: x\n",
    "\n",
    "sigmoid = lambda a: 1.0/(1.0 + np.exp(-a))\n",
    "\n",
    "def gen_data(n):\n",
    "    X1 = np.random.multivariate_normal(mean=M1, cov=S1, size=n)\n",
    "    X2 = np.random.multivariate_normal(mean=M2, cov=S2, size=n)\n",
    "    X = np.vstack((X1, X2))\n",
    "    T1 = np.ones(X1.shape[0])\n",
    "    T2 = np.zeros(X2.shape[0])\n",
    "    T = np.hstack((T1, T2))\n",
    "    \n",
    "    # add w0\n",
    "    X = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "    return X, T\n",
    "\n",
    "def plot_points(ax, X, T):\n",
    "    for i in range(len(X)):\n",
    "        if T[i]==1:\n",
    "            ax.scatter(X[i][1], X[i][2], s=50, color='red')\n",
    "        elif T[i]==0:\n",
    "            ax.scatter(X[i][1], X[i][2], s=50, color='blue')\n",
    "    \n",
    "def IRLS(X, T, w, n):\n",
    "    N = len(X)\n",
    "    R = np.identity(N)\n",
    "    Y = np.ones((N, ))\n",
    "    for i in range(n):\n",
    "        for j in range(N):\n",
    "            y = sigmoid(w @ X[j].T)\n",
    "            R[j][j] = y*(1-y)\n",
    "            Y[j] = y\n",
    "        if np.allclose(Y, T):\n",
    "            print(\"Iterate times {}\".format(i))\n",
    "            return w\n",
    "        try:\n",
    "            w = w - np.linalg.inv(X.T @ R @ X) @ X.T @ (Y.T - T.T)\n",
    "        except:\n",
    "            print(R)\n",
    "            print(Y)\n",
    "            raise\n",
    "    print(\"Iterate times {}\".format(n))\n",
    "    return w\n",
    "    \n",
    "def main():\n",
    "    fig = plt.figure(figsize=(6,6), dpi=60)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    ax.set_xlim(limX[0], limX[1])\n",
    "    ax.set_ylim(limY[0], limY[1])\n",
    "    \n",
    "    x = np.linspace(-2, 2, 10)\n",
    "    X, T = gen_data(N)\n",
    "    plot_points(ax, X, T)\n",
    "    \n",
    "    w = np.array([1.0, 1.0, 1.0])\n",
    "    w = IRLS(X, T, w, 100)\n",
    "\n",
    "    print(w)\n",
    "    ax.plot(x, -(w[1] * x + w[0])/w[2])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

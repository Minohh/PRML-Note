{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review\n",
    "\n",
    "In Section 4.2, we have seen that, under shared-covariance Gaussian assumption, the posterior probabilities are given by a softmax transformation of linear functions of the feature variables, so that\n",
    "\n",
    "$$p(C_k|\\phi) = y_k(\\phi) = \\frac{exp(a_k)}{\\sum_j exp(a_j)} \\tag{4.104}$$\n",
    "\n",
    "where\n",
    "- $p(C_k|\\phi)$ denotes the posterior probability of $\\phi$ classifying to $C_k$.\n",
    "- $\\phi$ denotes the transformation of input feature $\\mathbf{x}$.\n",
    "- $y_k$ is another expression of this posterior probability.\n",
    "- $a_k = \\mathbf{w}_k^T\\phi$ is called the 'activations', which is the linear combination of $\\phi$.\n",
    "- $\\mathbf{w}_k$ denotes the weights for class $C_k$.\n",
    "\n",
    "# From generative to discriminative\n",
    "\n",
    "The reason is for simplifing the parameters, which we have discussed in previous section.\n",
    "\n",
    "# Maximum likelihood\n",
    "\n",
    "The likelihood function\n",
    "\n",
    "$$p(\\mathbf{T}|\\mathbf{w}_1,\\cdots,\\mathbf{w}_K) = \\prod_{n=1}^N\\prod_{k=1}^K p(C_k|\\phi_n)^{t_{nk}}=\\prod_{n=1}^N\\prod_{k=1}^K y_{nk}^{t_{nk}} \\tag{4.107}$$\n",
    "\n",
    "where\n",
    "- $\\mathbf{X} = (\\mathbf{x}_1,\\cdots, \\mathbf{x}_N)^T$ is the omitted input features.\n",
    "- $\\mathbf{T} = (\\mathbf{t}_1,\\cdots, \\mathbf{t}_N)^T$ denotes the targets of the input features.\n",
    "- $\\mathbf{t}_n = (0,\\cdots,\\underbrace{1}_{k^{th}},\\cdots,0)$, if $\\mathbf{x}_n$ belongs to $C_k$.\n",
    "- $y_{nk} = y_k(\\phi_n)$.\n",
    "\n",
    "Generally, we use the cross-entropy, which is the negative logarithm of the likelihood function, to be the error function for classification problem.\n",
    "\n",
    "$$E(\\mathbf{w}_1,\\cdots,\\mathbf{w}_K) = -\\ln p(\\mathbf{T}|\\mathbf{w}_1,\\cdots,\\mathbf{w}_K) = -\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\ln y_{nk} \\tag{4.108}$$\n",
    "\n",
    "From Section 4.3.3, we have found that the logistic regression doesnot have closed form and we should use IRLS to solve the minumun of the error function, which require us to evaluate the gradient and Hessian of this error funtion.\n",
    "\n",
    "The gradient\n",
    "\n",
    "$$\\nabla E(\\mathbf{w}_1, \\cdots, \\mathbf{w}_K) = \\begin{bmatrix}\\frac{dE}{d\\mathbf{w}_1}\\\\ \\frac{dE}{d\\mathbf{w}_2}\\\\ \\vdots\\\\ \\frac{dE}{d\\mathbf{w}_K}\\end{bmatrix}$$\n",
    "\n",
    "where each element $\\frac{dE}{d\\mathbf{w}_j}$ is a vector with $M$ elements.\n",
    "\n",
    "$$\\begin{align*} \\frac{dE}{d\\mathbf{w}_j} &=\\frac{d}{d\\mathbf{w}_j}\\left\\{ -\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\ln y_{nk}\\right\\}\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{d\\ln y_{nk}}{d\\mathbf{w}_j}\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{d\\ln y_{nk}}{dy_{nk}}\\frac{dy_{nk}}{da_{nj}}\\frac{da_{nj}}{d\\mathbf{w}_j}\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{1}{y_{nk}}\\cdot\\frac{d\\left\\{exp(a_{nk})/\\sum_i exp(a_{ni})\\right\\}}{da_{nj}}\\cdot \\phi_n\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{\\phi_n}{y_{nk}}\\cdot\n",
    "\\frac{d\\left\\{exp(a_{nk})/\\sum_i exp(a_{ni})\\right\\}}{d exp(a_{nj})}\\frac{d exp(a_{nj})}{da_{nj}} \\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{\\phi_n}{y_{nk}}\\cdot\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\frac{\\sum_i exp(a_{ni})\\cdot 0-exp(a_{nk})\\cdot 1}{\\sum_i exp(a_{ni})\\sum_i exp(a_{ni})} \\cdot exp(a_{nj}) & if\\ j\\neq k\\\\\n",
    "\\frac{\\sum_i exp(a_{ni})\\cdot 1-exp(a_{nk})\\cdot 1}{\\sum_i exp(a_{ni})\\sum_i exp(a_{ni})} \\cdot exp(a_{nj}) & if\\ j= k\\\\\n",
    "\\end{array}\\right. \\qquad  quotient\\ rule\n",
    "\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{\\phi_n}{y_{nk}}\\cdot\n",
    "\\left\\{\\begin{array}{ll}\n",
    "\\frac{-exp(a_{nk})}{\\sum_i exp(a_{ni})}\\frac{exp(a_{nj})}{\\sum_i exp(a_{ni})} & if\\ j\\neq k\\\\\n",
    "\\frac{1- exp(a_{nk})}{\\sum_i exp(a_{ni})}\\frac{exp(a_{nj})}{\\sum_i exp(a_{ni})} & if\\ j= k\\\\\n",
    "\\end{array}\\right.\n",
    "\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{\\phi_n}{y_{nk}}\\cdot\n",
    "\\left\\{\\begin{array}{ll}\n",
    "-y_{nk}y_{nj} & if\\ j\\neq k\\\\\n",
    "y_{nk}(1-y_{nj}) & if\\ j= k\\\\\n",
    "\\end{array}\\right.\n",
    "\\\\\n",
    "&=-\\sum_{n=1}^N\\sum_{k=1}^K t_{nk}\\frac{\\phi_n}{y_{nk}}y_{nk}(I_{kj}-y_{nj})\\qquad I\\ is\\ the\\ indentity\\ matrix\\\\\n",
    "&=-\\sum_{n=1}^N \\sum_{k=1}^K t_{nk}\\phi_n(I_{kj}-y_{nj})\\\\\n",
    "&=-\\sum_{n=1}^N \\left\\{\\sum_{k=1}^K t_{nk}\\phi_n I_{kj}-\\sum_{k=1}^K t_{nk}\\phi_n y_{nj}\\right\\}\\\\\n",
    "&=-\\sum_{n=1}^N t_{nj}\\phi_n -\\phi_n y_{nj} \\qquad I_{kj}=\\left\\{\\begin{array}{ll}1 &if\\ k=j\\\\ 0 &otherwise\\end{array}\\right. \\qquad \\sum_{k=1}^K t_{nk} = 1\\\\\n",
    "&=\\color{red}{\\sum_{n=1}^N(y_{nj}-t_{nj})\\phi_n }\\tag{4.109}\n",
    "\\end{align*}$$\n",
    "\n",
    "The Hessian matrix\n",
    "\n",
    "$$\n",
    "\\mathbf{H} = \\nabla\\nabla E(\\mathbf{w}_1, \\cdots, \\mathbf{w}_K)=\\begin{bmatrix}\n",
    "\\frac{\\partial^2 E}{\\partial \\mathbf{w}_1\\partial\\mathbf{w}_1} &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_1\\partial\\mathbf{w}_2} &\\cdots &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_1\\partial\\mathbf{w}_K}\\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\mathbf{w}_2\\partial\\mathbf{w}_1} &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_2\\partial\\mathbf{w}_2} &\\cdots &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_2\\partial\\mathbf{w}_K}\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\frac{\\partial^2 E}{\\partial \\mathbf{w}_K\\partial\\mathbf{w}_1} &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_K\\partial\\mathbf{w}_2} &\\cdots &\\frac{\\partial^2 E}{\\partial \\mathbf{w}_K\\partial\\mathbf{w}_K}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each element $\\frac{\\partial^2 E}{\\partial \\mathbf{w}_k\\partial\\mathbf{w}_j}$ is a $M\\times M$ block\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial^2 E}{\\partial \\mathbf{w}_k\\partial\\mathbf{w}_j}\n",
    "&=\\frac{\\partial}{\\partial \\mathbf{w}_k}\\frac{\\partial E}{\\partial \\mathbf{w}_j}\\\\\n",
    "&=\\frac{\\partial}{\\partial \\mathbf{w}_k}\\sum_{n=1}^N(y_{nj}-t_{nj})\\phi_n\\\\\n",
    "&=\\sum_{n=1}^N \\frac{\\partial (y_{nj}-t_{nj})\\phi_n}{\\partial \\mathbf{w}_k}\\\\\n",
    "&=\\sum_{n=1}^N \\frac{\\partial y_{nj}\\phi_n}{\\partial \\mathbf{w}_k}\\\\\n",
    "&=\\sum_{n=1}^N \\frac{\\partial y_{nj}}{\\partial a_{nk}} \\cdot \\phi_n \\cdot \\frac{\\partial a_{nk}}{\\partial \\mathbf{w}_k}\\\\\n",
    "&=\\color{red}{\\sum_{n=1}^N y_{nk}(I_{kj}-y_{nj})\\phi_n\\phi_n^T} \\tag{4.110}\n",
    "\\end{align*}$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

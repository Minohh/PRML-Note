{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory\n",
    "\n",
    "## Information\n",
    "\n",
    "The amount of information can be viewed as the 'degree of surprise' on learning the value of $x$. If we are told that a highly improbable event has just occurred, we will have received more information than if we were told that some very likely event has just occurred, and if we knew that the event was certain to happen we would receive no information. So the measure of information content will depend on the probability distribution $p(x)$.\n",
    "\n",
    "- We use the form $h(\\cdot)$ to express information.\n",
    "- For two unrelated events $x$ and $y$, the information gain from observing both of these events should be $h(x,y)=h(x)+h(y)$.\n",
    "- For two unrelated events $x$ and $y$, the probability is $p(x,y)=p(x)p(y)$. But smaller probability, more information.\n",
    "\n",
    "From these relationships, it is easily shown that $h(x)$ must be given by the logarithm of $p(x)$ and so we have\n",
    "$$h(x)=-log_2p(x)$$\n",
    "\n",
    "## Entropy\n",
    "\n",
    "Entropy indicates the whole information of a system. We can say the entropy of the discrete random variable $x$ is \n",
    "$$H[x]=-\\sum_{x}p(x)log_2p(x)$$\n",
    "which is the sum of the imformation times their probabilities.  \n",
    "\n",
    "## Shortest coding length\n",
    "\n",
    "<font color='red'>If we want to transmit the information, what we firstly need to do is to encode the information.</font> The encoder shall assign different strings to the information, and the strings have to be able to separate by the decoder. For this purpose,it's optimal for us to encode the low probable information with higher bits, whereas high probable information with lower bits.\n",
    "This relationship between entropy and shortest coding length is a general one. The noiseless coding theorem (Shannon, 1948) states that the entropy is a lower bound on the number of bits needed to transmit the state of a random variable.\n",
    "\n",
    "Generally, while we are talking about the entropy, we will use the nature to be the base of logarithm instead of 2.\n",
    "\n",
    "## Differential Entropy\n",
    "For continuous random variables, the entropy is given by\n",
    "$$H[x] = -\\int p(\\mathbf{x})\\ln p(\\mathbf{x})d\\mathbf{x}$$\n",
    "If the distribution of this random variable $x$ is Gaussian, we obtain the entropy\n",
    "$$H[x]=\\frac{1}{2}\\{1+\\ln(2\\pi\\sigma^2)\\}$$\n",
    "\n",
    "In continuous case, the entropy can be infinite while $\\sigma^2\\to \\infty$, and can even be negative.  \n",
    "In discrete case, if a random variable has $M$ different possible values, the distribution that each value has the same probabilities will leads to highest entropy ($p(x_1)=p(x_2)=\\cdots=p(x_M)=\\frac{1}{M}$).  \n",
    "\n",
    "## Conditional Entropy\n",
    "Conditional entropy of $\\mathbf{y}$ given $\\mathbf{x}$, which is also called the average additional information needed to specify $y$.\n",
    "$$H[\\mathbf{y}|\\mathbf{x}]=-\\iint p(\\mathbf{y},\\mathbf{x})\\ln p(\\mathbf{y}|\\mathbf{x})d\\mathbf{y}d\\mathbf{x}$$\n",
    "We have the joint probability $p(\\mathbf{x},\\mathbf{y})$, and the value of $x$ is already known. If we get further to know the probability of $\\mathbf{y}$ given $\\mathbf{x}$, then we can achive the average additional information.  \n",
    "It is easily seen, the information of $\\mathbf{x}$ plus the information of $\\mathbf{y}$ given $\\mathbf{x}$ brings us the information of $\\mathbf{x}$ and $\\mathbf{y}$.\n",
    "$$H[\\mathbf{x},\\mathbf{y}]=H[\\mathbf{y}|\\mathbf{x}]+H[\\mathbf{x}]$$\n",
    "\n",
    "## Relative entropy and mutual information\n",
    "\n",
    "### Relative entropy (KL divergence)\n",
    "Consider some unknown distribution $p(\\mathbf{x})$, and suppose that we have modelled this using an approximating distribution $q(\\mathbf{x})$. If we use $q(\\mathbf{x})$ to construct a coding scheme for the purpose of transmitting values of $\\mathbf{x}$ to a receiver, then the average *additional* amount of information required to specify the value of $\\mathbf{x}$ is given by\n",
    "$$\\begin{align*}KL( p\\parallel q)\n",
    "&=-\\int p(\\mathbf{x})\\ln q(\\mathbf{x})d\\mathbf{x} - \\left(-\\int p(\\mathbf{x})\\ln p(\\mathbf{x})d\\mathbf{x} \\right)\\\\\n",
    "&=-\\int p(\\mathbf{x})\\ln\\left\\{ \\frac{q(\\mathbf{x})}{p(\\mathbf{x})} \\right\\}d\\mathbf{x} \\end{align*}$$\n",
    "This is known as the *relative entropy or Kullback-Leibler divergence, or KL divergence*.  \n",
    "When $q(\\mathbf{x})=p(\\mathbf{x})$, from the discussion above, we have the lest first term. Consequently receive the least KL convergence which is equal to 0.  \n",
    "$$KL(p\\parallel q) \\geqslant 0$$\n",
    "\n",
    "### Likelihood function\n",
    "Suppose that we are in the practical case. There is a finite training set $\\mathbf{x}=\\{\\mathbf{x}_1,\\mathbf{x}_2,\\cdots,\\mathbf{x}_N\\}$. Our goal is to estimate the unknown distribution $p(\\mathbf{x})$ from these data. We can try to approximate this distribution using some parametric distribution $q(\\mathbf{x}|\\mathbf{\\theta})$, governed by a set of adjustable parameters $\\mathbf{\\theta}$, for example a multivariate Gaussian. One way to get $q(\\mathbf{x}|\\mathbf{\\theta})$ close to the real distribtuion $p(\\mathbf{x})$ is to minimize the KL divergence between $p(\\mathbf{x})$ and $q(\\mathbf{x}|\\mathbf{\\theta})$ with respect to $\\mathbf{\\theta}$. \n",
    "$$\\mathbf{\\theta} = \\arg \\underset{\\mathbf{\\theta}}{min} KL(p\\parallel q)$$\n",
    "The KL divergence is given by\n",
    "$$\\begin{align*}KL( p\\parallel q)\n",
    "&=-\\int p(\\mathbf{x})\\ln q(\\mathbf{x}|\\mathbf{\\theta})d\\mathbf{x} - \\left(-\\int p(\\mathbf{x})\\ln p(\\mathbf{x})d\\mathbf{x} \\right)\\\\\n",
    "&\\approx \\sum_{n-1}^N\\{-lnq(\\mathbf{x}_n|\\mathbf{\\theta})+\\ln p(\\mathbf{x}_n)\\} \\qquad \\mathbb{E}[f]\\approx \\frac{1}{N}\\sum_{n=1}^N f(x_n)\\\\\n",
    "&=negative\\ log\\ likelihood + fixed\\ term\n",
    "\\end{align*}$$\n",
    "Thus we see that minimizing this KL divergence is equivalent to maximizing the likelihood function with respect to parameter $\\mathbf{\\theta}$.\n",
    "\n",
    "### Mutual information\n",
    "We can also use the KL divergence to distinguish whether variables $\\mathbf{x}$ and $\\mathbf{y}$ are independent or not. \n",
    "$$\\begin{align*}\n",
    "I[\\mathbf{x},\\mathbf{y}] &\\equiv KL(p(\\mathbf{x},\\mathbf{y})\\parallel p(\\mathbf{x})p(\\mathbf{y})) \\\\\n",
    "&=-\\iint p(\\mathbf{x},\\mathbf{y})\\ln \\left(\\frac{p(\\mathbf{x})p(\\mathbf{y})}{p(\\mathbf{x},\\mathbf{y})}\\right)d\\mathbf{x}d\\mathbf{y}\\\\\n",
    "&=- \\iint p(\\mathbf{x},\\mathbf{y})\\ln p(\\mathbf{x})d\\mathbf{x}d\\mathbf{y}\n",
    " - \\iint p(\\mathbf{x},\\mathbf{y})\\ln p(\\mathbf{y})d\\mathbf{x}d\\mathbf{y}\n",
    " + \\iint p(\\mathbf{x},\\mathbf{y})\\ln p(\\mathbf{x},\\mathbf{y})d\\mathbf{x}d\\mathbf{y}\\\\\n",
    "&=H[\\mathbf{x}]+H[\\mathbf{y}]-H[\\mathbf{x},\\mathbf{y}]\n",
    "\\end{align*}$$\n",
    "which is called the *mutual information* between $\\mathbf{x}$ and $\\mathbf{y}$. When $p(\\mathbf{x},\\mathbf{y})=p(\\mathbf{x})p(\\mathbf{y})$, the mutual information is 0.  \n",
    "From the view of information theory, \n",
    "$$I[\\mathbf{x},\\mathbf{y}]=H[\\mathbf{x}]+H[\\mathbf{y}]-H[\\mathbf{x},\\mathbf{y}]=H[\\mathbf{x}]-H[\\mathbf{x}|\\mathbf{y}]=H[\\mathbf{y}]-H[\\mathbf{y}|\\mathbf{x}]$$\n",
    "which can be expressed as the reduction in the uncertainty about $\\mathbf{x}$ by virtue of being told the value of $\\mathbf{y}$ (or vice versa). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

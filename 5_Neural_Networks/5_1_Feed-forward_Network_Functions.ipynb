{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive basis functions\n",
    "In the discussion of linear regression and logistic regression, the model are based on linear combinations of **fixed** nonlinear basis fucntions $\\phi(\\mathbf{x})$ and take the form\n",
    "\n",
    "$$y(\\mathbf{x},\\mathbf{w}) = f\\left(\\sum_{j=1}^M w_j\\phi_j(\\mathbf{x})\\right) \\tag{5.1}$$\n",
    "\n",
    "Our goal is to extend this model by making the basis functions $\\phi_j(\\mathbf{x})$ depend on parameters and then to allow these parameters to be adjusted, along with the coefficients $\\{w_j\\}$, during training. There are, of course, many ways to construct parametric nonlinear basis functions. <font color='red'>Neural networks use basis functions that follow the same form as (5.1),</font> so that each basis function is itself a nonlinear function of a linear combination of the inputs, where the coefficients in the linear combination are adaptive parameters.\n",
    "\n",
    "The model therefore can be partitioned into two layers.\n",
    "\n",
    "#### First layer\n",
    "\n",
    "$$z_j = \\phi_j(\\mathbf{x}) = h\\left(a_j\\right),\\qquad let\\ a_j=\\sum_{i=0}^D w_{ji}^{(1)}x_i,\\quad x_{0} = 1 \\tag{5.2,5.3}$$\n",
    "\n",
    "where\n",
    "- $\\phi_j(\\mathbf{x})$ denotes the $j^{th}$ basis function.\n",
    "- $\\mathbf{x}$ is a $D$-dimensional input vector.\n",
    "- $\\mathbf{z}$ is the $M$-dimensional output vector of the first layer. Each element in this vector is called *hidden units*.\n",
    "- $a_j$ are known as *activations*, which is the linear combinations of the input vector.\n",
    "- $w_{ji}$ denote the weight of the $i^{th}$ unit of the input vector for combining the $j^{th}$ unit of the output vector in the first layer.\n",
    "- $h(\\cdot)$ is the *activation function*, and is generally chosen to be sigmoidal functions suchas the logistic sigmoid or the 'tanh' function.\n",
    "\n",
    "\n",
    "#### Second layer\n",
    "\n",
    "$$y_k = f\\left(a_k\\right),\\qquad let\\ a_k=\\sum_{j=0}^M w_{kj}^{(2)}z_j, \\quad z_0 = 1 \\tag{5.4,5.5}$$\n",
    "\n",
    "where\n",
    "\n",
    "- $\\mathbf{z}$ is the $M$-dimensional input vector of the second layer.\n",
    "- $\\mathbf{y}$ is the $K$-dimensional output vector of the second layer.\n",
    "- $a_k$ are the *activations* of the second layer.\n",
    "- $w_{kj}$ denote the weight of the $j^{th}$ unit of the input vector for combining the $k^{th}$ unit of the output vector in the second layer.\n",
    "- $f(\\cdot)$ is the *activation function* of the second layer. If this is a 2-classes classification model, then $f$ is a logistic sigmoid function. If this is a multiclass classification model, then $f$ is a softmax function.\n",
    "\n",
    "# Multilayer perceptron\n",
    "\n",
    "We can combine these two layers to give the overall network function that takes the form\n",
    "\n",
    "$$y_k(\\mathbf{x},\\mathbf{w}) = \\sigma\\left(\\sum_{j=0}^M w_{kj}^{(2)} h\\left(\\sum_{i=0}^D w_{ji}^{(1)}x_i\\right)\\right) \\tag{5.9}$$\n",
    "\n",
    "where the set of all weight and bias parameters have been grouped together into a vector $\\mathbf{w}$.\n",
    "\n",
    "We can see that the neural network model comprises two stages of processing, each of which resembles the preceptron model of Section 4.1.7, and for this reson the neural network is also known as the *multilayer perceptron*, or MLP. <font color='red'>A key difference compared to the perceptron, however, is that the neural network uses continous sigmoidal nonlinerarities in the hidden units, whereas the perceptron uses step-function nonlinearities.</font> This means that the neural network function is differentiable with respect to the network parameters, and this property will play a central role in network training.\n",
    "\n",
    "\n",
    "# Neural network forms\n",
    "\n",
    "## Additional layers\n",
    "\n",
    "We use the number of layers of adaptive weights to represent the number of layers of neural network.\n",
    "\n",
    "The neural networks we discussed above has only two layers. However, we can easily add layers by consisting of a weighted linear combination of each output followed by an element-wise transformation using a nonlinear activation function.\n",
    "\n",
    "## Skip-layer\n",
    "\n",
    "For instance, in a two-layer network these skip-layer would go directly from inputs to outputs.\n",
    "\n",
    "## Sparse network\n",
    "\n",
    "A sparse network is the case that not all possible connections within a layer being present.\n",
    "\n",
    "## General feed-forward architecture\n",
    "\n",
    "A feed-forward network must satisfy the condition of having no closed directed cycles, which ensure that the outputs are deterministic functions of the inputs.\n",
    "\n",
    "\n",
    "# Weight-space symmetries\n",
    "\n",
    "Consider a two-layer neural network with $M$ hidden units having 'tanh' activation functions and full connectiveity in both layers. If we change the sign of all of the weights and the bias feeding into a particular hidden unit, then, for a given input pattern, the sign of the activation of the hidden unit will be reversed, because $tanh(-a) = -tanh(a)$. That is to say if we change the signs of all the weights that connet to the hidden unit, it is still equivalent to the original scheme. Thus, for a single hidden unit, there are two different weight vector groups give the same output. \n",
    "\n",
    "$$\\color{red}{\\left .\n",
    "\\begin{array}{ll}\n",
    "w_{0m}^{(1)}\\\\\n",
    "w_{1m}^{(1)}\\\\\n",
    "\\vdots\\\\\n",
    "w_{Dm}^{(1)}\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "\\xrightarrow[]{\\mathbf{y} = tanh(\\mathbf{w}^T\\mathbf{x})}\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "w_{m1}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "w_{mD'}^{(2)}\n",
    "\\end{array}\n",
    "\\right .}\n",
    "\\overset{equivalent}{\\Leftrightarrow}\n",
    "\\color{blue}{\\left .\n",
    "\\begin{array}{ll}\n",
    "-w_{0m}^{(1)}\\\\\n",
    "-w_{1m}^{(1)}\\\\\n",
    "\\vdots\\\\\n",
    "-w_{Dm}^{(1)}\n",
    "\\end{array}\n",
    "\\right\\}\n",
    "\\xrightarrow[]{-\\mathbf{y} = tanh(-\\mathbf{w}^T\\mathbf{x})}\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "-w_{m1}^{(2)}\\\\\n",
    "\\vdots\\\\\n",
    "-w_{mD'}^{(2)}\n",
    "\\end{array}\n",
    "\\right .}\n",
    "$$\n",
    "\n",
    "And there are $M$ hidden layers, thus any given weight vector will be one of a set $2^M$ equivalent weight vectors. Moreover, for the reason that the order of the $M$ hidden unit is not determined, there are $M!$ different orderings of the hidden units. The network will therefore have overall weight-space symmetry factor of $M!2^M$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

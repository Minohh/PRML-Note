{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to reduce the effective complexity of a network with a large number of weights is to constrain weights within certain group to be equal (CNN). However, this approach is only applicable to particular problems. <font color='red'> Here we consider a form of *soft weight sharing* in which the hard constraint of equal weights is replaced by a form of regularization in which groups of weights are encouraged to have similar values.</font>\n",
    "\n",
    "Weight decay can be interpreted as a soft weight sharing algorithm that encourage weights to be $0$. In the perspective of probability, these weights are determined by a zero-mean isotropic Gaussian distribution.  \n",
    "\n",
    "$$p(\\mathbf{w}) = \\prod_i p(w_i) = \\prod_i\\mathcal{N}(w_i|0, \\sigma^2) $$\n",
    "\n",
    "In a more genaral case, we can assume that these weights are determined by the mixture of multiple isotropic Gaussian distributions, which is equivalent to the division of weights into groups.\n",
    "\n",
    "$$p(\\mathbf{w}) = \\prod_i p(w_i) = \\prod_i\\left(\\sum_{j=1}^M \\pi_j \\mathcal{N}(w_i|\\mu_j, \\sigma_j^2)\\right) \\tag{5.136,5.137} $$\n",
    "\n",
    "where the term inside the brace is the mixture gaussian density given in (2.193), and $\\pi_j$ are the mixing coeffieients. The same as weight decay, taking the negative logarithm then leads to a regularization of the form.\n",
    "\n",
    "$$\\Omega(\\mathbf{w}) = -\\sum_i\\ln\\left(\\sum_{j=1}^M\\pi_j\\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right) \\tag{5.138}$$\n",
    "\n",
    "The total error function is then given by\n",
    "\n",
    "$$\\tilde{E}(\\mathbf{w}) = E(\\mathbf{w})+\\lambda\\Omega(\\mathbf{w}) \\tag{5.139}$$\n",
    "\n",
    "------------------\n",
    "\n",
    "# Derivative with respect to weight\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\tilde{E}}{\\partial w_i} \n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\frac{\\partial\\left[ -\\sum_i\\ln\\left(\\sum_j \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)\\right]}{\\partial w_i}\\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\frac{\\partial\\left[ -\\ln\\left(\\sum_j \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)\\right]}{\\partial w_i}\\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\frac{\\partial\\left[ -\\ln\\left(\\sum_j \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)\\right]}{\\partial\\left(\\sum_j \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)}\n",
    "\\sum_j \\left\\{\\frac{\\partial \\left(\\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)}{\\partial\\left(-(w_i-\\mu_j)^2/(2\\sigma_j^2)\\right)}\n",
    "\\frac{\\partial\\left(-(w_i-\\mu_j)^2/(2\\sigma_j^2)\\right)}{\\partial w_i}\\right\\}\\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\frac{-1}{\\left(\\sum_j \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\right)}\n",
    "\\sum_j \\left\\{\\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\frac{-(w_i-\\mu_j)}{\\sigma_j^2}\\right\\}\\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\sum_j\\frac{1}{\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}\n",
    " \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\frac{(w_i-\\mu_j)}{\\sigma_j^2}\\\\\n",
    "&= \\frac{\\partial E}{\\partial w_i} + \\lambda\\sum_j\\gamma_j(w_i)\\frac{w_i-\\mu_j}{\\sigma_j^2}\\qquad \\text{where }\\gamma_j(w)=\\frac{\\pi_j\\mathcal{N}(w|\\mu_j,\\sigma_j^2)}{\\sum_k\\pi_k\\mathcal{N}(w|\\mu_k,\\sigma_k^2)} \\tag{5.140,5.141}\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\gamma_j(w)$ , is from (2.192), denotes the posterior probability that $w$ belongs to the $j^{th}$ Gaussian distribution in the mixture distributions.\n",
    "\n",
    "Network training tends to find the stationary point in which $\\frac{\\partial E}{\\partial w_i}=0$. The effect of the regularization term is therefore to pull each weight towards the center of the $j^{th}$ Gaussian, with a force proportional to the posterior probability of that Gaussian for the given weight.\n",
    "\n",
    "\n",
    "------------\n",
    "# Derivative with respect to mean\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\tilde{E}}{\\partial \\mu_j}\n",
    "&= \\frac{\\partial E}{\\partial \\mu_j} + \\lambda\\frac{\\partial\\left[ -\\sum_i\\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial \\mu_j}\\\\\n",
    "&= 0 -\\lambda\\sum_i\\frac{\\partial\\left[\\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial \\mu_j}\\\\\n",
    "&= - \\lambda\\sum_i\\frac{\\partial\\left[ \\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}\n",
    "\\frac{\\partial \\sum_k \\left(\\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}{\\partial\\left(-(w_i-\\mu_j)^2/(2\\sigma_j^2)\\right)}\n",
    "\\frac{\\partial\\left(-(w_i-\\mu_j)^2/(2\\sigma_j^2)\\right)}{\\partial \\mu_j}\\\\\n",
    "&= - \\lambda\\sum_i\\frac{1}{\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}\n",
    " \\pi_j \\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)\\frac{(w_i-\\mu_j)}{\\sigma_j^2}\\\\\n",
    "&= \\lambda\\sum_i\\gamma_j(w_i)\\frac{\\mu_j-w_i}{\\sigma_j^2} \\tag{5.142}\n",
    "\\end{align*}$$\n",
    "\n",
    "$\\mu_j$ is pushed towards an average of the weight values, with a force proportional to the posterior probability of that Gaussian for the given weight.\n",
    "\n",
    "\n",
    "-----------\n",
    "# Derivative with respect to variance\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\tilde{E}}{\\partial \\sigma_j}\n",
    "&= \\frac{\\partial E}{\\partial \\sigma_j} + \\lambda\\frac{\\partial\\left[ -\\sum_i\\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial \\sigma_j}\\\\\n",
    "&= \\lambda\\sum_j\\gamma_j(w_i)\\left(\\frac{1}{\\sigma_j}-\\frac{\\partial\\left(-(w_i-\\mu_j)^2/(2\\sigma_j^2)\\right)}{\\partial \\sigma_j}\\right)\\\\\n",
    "&= \\lambda\\sum_j\\gamma_j(w_i)\\left(\\frac{1}{\\sigma_j}-\\frac{(w_i-\\mu_j)^2}{\\sigma_j^3}\\right) \\tag{5.143}\n",
    "\\end{align*}$$\n",
    "\n",
    "which drives $\\sigma_j$ towards the weighted average of the squared deviations of the weights around the corresponding $\\mu_j$, where the weighting coefficients are again given by the posterior probability that each weight is generated by the $j^{th}$ Gaussian component.\n",
    "\n",
    "In pratice, we need to keep the variance non-negative. This can be done by introducing a set of auxiliary variables $\\eta_j$\n",
    "\n",
    "$$\\sigma_j^2 = exp(\\eta_j) \\tag{5.144}$$\n",
    "\n",
    "\n",
    "\n",
    "----------------\n",
    "# Derivative with respect to coefficient\n",
    "\n",
    "For the derivatives with respect to the mixing coefficients $\\pi_j$, we need to take accound of the constraints\n",
    "\n",
    "$$\\sum_j \\pi_j = 1,\\qquad 0\\leqslant \\pi_i\\leqslant 1 \\tag{5.145}$$\n",
    "\n",
    "which can be done by expressing the mixing coefficnets in terms of a set of auxiliary variables $\\eta_j$ using the softmax function given by\n",
    "\n",
    "$$\\pi_j = \\frac{exp(\\eta_j)}{\\sum_k exp(\\eta_k)} \\tag{5.146}$$\n",
    "\n",
    "The derivatives of the regularized error function with respect to $\\eta_j$ then takes the form\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial \\tilde{E}}{\\partial \\eta_j}\n",
    "&= \\frac{\\partial E}{\\partial \\eta_j} + \\lambda\\frac{\\partial\\left[ -\\sum_i\\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial \\eta_j}\\\\\n",
    "&= \\lambda\\sum_i\\frac{\\partial\\left[ -\\ln\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\right]}{\\partial\\left(\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}\n",
    "\\frac{\\partial \\sum_k \\left(\\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)}{\\partial \\eta_j}\\\\\n",
    "&= - \\lambda\\sum_i\\frac{1}{\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)}\n",
    "\\sum_k\\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\frac{\\partial\\pi_k}{\\partial \\eta_j}\\\\\n",
    "&= - \\lambda\\sum_i\\frac{1}{\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)}\n",
    "\\sum_k\\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\n",
    "\\left\\{\\begin{array}{ll}\n",
    "(-\\pi_k\\pi_j) &k\\neq j\\\\\n",
    "\\pi_j(1-\\pi_j) &k=j\n",
    "\\end{array}\\right.\\\\\n",
    "&= - \\lambda\\sum_i\\frac{1}{\\sum_k \\pi_k \\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)}\n",
    "\\left(\\pi_j\\mathcal{N}(w_i|\\mu_j,\\sigma_j^2)-\\sum_k \\pi_k\\pi_j\\mathcal{N}(w_i|\\mu_k,\\sigma_k^2)\\right)\\\\\n",
    "&=\\sum_i\\{\\pi_j-\\gamma_j(w_i)\\} \\tag{5.147}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "We see that $\\pi_j$ is therefore driven towards the average posterior probability for the $j^{th}$ Gaussian component.\n",
    "\n",
    "\n",
    "---------------------\n",
    "\n",
    "# The learning process\n",
    "\n",
    "The learning process is simple. \n",
    "\n",
    "1. Pick initial values for these parameters ($w_i, \\pi_j, \\mu_j, \\sigma_j$).\n",
    "2. Compute the derivatives through the equations above.\n",
    "3. Use these derivatives to refresh the parameters.\n",
    "4. Execute the step 2 and 3 iteratively until the stationary point is founds\n",
    "\n",
    "Following this procedure, the parameters $w_i, \\pi_j, \\mu_j, \\sigma_j$ can be determined adaptively in the learning process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

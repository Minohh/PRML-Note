{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Basis Function Models\n",
    "\n",
    "## Function Models\n",
    "$$\\begin{align*}y(\\mathbf{x},\\mathbf{w})\n",
    "&=w_0+\\sum_{j=1}^{M-1}w_j\\phi_j(\\mathbf{x}) \\tag{3.2}\\\\\n",
    "&=\\sum_{j=0}^{M-1}w_j\\phi_j(\\mathbf{x})\\qquad let\\ \\phi_0(\\mathbf{x})=1 \\\\\n",
    "&=\\mathbf{w}^T\\phi(\\mathbf{x}) \\tag{3.3}\n",
    "\\end{align*}$$\n",
    "where\n",
    "- $\\mathbf{x}=(x_1,\\cdots,x_{M-1})^T$, denote the input of this model.\n",
    "- $\\mathbf{w}=(w_0,\\cdots,w_{M-1})^T$, denote the parameters of this model.\n",
    "- $\\phi = (\\phi_0,\\cdots,\\phi_{M-1})^T$, are known as *basis functions*.\n",
    "\n",
    "## Basis Functions\n",
    "1. $\\phi(\\mathbf{x})=\\mathbf{x}$. This is the most common and simple basis function. Then the function model takes the form\n",
    "$$y(\\mathbf{x},\\mathbf{w})=w_0+w_1x_1+\\cdots+w_{M-1}x_{M-1} \\tag{3.1}$$\n",
    "which is often simply known as *linear regression*.\n",
    "2. $\\phi_j(x)=x^j$. For a single input variable $x$, the basis functions take the powers of $x$. Then the function model takes the form\n",
    "$$y(x,\\mathbf{w})=w_0+w_1x+w_2x^2+\\cdots+w_{M-1}x^{M-1}$$\n",
    "3. $\\displaystyle{\\phi_j(x)=exp\\left\\{-\\frac{(x-\\mu_j)^2}{2s^2}\\right\\} }$. This is usually referred to as 'Gaussian' basis function.\n",
    "4. $\\displaystyle{\\phi_j(x)=\\sigma\\left(\\frac{x-\\mu_j}{s}\\right)\\quad where\\ \\sigma(a)=\\frac{1}{1+exp(-a)}}$. This is the sigmoidal bisis function. Equivalently, we can use the 'tanh' function because it is related to the logistic sigmoid by $tanh(a)=2\\sigma(a)-1$, which can be easily achived by modifying the parameters $w_j$.\n",
    "5. Fourier basis.\n",
    "6. Wavelet basis.\n",
    "\n",
    "Most of the discussion in this chapter is independent of the particular choice of basis function set. Indeed, much of our discussion will be equally applicable to the situation in which the vector $\\phi(\\mathbf{x})$ of basis functions is simply the identity $\\phi(\\mathbf{x})=\\mathbf{x}$. Furthermore, in order to keep the notation simple, we shall focus on the case of a single targe variable $t$.\n",
    "\n",
    "\n",
    "# Maximun Likelihood\n",
    "\n",
    "## Least Squares\n",
    "\n",
    "Assume that the target variable $t$ is given by a deterministic function $y(\\mathbf{x},\\mathbf{w})$ with addictive Gaussian noise so that\n",
    "$$t = y(\\mathbf{x},\\mathbf{w})+\\epsilon \\tag{3.7}$$\n",
    "where $\\epsilon$ is a zero mean Gaussian random variable with precision (inverse variance) $\\beta$. Thus we can write\n",
    "$$p(t|\\mathbf{x}, \\mathbf{w},\\beta)=\\mathcal{N}(t|y(\\mathbf{x},\\mathbf{w}), \\beta^{-1}) \\tag{3.8}$$\n",
    "Now consider a data set of inputs $\\mathbf{X}=\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_N\\}$ with corresponding target values $\\mathbb{t}=\\{t_1,\\cdots,t_N\\}$. With the assumption that these data points are drawn independently from the distribution (3.8), we obtain the following expression for the likelihood function, which is a function of the adjustable parameters $\\mathbf{w}$ and $\\beta$, in the form\n",
    "$$p(\\mathbb{t}|\\mathbf{X}, \\mathbf{w},\\beta)=\\prod_{n=1}^N\\mathcal{N}(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\beta^{-1})\\tag{3.10}$$\n",
    "<font color='red'>*Note that in supervised learning problems such as regression and classification, we are not seeking to model the distribution of the input variables. Thus $\\mathbf{x}$ will always appear in the set of conditioning variables, and so from now on we will drop the explicit $\\mathbf{x}$ from the expressions such as $p(\\mathbb{t}|\\mathbf{x},\\mathbf{w},\\beta)$ in order to keep the notation uncluttered.*</font>  \n",
    "Logarithm likelihood function\n",
    "$$\\begin{align*}\\ln p(\\mathbb{t}|\\mathbf{w},\\beta)\n",
    "&=\\sum_{n=1}^N\\ln\\mathcal{N}(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\beta^{-1})\\\\\n",
    "&=\\frac{N}{2}\\ln\\beta-\\frac{N}{2}\\ln(2\\pi)-\\beta E_D(\\mathbf{w})\\qquad  where\\ E_D(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2 \\tag{3.11,3.12}\n",
    "\\end{align*}$$\n",
    "Maximizing the log likelihood function is equivlant to minimizing the error function. Thus the gradient of the log likelihood function with respect to $\\mathbf{w}$ takes the form\n",
    "$$\\begin{align*}-\\nabla\\ln p(\\mathbb{t}|\\mathbf{w},\\beta)\n",
    "&=\\nabla E_D(\\mathbf{w})\\\\\n",
    "&=\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}\\phi(\\mathbf{x}_n)^T \\tag{3.13}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "### The parameters $\\mathbf{w}$\n",
    "Maximize the likelihood function is equivalent to setting this gradient to zero, which takes the form\n",
    "$$\\begin{align*}0\n",
    "&=\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}\\phi(\\mathbf{x}_n)^T\\\\\n",
    "&=\\sum_{n=1}^N t_n\\phi(\\mathbf{x}_n)^T-\\sum_{n=1}^N\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\phi(\\mathbf{x}_n)^T\\\\\n",
    "\\Rightarrow \\mathbf{w}^T\\sum_{n=1}^N\\phi(\\mathbf{x}_n)\\phi(\\mathbf{x}_n)^T&=\\sum_{n=1}^N t_n\\phi(\\mathbf{x}_n)^T\\\\\n",
    "\\Rightarrow \\mathbf{w}^T \\sum_{n=1}^N\\begin{bmatrix}\\phi_0(\\mathbf{x}_n)\\\\ \\vdots \\\\ \\phi_{M-1}(\\mathbf{x}_n) \\end{bmatrix}\n",
    "\\begin{bmatrix}\\phi_0(\\mathbf{x}_n) &\\cdots & \\phi_{M-1}(\\mathbf{x}_n) \\end{bmatrix}\n",
    "&=\\sum_{n=1}^N t_n\\begin{bmatrix}\\phi_0(\\mathbf{x}_n) &\\cdots & \\phi_{M-1}(\\mathbf{x}_n) \\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}^T \\sum_{n=1}^N\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\phi_0(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) & \\cdots &\\phi_0(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n)\\\\\n",
    "\\phi_1(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\phi_1(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) & \\cdots &\\phi_1(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n)\\\\\n",
    "\\vdots &\\cdots &\\ddots &\\vdots \\\\\n",
    "\\phi_{M-1}(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\phi_{M-1}(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) &\\cdots &\\phi_{M-1}(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n) \n",
    "\\end{bmatrix}\n",
    "&=\\begin{bmatrix}t_1 &\\cdots &t_N\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}^T \n",
    "\\begin{bmatrix}\n",
    "\\sum_{n=1}^N \\phi_0(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\sum_{n=1}^N \\phi_0(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) & \\cdots &\\sum_{n=1}^N\\phi_0(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n)\\\\\n",
    "\\sum_{n=1}^N \\phi_1(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\sum_{n=1}^N \\phi_1(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) & \\cdots &\\sum_{n=1}^N \\phi_1(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n)\\\\\n",
    "\\vdots &\\cdots &\\ddots &\\vdots \\\\\n",
    "\\sum_{n=1}^N \\phi_{M-1}(\\mathbf{x}_n)\\phi_0(\\mathbf{x}_n) &\\sum_{n=1}^N \\phi_{M-1}(\\mathbf{x}_n)\\phi_1(\\mathbf{x}_n) &\\cdots &\\sum_{n=1}^N \\phi_{M-1}(\\mathbf{x}_n)\\phi_{M-1}(\\mathbf{x}_n)\n",
    "\\end{bmatrix}\n",
    "&=\\mathbb{t}^T\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}^T\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_0(\\mathbf{x}_2) &\\cdots &\\phi_0(\\mathbf{x}_N)\\\\\n",
    "\\phi_1(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_1(\\mathbf{x}_N)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_{M-1}(\\mathbf{x}_1) &\\phi_{M-1}(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\n",
    "&=\\mathbb{t}^T\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}^T\\Phi^T\\Phi =\\mathbb{t}^T\\Phi\\qquad let\\ \n",
    "\\Phi&=\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}^T&=\\mathbb{t}^T\\Phi(\\Phi^{T}\\Phi)^{-1}\\\\\n",
    "\\Rightarrow \\mathbf{w}_{ML} &= ((\\Phi^{T}\\Phi)^{-1})^{T}\\Phi^T\\mathbb{t}\\\\\n",
    "&= ((\\Phi^{T}\\Phi)^{T})^{-1}\\Phi^T\\mathbb{t}\\\\\n",
    "&=(\\Phi^{T}\\Phi)^{-1}\\Phi^T\\mathbb{t}\\\\\n",
    "&=\\Phi^{\\dagger }\\mathbb{t} \\qquad let\\ \\Phi^{\\dagger }\\equiv (\\Phi^{T}\\Phi)^{-1}\\Phi^T\n",
    "\\end{align*}$$\n",
    "\n",
    "where $\\Phi^{\\dagger }$ is known as the *Moore-Penrose pseudo-inverse* of the matrix $\\Phi$. And if $M=N$, then $\\Phi^{\\dagger }=(\\Phi^{T}\\Phi)^{-1}\\Phi^T=\\Phi^{-1}(\\Phi^{T})^{-1}\\Phi^T=\\Phi^{-1}$.\n",
    "\n",
    "\n",
    "### The parameter $w_0$\n",
    "The parameter $w_0$ is the bias of our model. We shall analyze this bias parameter individually here. Thus the error function is given by\n",
    "$$E_D(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N\\{t_n-w_0-\\sum_{j=1}^{M-1}w_j\\phi_j(\\mathbf{x}_n)\\}^2 \\tag{3.18}$$\n",
    "Setting the derivative with respect to $w_0$ equal to zero in order to maximize the likelihood function, we obtain\n",
    "$$w_0=\\bar{t}-\\sum_{j=1}^{M-1}w_j\\bar{\\phi_j}\\qquad where\\quad \\bar{t}=\\frac{1}{N}\\sum_{n=1}^Nt_n,\\bar{\\phi_j}=\\frac{1}{N}\\sum_{n=1}^N\\phi_j(\\mathbf{x}_n)\\tag{3.19,3.20}$$\n",
    "Thus the bias $w_0$ compensates for the difference between the avarage (over the training set) of the target values and the weighted sum of the averages of the basis function values.\n",
    "\n",
    "### The parameter $\\beta$\n",
    "Maximize the log likelihood function with respect to the noise precision parameter $\\beta$ gives\n",
    "$$\\frac{1}{\\beta_{ML}}=\\frac{1}{N}\\sum_{n=1}^N\\{t_n-\\mathbf{w}_{ML}^{T}\\phi(\\mathbf{x}_n)\\}^2$$\n",
    "\n",
    "\n",
    "## Geometry of Least Squares\n",
    "Back to the error function \n",
    "$$\\begin{align*}\n",
    "E_D(\\mathbf{w})&=\\frac{1}{2}\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2 \\tag{3.12}\\\\\n",
    "\\Rightarrow 2E_D(\\mathbf{w})&=\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2 \\\\\n",
    "&=\\left\\| \n",
    "\\begin{bmatrix}\n",
    "t_1\\\\\n",
    "t_2\\\\\n",
    "\\vdots\\\\\n",
    "t_N\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\mathbf{w}^T\\phi(\\mathbf{x}_1)\\\\\n",
    "\\mathbf{w}^T\\phi(\\mathbf{x}_2)\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{w}^T\\phi(\\mathbf{x}_N)\n",
    "\\end{bmatrix}\n",
    "\\right\\|^2\\\\\n",
    "&=\\left\\| \n",
    "\\begin{bmatrix}\n",
    "t_1\\\\\n",
    "t_2\\\\\n",
    "\\vdots\\\\\n",
    "t_N\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\begin{bmatrix}\n",
    "\\phi_0(\\mathbf{x}_1) &\\phi_1(\\mathbf{x}_1) &\\cdots &\\phi_{M-1}(\\mathbf{x}_1)\\\\\n",
    "\\phi_0(\\mathbf{x}_2) &\\phi_1(\\mathbf{x}_2) &\\cdots &\\phi_{M-1}(\\mathbf{x}_2)\\\\\n",
    "\\vdots\\\\\n",
    "\\phi_0(\\mathbf{x}_N) &\\phi_1(\\mathbf{x}_N) &\\cdots &\\phi_{M-1}(\\mathbf{x}_N)\\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "w_0\\\\\n",
    "w_1\\\\\n",
    "\\vdots\\\\\n",
    "w_{M-1}\n",
    "\\end{bmatrix}\n",
    "\\right\\|^2\\\\\n",
    "&=\\left\\| \n",
    "\\begin{bmatrix}\n",
    "t_1\\\\\n",
    "t_2\\\\\n",
    "\\vdots\\\\\n",
    "t_N\n",
    "\\end{bmatrix}\n",
    "-\n",
    "\\left(\n",
    "w_0\n",
    "\\begin{bmatrix}\\phi_0(\\mathbf{x}_1)\\\\\\phi_0(\\mathbf{x}_2)\\\\ \\vdots\\\\ \\phi_0(\\mathbf{x}_N)\\end{bmatrix}\n",
    "+\n",
    "w_1\n",
    "\\begin{bmatrix}\\phi_1(\\mathbf{x}_1)\\\\\\phi_1(\\mathbf{x}_2)\\\\ \\vdots\\\\ \\phi_1(\\mathbf{x}_N)\\end{bmatrix}\n",
    "+\\cdots\n",
    "+\n",
    "w_{M-1}\n",
    "\\begin{bmatrix}\\phi_{M-1}(\\mathbf{x}_1)\\\\\\phi_{M-1}(\\mathbf{x}_2)\\\\ \\vdots\\\\ \\phi_{M-1}(\\mathbf{x}_N)\\end{bmatrix}\n",
    "\\right)\n",
    "\\right\\|^2\n",
    "\\end{align*}$$\n",
    "This expression is the Euclidean distance between the target vector $\\mathbb{t}=\\begin{bmatrix}t_1\\\\ t_2\\\\ \\vdots \\\\t_N\\end{bmatrix}$ and the linear combination of $M$ basis function vectors $\\mathbb{y}=w_0\\phi_0+\\cdots+w_{M-1}\\phi_{M-1}$, where $\\phi_j=\\begin{bmatrix}\\phi_j(\\mathbf{x}_1)\\\\ \\phi_j(\\mathbf{x}_2)\\\\ \\vdots\\\\ \\phi_j(\\mathbf{x}_N)\\}\\end{bmatrix}$. For minimizing the error function in the perspective of the Geometry, it can be partition into $3$ situations \n",
    "1. If $M < N$, then the linear combination of $\\phi_j$ is a subspace. In order to obtain the nearest distance, $\\mathbb{y}$ has to be the projection of the $\\mathbb{t}$ to the subspace.\n",
    "2. If $M\\geq N$, then $\\mathbb{y}$ can be the same as the vector $\\mathbb{t}$.\n",
    "3. If $\\Phi^T\\Phi$ is a singular martrix, where $\\Phi=(\\phi_0, \\cdots, \\phi_{M-1})$, then the resulting numerical difficulties can be addressed using the technique SVD.\n",
    "\n",
    "\n",
    "## Sequential learning\n",
    "The model parameters update after each data presentation.\n",
    "\n",
    "We can obtain a sequential learning algorithm by applying the technique of *stochastic gradient descent*, also known as *sequential gradient descent*. This approach assumes that the error function comprises a sum over data points $E=\\sum_nE_n$, then after presentation of pattern $n$\n",
    "$$\\mathbf{w}^{(\\tau+1)}=\\mathbf{(\\tau)}-\\eta\\nabla E_n \\tag{3.22}$$\n",
    "- $\\tau$ denotes the iteration number.\n",
    "- $\\eta$ denotes the learning rate.\n",
    "- $\\nabla E_n$ denotes the gradient of the $nth$ observation data with respect to the parameter $\\mathbf{w}$.\n",
    "\n",
    "For the case of the sum-of-squares error function (3.12), this gives\n",
    "$$\\mathbf{w}^{(\\tau+1)}=\\mathbf{w}^{(\\tau)}+\\eta(t_n-\\mathbf{w}^{(\\tau)T}\\phi_n)\\phi_n\\qquad where\\quad \\phi_n=\\phi(\\mathbf{x}_n) \\tag{3.23}$$\n",
    "\n",
    "\n",
    "## Regularized least squares\n",
    "### Quadratic regularizer\n",
    "The regularized error function (least squares) takes the form\n",
    "$$E_D(\\mathbf{w})+\\lambda E_W(\\mathbf{w})=\\frac{1}{2}\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2+\\frac{\\lambda}{2}\\mathbf{w}^T\\mathbf{w}\\tag{3.24,3.27}$$\n",
    "For maximizing the log likelihood function, equivlantly, for minimizing the error function, we obtain\n",
    "$$\\mathbf{w}_{ML}=(\\lambda\\mathbf{I}+\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbb{t} \\tag{3.28}$$\n",
    "\n",
    "### General regularizer\n",
    "A more general regularizer is sometimes used, for which the regularized error function takes the form\n",
    "$$\\frac{1}{2}\\sum_{n=1}^N\\{t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n)\\}^2+\\frac{\\lambda}{2}\\sum_{j=1}^{M}|w_j|^q \\tag{3.29}$$\n",
    "where\n",
    "- $q = 1$ is known as the lasso in the statistics literature.\n",
    "- $q = 2$ corresponds to the quadratic regularizer (3.27) that we always uses.\n",
    "\n",
    "Using the approach called *Lagrange Multipliers*, we can find the optimun parameters $\\mathbf{w}_{ML}$ in the perspective of geometry.  \n",
    "Minimizing (3.29) is equivalent to minimizing $E_D(\\mathbf{w})$ subject to the constraint\n",
    "$$E_W(\\mathbf{w})=\\sum_{j=1}^M|w_j|^q\\leq \\eta \\tag{3.30}$$\n",
    "for and appropriate value of the parameter $\\eta$. This is also equivalent to finding a point $\\mathbf{w}_{ML}$ that lying on $E_W(\\mathbf{w})-\\eta$ to make the $E_D(\\mathbf{w})$ smallest.\n",
    "\n",
    "### Limitation of regularization\n",
    "<font color='red'>Regularization is more likely to drive the parameters to zero, which limits the effective model complexity and avoids over-fitting on traning data. However, the problem of determining the optimal model complexity is then shifted from one of finding the appropriate number of basis functions to one of determining a suitable value of the regularization coefficient $\\lambda$.</font>\n",
    "\n",
    "## Multiple outputs\n",
    "Consider the input is $\\mathbf{x}$ which is the same as we discussed before, but the output is a vector $\\mathbf{t}$ within $K$ elements instead of a single variable $t$. In this case, the function model takes the form\n",
    "$$y(\\mathbf{x},\\mathbf{W})=\\mathbf{W}^T\\phi(\\mathbf{x}) \\tag{3.31}$$\n",
    "and the Gaussian conditional distribution takes the form\n",
    "$$p(\\mathbf{t}|\\mathbf{x},\\mathbf{W},\\beta)=\\mathcal{N}(\\mathbf{t}|\\mathbf{W}^T\\phi(\\mathbf{x}),\\beta^{-1}\\mathbf{I}) \\tag{3.32}$$\n",
    "where\n",
    "- $\\mathbf{x}$ is the same as we discussed before.\n",
    "- $y(\\mathbf{x},\\mathbf{W})$ is a function model for which the output is a vector within $K$ elements.\n",
    "- $\\mathbf{W}$ is an $M\\times K$ matrix of parameters $\\mathbf{W}=\\begin{bmatrix}\\mathbf{w}_1 &\\cdots &\\mathbf{w}_K\\end{bmatrix}=\\begin{bmatrix}w_{01} &\\cdots &w_{0K}\\\\ \\vdots &\\ddots &\\vdots\\\\ w_{(M-1)1} &\\cdots &w_{(M-1)K}\\end{bmatrix}$.\n",
    "- $\\phi(\\mathbf{x})$ is an $M$-dimensional column vector, which takes the same form as we discussed before.\n",
    "- $\\mathbf{t}$ is the target vector.\n",
    "\n",
    "If we have a set of observations for which \n",
    "- the inputs are $\\mathbf{X}=\\begin{bmatrix}\\mathbf{x}_1^T \\\\ \\vdots \\\\ \\mathbf{x}_N^T\\end{bmatrix}$.\n",
    "- the targets are $\\mathbf{T}=\\begin{bmatrix}\\mathbf{t}_1^T \\\\ \\vdots \\\\ \\mathbf{t}_N^T\\end{bmatrix}=\\begin{bmatrix}t_{11} &\\cdots &t_{1K} \\\\ \\vdots &\\ddots &\\vdots \\\\ t_{N1} &\\cdots &t_{NK}\\end{bmatrix}$.\n",
    "\n",
    "The log likelihood function is then given by\n",
    "$$\\begin{align*}\n",
    "\\ln p(\\mathbf{T}|\\mathbf{X},\\mathbf{W},\\beta)\n",
    "&=\\sum_{n=1}^N\\ln \\mathcal{N}(\\mathbf{t}_n|\\mathbf{W}^T\\phi(\\mathbf{x}_n),\\beta^{-1}\\mathbf{I})\\\\\n",
    "&=\\frac{NK}{2}\\ln\\left(\\frac{\\beta}{2\\pi}\\right)-\\frac{\\beta}{2}\\sum_{n=1}^N\\|\\mathbf{t}_n-\\mathbf{W}^T\\phi(\\mathbf{x}_n)\\|^2 \\tag{3.33}\n",
    "\\end{align*}$$\n",
    "Maximizing this function with respect to $\\mathbf{W}$ gives\n",
    "$$\\mathbf{W}_{ML}=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf{T} \\tag{3.34}$$\n",
    "\n",
    "Decouple this expression\n",
    "$$\\begin{align*}\n",
    "\\begin{bmatrix}w_{01} &\\cdots &w_{0K}\\\\ \\vdots &\\ddots &\\vdots\\\\ w_{(M-1)1} &\\cdots &w_{(M-1)K}\\end{bmatrix}\n",
    "&=(\\Phi^T\\Phi)^{-1}\\Phi^T\\begin{bmatrix}t_{11} &\\cdots &t_{1K} \\\\ \\vdots &\\ddots &\\vdots \\\\ t_{N1} &\\cdots &t_{NK}\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\begin{bmatrix}\\mathbf{w}_1 &\\cdots &\\mathbf{w}_K\\end{bmatrix}\n",
    "&=(\\Phi^T\\Phi)^{-1}\\Phi^T \\begin{bmatrix}\\mathbb{t}_1 &\\cdots &\\mathbb{t}_K\\end{bmatrix} \n",
    "\\qquad let\\ \\mathbb{t}_k=\\begin{bmatrix}t_{1k}\\\\ \\vdots\\\\ t_{Nk}\\end{bmatrix}\\\\\n",
    "\\Rightarrow \\mathbf{w}_k&=(\\Phi^T\\Phi)^{-1}\\Phi^T\\mathbb{t}_k\\\\\n",
    "&=\\Phi^{\\dagger}\\mathbb{t}_k \\qquad let\\ \\Phi^{\\dagger}=(\\Phi^T\\Phi)^{-1}\\Phi^T \\tag{3.35}\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

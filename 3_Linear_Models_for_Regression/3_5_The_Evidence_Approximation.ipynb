{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "From (3.57) we have obtained that\n",
    "$$p(t|\\mathbf{x}, \\mathbf{X}, \\mathbb{t}) = \\int p(t|\\mathbf{x}, \\mathbf{w})p(\\mathbf{w}|\\mathbf{X}, \\mathbb{t})d\\mathbf{w}\\tag{3.57}$$\n",
    "<font color='Red'>where the parameter $\\beta$ and $\\alpha$ are known so that we ommitted them in this expression. Now supposed that $\\alpha$ and $\\beta$ are both unknown, and there should be a posterior distribution (conditional distribution) of them given the data set $\\mathbf{X},\\mathbb{t}$, which is denoted by $p(\\alpha,\\beta|\\mathbf{X},\\mathbb{t})$. </font>Then the predictive distribution takes the form\n",
    "$$p(t|\\mathbf{x}, \\mathbf{X},\\mathbb{t}) = \\iiint p(t|\\mathbf{x},\\mathbf{w},\\beta)p(\\mathbf{w}|\\mathbf{X},\\mathbb{t},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbf{X},\\mathbb{t})d\\mathbf{w}d\\alpha d\\beta$$\n",
    "After omitting the new input variable $\\mathbf{x}$ and input variables from the training set denoted by $\\mathbf{X}$ to keep the notation uncluttered, the predictive distribution takes the form\n",
    "$$p(t|\\mathbb{t}) = \\iiint p(t|\\mathbf{w},\\beta)p(\\mathbf{w}|\\mathbb{t},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbb{t})d\\mathbf{w}d\\alpha d\\beta \\tag{3.74}$$\n",
    "\n",
    "If the posterior distribution $p(\\alpha,\\beta|\\mathbb{t})$ is sharply peaked around values $\\hat{\\alpha}$ and $\\beta$\n",
    "$$\\begin{align*}\n",
    "p(t|\\mathbb{t}) &= \\iiint p(t|\\mathbf{w},\\beta)p(\\mathbf{w}|\\mathbb{t},\\alpha,\\beta)p(\\alpha,\\beta|\\mathbb{t})d\\mathbf{w}d\\alpha d\\beta \\\\\n",
    "&\\approx \\int p(t|\\mathbf{w},\\hat{\\beta})p(\\mathbf{w}|\\mathbb{t},\\hat{\\alpha},\\hat{\\beta})p(\\hat{\\alpha},\\hat{\\beta}|\\mathbb{t})\\Delta\\alpha \\Delta\\beta d\\mathbf{w} \\qquad assume\\ p(\\alpha,\\beta|\\mathbb{t})=\\left\\{\\begin{array}{ll}\\approx 1/(\\Delta\\alpha\\Delta\\beta), &\\alpha=\\hat{\\alpha}\\pm\\Delta\\alpha/2,,\\beta=\\hat{\\beta}\\pm\\Delta\\beta/2\\\\ 0, &otherwise \\end{array}\\right.\\\\\n",
    "&= \\int p(t|\\mathbf{w},\\hat{\\beta})p(\\mathbf{w}|\\mathbb{t},\\hat{\\alpha},\\hat{\\beta}) d\\mathbf{w} \\tag{3.75}\n",
    "\\end{align*}$$\n",
    "From this approximation, it's obvious that <font color='Red'>our goal is to evaluate the values of $\\hat{\\alpha}$ and $\\hat{\\beta}$</font>.\n",
    "\n",
    "So, how shall we evaluate $\\hat{\\alpha}$ and $\\hat{\\beta}$ ?\n",
    "\n",
    "We notice that $p(\\alpha,\\beta|\\mathbb{t})$ is a posterior distribution, then from Bayes' theorem, the posterior distribution for $\\alpha$ and $\\beta$ is given by\n",
    "$$p(\\alpha,\\beta|\\mathbb{t})\\propto p(\\mathbb{t}|\\alpha,\\beta)p(\\alpha,\\beta) \\tag{3.76}$$\n",
    "1. If the prior is relatively flat, then the posterior distribution is equal to the nomalized likelihood function $p(\\mathbb{t}|\\alpha,\\beta)$.\n",
    "2. Evaluate the form of the likelihood function $p(\\mathbb{t}|\\alpha,\\beta)$.\n",
    "3. Maximazing the likelihood function to obtain the values of $\\hat{\\alpha}$ and $\\beta$.\n",
    "\n",
    "\n",
    "## Evaluation of the evidence function\n",
    "Generally, we call the likelihood function *evidence function* here.\n",
    "\n",
    "The evidence function $p(\\mathbb{t}|\\alpha,\\beta)$ is obtained by integrating over the weight parameters $\\mathbf{w}$, so that\n",
    "$$p(\\mathbb{t}|\\alpha, \\beta)=\\int p(\\mathbb{t}|\\mathbf{w},\\beta)p(\\mathbf{w}|\\alpha)d\\mathbf{w} \\tag{3.77}$$\n",
    "\n",
    "where the 2 factors of the integrand have been seen by us before.\n",
    "\n",
    "- $p(\\mathbf{w}|\\alpha)$ is the prior distribution of $\\mathbf{w}$ from (3.52).\n",
    "- $p(\\mathbb{t}|\\mathbf{w},\\beta)$ is the likelihood function of $\\mathbf{w}$ from (3.10).\n",
    "\n",
    "Thus the derivation of the evidence function is given by\n",
    "$$\\begin{align*}\n",
    "p(\\mathbb{t}|\\alpha, \\beta) &=\\int p(\\mathbb{t}|\\mathbf{w},\\beta)p(\\mathbf{w}|\\alpha)d\\mathbf{w} \\\\\n",
    "&=\\int \\left(\\prod_{n=1}^N\\mathcal{N}(t_n|\\mathbf{w}^T\\phi(\\mathbf{x}_n),\\beta^{-1})\\right)\\left(\\mathcal{N}(\\mathbf{w}|0, \\alpha^{-1}\\mathbf{I})\\right)d\\mathbf{w}\\\\\n",
    "&=\\int \\left(\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}exp\\left\\{-\\frac{\\beta}{2}\\sum_{n=1}^N(t_n-\\mathbf{w}^T\\phi(\\mathbf{x}_n))^2\\right\\}\\right)\n",
    "\\left(\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}exp\\left\\{-\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right\\}\\right)d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\\int exp\\left\\{ -\\left(\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{w}\\|^2+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\right)\\right\\}d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\\int exp\\{-E(\\mathbf{w})\\}d\\mathbf{w}  \\qquad let\\ E(\\mathbf{w})=\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{w}\\|^2+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\tag{3.79}\\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "where $E(\\mathbf{w})$ is the error function. Next we extract the terms that is irrelative to $\\mathbf{w}$ and place them outside the integral.\n",
    "$$\\begin{align*}\n",
    "E(\\mathbf{w}) &=\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{w}\\|^2+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=\\frac{\\beta}{2}\\|(\\mathbb{t}-\\Phi\\mathbf{m}_N)+(\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w})\\|^2+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\qquad introduce\\ a\\ vector\\ \\mathbf{m}_N\\\\\n",
    "&=\\frac{\\beta}{2}\\left(\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2+\\|\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w}\\|^2+(\\mathbb{t}-\\Phi\\mathbf{m}_N)^T(\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w})+(\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w})^T(\\mathbb{t}-\\Phi\\mathbf{m}_N)\\right)+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=\\frac{\\beta}{2}\\left(\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2+\\|\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w}\\|^2+(\\mathbb{t}^T-\\mathbf{m}_N^T\\Phi^T)(\\Phi\\mathbf{m}_N-\\Phi\\mathbf{w})+(\\mathbf{m}_N^T\\Phi^T-\\mathbf{w}^T\\Phi^T)(\\mathbb{t}-\\Phi\\mathbf{m}_N)\\right)+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=\\frac{\\beta}{2}\\Big(\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2 +\\Big(\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{m}_N+\\mathbf{w}^T\\Phi^T\\Phi\\mathbf{w}- \\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{w} - \\mathbf{w}^T\\Phi^T\\Phi\\mathbf{m}_N\\Big)\\\\\n",
    "&\\qquad+\\Big(\\mathbb{t}^T\\Phi\\mathbf{m}_N-\\mathbb{t}^T\\Phi\\mathbf{w}-\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{m}_N+\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{w}\\Big)+\\Big(\\mathbf{m}_N^T\\Phi^T\\mathbb{t}-\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{m}_N-\\mathbf{w}^T\\Phi^T\\mathbb{t}+\\mathbf{w}^T\\Phi^T\\Phi\\mathbf{m}_N\\Big)\\Big)\\\\\n",
    "&\\qquad+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=\\frac{\\beta}{2}\\Big(\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2 +\\mathbf{w}^T\\Phi^T\\Phi\\mathbf{w}+\\mathbb{t}^T\\Phi\\mathbf{m}_N-\\mathbb{t}^T\\Phi\\mathbf{w}+\\mathbf{m}_N^T\\Phi^T\\mathbb{t}-\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{m}_N-\\mathbf{w}^T\\Phi^T\\mathbb{t}\\Big)+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\\\\n",
    "&=\\frac{\\beta}{2}\\Big(\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2 +\\mathbf{w}^T\\Phi^T\\Phi\\mathbf{w}+\\frac{1}{\\beta}\\mathbf{m}_N^T A \\mathbf{m}_N-\\frac{1}{\\beta}\\mathbf{m}_N^T A\\mathbf{w}+\\frac{1}{\\beta}\\mathbf{m}_N^T A\\mathbf{m}_N-\\mathbf{m}_N^T\\Phi^T\\Phi\\mathbf{m}_N-\\frac{1}{\\beta}\\mathbf{w}^T A\\mathbf{m}_N\\Big)\\\\\n",
    "&\\qquad+\\frac{\\alpha}{2}\\mathbf{w}^T\\mathbf{w}\\qquad \\color{Red}{let\\ \\left\\{\\begin{array}{ll}\\mathbf{m}_N = \\beta A^{-1}\\Phi^T\\mathbb{t} &\\qquad(3.84)\\\\ A = \\alpha\\mathbf{I}+\\beta\\Phi^T\\Phi & \\qquad(3.81)\\end{array}\\right. }\\\\\n",
    "&=\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2+\\mathbf{w}^T\\left(\\frac{\\beta}{2}\\Phi^T\\Phi+\\frac{\\alpha}{2}\\mathbf{I}\\right)\\mathbf{w}+\\mathbf{m}_N^T\\left(\\frac{A}{2}+\\frac{A}{2}-\\frac{\\beta\\Phi^T\\Phi}{2}\\right)\\mathbf{m}_N-\\frac{1}{2}\\mathbf{m}_N^T A\\mathbf{w}-\\frac{1}{2}\\mathbf{w}^T A\\mathbf{m}_N\\\\\n",
    "&=\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2+\\frac{1}{2}\\mathbf{w}^T A\\mathbf{w}+\\frac{1}{2}\\mathbf{m}_N^T\\left(A+\\alpha\\mathbf{I}\\right)\\mathbf{m}_N-\\frac{1}{2}\\mathbf{m}_N^T A\\mathbf{w}-\\frac{1}{2}\\mathbf{w}^T A\\mathbf{m}_N\\\\\n",
    "&=\\underbrace{\\frac{\\beta}{2}\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2+\\frac{\\alpha}{2}\\mathbf{m}_N^T\\mathbf{m}_N}_{E(\\mathbf{m}_N)\\quad(3.82)}\n",
    "+\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_N)^TA(\\mathbf{w}-\\mathbf{m}_N)\\tag{3.80}\n",
    "\\end{align*}$$\n",
    "\n",
    "Note that\n",
    "- $A$ corresponds to the matrix of second derivatives of the error function\n",
    "$$A = \\nabla\\nabla E(\\mathbf{w}) \\tag{3.83}$$\n",
    "and is known as the *Hessian matrix*.\n",
    "- $\\mathbf{m}_N$ is equivalent to (3.53) which is the mean of the posterior distribution of $\\mathbf{w}$ given known $\\alpha$, $\\beta$ and the training set, because $A = S^{-1}$, where $S^{-1}$ is from (3.54). \n",
    "\n",
    "We see $E(\\mathbf{m}_N)$ has nothing to do with $\\mathbf{w}$ so that can be extract from the integrand. Then the evidence function takes the form\n",
    "$$\\begin{align*}\n",
    "p(\\mathbb{t}|\\alpha, \\beta) &=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\\int exp\\{-E(\\mathbf{w})\\}d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}\\int exp\\left\\{-\\left(E(\\mathbf{m}_N)\n",
    "+\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_N)^TA(\\mathbf{w}-\\mathbf{m}_N)\\right)\\right\\}d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}exp\\big\\{-E(\\mathbf{m}_N)\\big\\}\\int exp\\left\\{-\\frac{1}{2}(\\mathbf{w}-\\mathbf{m}_N)^TA(\\mathbf{w}-\\mathbf{m}_N)\\right\\}d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}exp\\big\\{-E(\\mathbf{m}_N)\\big\\}\\int (2\\pi)^{M/2}|A|^{-1/2}\\mathcal{N}(\\mathbf{w}|\\mathbf{m}_N, A^{-1})d\\mathbf{w}\\\\\n",
    "&=\\left(\\frac{\\beta}{2\\pi}\\right)^{N/2}\\left(\\frac{\\alpha}{2\\pi}\\right)^{M/2}exp\\big\\{-E(\\mathbf{m}_N)\\big\\}(2\\pi)^{M/2}|A|^{-1/2} \\tag{3.85}\n",
    "\\end{align*}$$\n",
    "\n",
    "The log evidence function is given by\n",
    "<font color='Red'>$$\\ln p(\\mathbb{t}|\\alpha, \\beta)=\\frac{M}{2}\\ln \\alpha+\\frac{N}{2}\\ln\\beta - E(\\mathbf{m}_N)-\\frac{1}{2}\\ln|A|-\\frac{M}{2}\\ln(2\\pi)\\tag{3.86}$$</font>\n",
    "\n",
    "## Maximizing the evidence function\n",
    "\n",
    "### Find $\\hat{\\alpha}$\n",
    "The stationary points of (3.86) with respect to $\\alpha$ which is denoted by $\\hat{\\alpha}$ should satisfy\n",
    "$$\\begin{align*}\n",
    "0 &= \\frac{d}{d\\alpha}\\ln p(\\mathbb{t}|\\alpha, \\beta) \\\\\n",
    "&=\\frac{M}{2\\alpha}-\\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N-\\frac{1}{2}\\frac{d}{d\\alpha}\\ln |A| \\qquad assume\\ \\mathbf{m}_N\\ is\\ independent\\ of\\ \\alpha\\\\\n",
    "&=\\frac{M}{2\\alpha}-\\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N-\\frac{1}{2}\\frac{d}{d\\alpha}\\ln |\\alpha\\mathbf{I}+\\beta\\Phi^T\\Phi|\\qquad (3.81)\\\\\n",
    "&=\\frac{M}{2\\alpha}-\\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N-\\frac{1}{2}\\frac{d}{d\\alpha}\\ln \\prod_i^M(\\lambda_i+\\alpha)\\qquad diagonalization\\ \\big(\\beta\\Phi^T\\Phi\\big)\\mathbf{u}_i=\\lambda_i\\mathbf{u}_i\\tag{3.87}\\\\\n",
    "&=\\frac{M}{2\\alpha}-\\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N-\\frac{1}{2}\\frac{d}{d\\alpha}\\sum_i^M\\ln(\\lambda_i+\\alpha)\\\\\n",
    "&=\\frac{M}{2\\alpha}-\\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N-\\frac{1}{2}\\sum_i^M\\frac{1}{\\lambda_i+\\alpha} \\tag{3.89}\\\\\n",
    "\\end{align*}$$\n",
    "Multiplying through by $2\\alpha$ and rearranging, we obtain\n",
    "$$\\alpha\\mathbf{m}_N^T\\mathbf{m}_N=M-\\alpha\\sum_i^M\\frac{1}{\\lambda_i+\\alpha}=\\gamma \\tag{3.90}$$\n",
    "Then the quantity $\\gamma$ can be writen\n",
    "$$\\gamma=\\sum_i^M\\frac{\\lambda_i}{\\alpha+\\lambda_i} \\tag{3.91}$$\n",
    "And From (3.90) we see that the value of $\\alpha$ that maximizes the evidence function satisfies\n",
    "<font color='Red'>$$\\alpha=\\frac{\\gamma}{\\mathbf{m}_N^T\\mathbf{m}_N} \\tag{3.92}$$</font>\n",
    "\n",
    "Notice that we have assumed that $\\mathbf{m}_N$ is independent of $\\alpha$ in the evaluation of the stationary points, but actually $\\mathbf{m}_N$ depends on $\\alpha$. So we adopt an iterative procedure to obtain $\\alpha$.\n",
    "1. Make an initial choice for $\\alpha$.\n",
    "2. Diagnolize $\\Phi^T\\Phi$ to obtain the eigenvalues $\\lambda_i/\\beta$.\n",
    "3. Use $\\alpha$ to find $\\mathbf{m}_N$, which is given by (3.81).\n",
    "4. Use $\\lambda_i$ to evaluate $\\gamma$, which is given by (3.91).\n",
    "5. Use (3.92) to re-estimate $\\alpha$.\n",
    "6. Execute the steps 3-5 repeatly until $\\alpha$ convergence.\n",
    "\n",
    "\n",
    "### Find $\\hat{\\beta}$\n",
    "The stationary points of (3.86) with respect to $\\beta$ which is denoted by $\\hat{\\beta}$ should satisfy\n",
    "$$\\begin{align*}\n",
    "0 &= \\frac{d}{d\\beta}\\ln p(\\mathbb{t}|\\alpha, \\beta) \\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\|\\mathbb{t}-\\Phi\\mathbf{m}_N\\|^2-\\frac{1}{2}\\frac{d}{d\\beta}\\ln |A| \\qquad assume\\ \\mathbf{m}_N\\ is\\ independent\\ of\\ \\beta\\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2-\\frac{1}{2}\\frac{d}{d\\beta}\\sum_i^M\\ln(\\lambda_i+\\alpha)\\qquad diagnolization\\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2-\\frac{1}{2}\\frac{d}{d\\beta}\\sum_i^M\\ln(c_i\\beta+\\alpha)\\qquad From\\ (3.87)\\ \\big(\\beta\\Phi^T\\Phi\\big)\\mathbf{u}_i=\\lambda_i\\mathbf{u}_i\\Rightarrow \\lambda_i=c_i\\beta\\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2-\\frac{1}{2}\\sum_i^M\\frac{c_i}{c_i\\beta+\\alpha}\\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2-\\frac{1}{2\\beta}\\sum_i^M\\frac{\\lambda_i}{\\lambda_i+\\alpha}\\\\\n",
    "&=\\frac{N}{2\\beta}-\\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2-\\frac{\\gamma}{2\\beta} \\tag{3.94}\n",
    "\\end{align*}$$\n",
    "Rearranging, we obtain\n",
    "<font color='Red'>$$\\frac{1}{\\beta}=\\frac{1}{N-\\gamma}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2 \\tag{3.95}$$</font>\n",
    "\n",
    "Notice that we have assumed that $\\mathbf{m}_N$ is independent of $\\beta$ in the evaluation of the stationary points, but actually $\\mathbf{m}_N$ depends on $\\beta$. So we adopt an iterative procedure to obtain $\\beta$.\n",
    "1. Make an initial choice for $\\beta$.\n",
    "2. Diagnolize $\\Phi^T\\Phi$ to obtain the eigenvalues $\\lambda_i/\\beta$.\n",
    "3. Use $\\beta$ to find $\\mathbf{m}_N$, which is given by (3.81).\n",
    "4. Use $\\lambda_i$ to evaluate $\\gamma$, which is given by (3.91).\n",
    "5. Use (3.95) to re-estimate $\\beta$.\n",
    "6. Execute the steps 3-5 repeatly until $\\beta$ convergence.\n",
    "\n",
    "If both $\\alpha$ and $\\beta$ are to be determined from the data, then their values can be re-estimated together after each update of $\\gamma$.\n",
    "\n",
    "# Insight\n",
    "## Meaning of $\\gamma$\n",
    "Looking at the form of $\\gamma$\n",
    "<font color='Red'>$$\\gamma=\\sum_i^M\\frac{\\lambda_i}{\\alpha+\\lambda_i} \\tag{3.91}$$</font>\n",
    "where\n",
    "- $M$ is the quantities of the parameters $\\mathbf{w}$.\n",
    "- $\\alpha$ is the precision of the zero-mean isotropic prior distribution of $\\mathbf{w}$.\n",
    "- $\\lambda_i$ is the eigenvalues of $\\beta\\Phi^T\\Phi$, which is the precision matrix of the likelihood distribution from (3.54) with $\\alpha=0$.\n",
    "$$S_{likelihood}^{-1}=\\beta\\Phi^T\\Phi$$\n",
    "Looking back to the conclusion we obtained in (2_3_0), \n",
    "$$\\begin{align*}\n",
    "\\Delta^2 &= (\\mathbf{w}-\\mathbf{w}_{ML})^TS_{likelihood}^{-1}(\\mathbf{w}-\\mathbf{w}_{ML}) \\\\\n",
    "&=(\\mathbf{w}-\\mathbf{w}_{ML})^T U\\Lambda U^T (\\mathbf{w}-\\mathbf{w}_{ML})\\\\\n",
    "\\end{align*}$$\n",
    "where\n",
    "$U$ can be regarded as a rotation matrix, which will rotate the axes from Cartesian coordinate to align with the eigenvectors $\\mathbf{u}_i$. For each direction of $\\mathbf{u}_i$, there is a corresponding eigenvalue $\\lambda_i$, and the eigenvalue $\\lambda_i$ measures the curvature of the likelihood function, and its reciprocal $\\lambda_i^{-1}$ represent the elongtion of the contour of the likelihood function. <font color='Red'>Moreover, from the likelihood distribution, we find that the elongtion of the direction of $\\mathbf{u}_i$ that is the rotated axes represents the concentration of the corresponding parameter $w_i$. The more concentration means that the parameter is more sensitive to the training data such that the parameter can be called *well determimed*. Conversely, the less concentration means the parameter is poorly sensitive to the training data and hence the parameter will be set to a small value by the prior.</font>\n",
    "\n",
    "With the knowledge above, we find that each entry of $\\gamma$ represent the contribution from the $ith$ parameter to the curve-fitting. For directions in which \n",
    "- $\\lambda_i>>\\alpha$, $\\lambda_i/(\\lambda_i+\\alpha)$ will be close to $1$, then the corresponding parameter $w_i$ is well determined.\n",
    "- $\\lambda_i<<\\alpha$, $\\lambda_i/(\\lambda_i+\\alpha)$ will be close to $0$, then the corresponding parameter $w_i$ is poorly determined.\n",
    "\n",
    "<font color='Red'>Thus, $\\gamma$ expresses the effective parameters in the overall $M$ parameters.</font>\n",
    "\n",
    "*For large training set, the values of $\\lambda_i$ become big either, so that $\\lambda_i>>\\alpha$ hold for every $i$. Every parameter will therefore be tightly constrained by the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from numpy import linalg as LA\n",
    "\n",
    "A = [1.5, 0.0]\n",
    "NOISE_SIGMA = 0.2\n",
    "\n",
    "num_x = 100\n",
    "num_y = 100\n",
    "\n",
    "BETA = (1.0/NOISE_SIGMA)**2\n",
    "ALPHA = 10.0\n",
    "\n",
    "PRIOR_MU = [0.0, 0.0]\n",
    "PRIOR_S = ALPHA * np.array([[1.0, 0.0],\\\n",
    "                           [0.0, 1.0]])\n",
    "\n",
    "def Gaussian2D(w0, w1, M, S):\n",
    "    diff0 = w0 - M[0]\n",
    "    diff1 = w1 - M[1]\n",
    "    diff = np.vstack((diff0, diff1))\n",
    "    P = 1.0/(2*np.pi) * 1.0/(np.linalg.det(S)**0.5) * np.exp(-0.5 * np.sum(diff.T @ np.linalg.inv(S) * diff.T, axis=1))\n",
    "    return P\n",
    "\n",
    "def gen_data(n):\n",
    "    if n == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    x = np.random.uniform(-1, 1, n)\n",
    "    noise = np.random.normal(loc=0,scale=NOISE_SIGMA, size=x.shape)\n",
    "    y = A[0] + A[1]*x + noise\n",
    "    return x,y\n",
    "\n",
    "def plot_info(ax, MU, S):\n",
    "    LAMBDA, UM = LA.eig(S)\n",
    "    UM = UM.T\n",
    "    \n",
    "    ax.scatter(MU[0], MU[1], color='red', s=50, marker='o')\n",
    "    ax.text(x=MU[0]-0.2, y=MU[1]-0.2, s=r\"$\\mathbf{w}_{ML}$\", fontsize=14, color='red')\n",
    "    \n",
    "    LenX, LenY = np.array((UM[0]*LAMBDA[0]**(.5),UM[1]*LAMBDA[1]**(.5))).T*2\n",
    "\n",
    "    \n",
    "    x1 = MU[0]-LenX[0]\n",
    "    y1 = MU[1]-LenY[0]\n",
    "    x2 = x1 + LenX[1]\n",
    "    y2 = y1 + LenY[1]\n",
    "    plt.annotate(\n",
    "        '', xy=(x2, y2), xycoords='data',\n",
    "        xytext=(x1, y1), textcoords='data',\n",
    "        arrowprops={'arrowstyle': '<->', 'color':'k'})\n",
    "    ax.text(x=(x1+x2)/2, y=(y1+y2)/2, s=r\"$\\lambda_1^{-1/2}$\", fontsize=18, color='k')\n",
    "\n",
    "    x3 = MU[0]-LenX[1]\n",
    "    y3 = MU[1]-LenY[1]\n",
    "    x4 = x3 + LenX[0]\n",
    "    y4 = y3 + LenY[0]\n",
    "    plt.annotate(\n",
    "        '', xy=(x3, y3), xycoords='data',\n",
    "        xytext=(x4, y4), textcoords='data',\n",
    "        arrowprops={'arrowstyle': '<->', 'color':'k'})\n",
    "    ax.text(x=(x3+x4)/2, y=(y3+y4)/2, s=r\"$\\lambda_0^{-1/2}$\", fontsize=18, color='k')\n",
    "    \n",
    "\n",
    "def plot_likelihood(ax, xs, ys, W0, W1):\n",
    "    WR0 = np.reshape(W0, num_x*num_y)\n",
    "    WR1 = np.reshape(W1, num_x*num_y)\n",
    "    \n",
    "    Phi = np.c_[np.ones((len(xs.T), 1)), xs.T]\n",
    "    M = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ ys.T\n",
    "    S = np.linalg.inv(BETA* Phi.T @ Phi)\n",
    "    \n",
    "    # likelihood\n",
    "    P = Gaussian2D(WR0, WR1, M, S)\n",
    "    LLH = np.reshape(P, (num_x, num_y))\n",
    "    ax.contour(W0, W1, LLH, levels=[0.2], colors=['red'])\n",
    "\n",
    "    ax.set_xlabel(r\"$w_0$\", fontsize=14, x=1)\n",
    "    ax.set_ylabel(r\"$w_1$\", fontsize=14, y=1)\n",
    "    plot_info(ax, M, S)\n",
    "\n",
    "def plot_prior(ax, W0, W1):\n",
    "    WR0 = np.reshape(W0, num_x*num_y)\n",
    "    WR1 = np.reshape(W1, num_x*num_y)\n",
    "    \n",
    "    S = np.linalg.inv(PRIOR_S)\n",
    "    M = PRIOR_MU\n",
    "    \n",
    "    # Prior (first row) / Posterior distribution\n",
    "    P = Gaussian2D(WR0, WR1, M, S)\n",
    "    Posterior = np.reshape(P, (num_x, num_y))\n",
    "    ax.contour(W0, W1, Posterior, levels=[0.2], colors=['green'])\n",
    "    ax.set_xlabel(r\"$w_0$\", fontsize=14)\n",
    "    ax.set_ylabel(r\"$w_1$\", fontsize=14)\n",
    "    \n",
    "def plot_posterior(ax, xs, ys, W0, W1):\n",
    "    WR0 = np.reshape(W0, num_x*num_y)\n",
    "    WR1 = np.reshape(W1, num_x*num_y)\n",
    "    \n",
    "    # Phi Mean and covariance\n",
    "    # y = w0+w1*x -> phi0 = 1, phi1 = x\n",
    "    Phi = np.c_[np.ones((len(xs.T), 1)), xs.T]\n",
    "    S = np.linalg.inv(PRIOR_S + BETA * Phi.T @ Phi)\n",
    "    M = BETA * S @ Phi.T @ ys.T\n",
    "    \n",
    "    # Prior (first row) / Posterior distribution\n",
    "    P = Gaussian2D(WR0, WR1, M, S)\n",
    "    Posterior = np.reshape(P, (num_x, num_y))\n",
    "    ax.contour(W0, W1, Posterior, levels=[0.2], colors=['blue'])\n",
    "    \n",
    "    ax.scatter(M[0], M[1], color='blue', s=50, marker='o')\n",
    "    ax.text(x=M[0]-0.2, y=M[1]-0.2, s=r\"$\\mathbf{w}_{MAP}$\", fontsize=14, color='blue')\n",
    "    \n",
    "def main():\n",
    "    fig = plt.figure(figsize=(6,6), dpi=60)\n",
    "    ax = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['bottom'].set_position('zero')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks([])\n",
    "    ax.yaxis.set_ticks([])\n",
    "    \n",
    "    W0, W1 = np.mgrid[-1:3:num_x*1j, -2:2:num_y*1j]\n",
    "    plot_prior(ax, W0, W1)\n",
    "    \n",
    "    xs, ys = gen_data(2)\n",
    "    plot_likelihood(ax, xs, ys, W0, W1)\n",
    "    \n",
    "    plot_posterior(ax, xs, ys, W0, W1)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About $\\beta$\n",
    "Biased estimate of variance\n",
    "$$\\sigma_{ML}^2=\\frac{1}{N}\\sum_{n=1}^N(x_n-\\mu_{ML})^2 \\tag{3.96}$$\n",
    "Unbiased estimate of variance\n",
    "$$\\sigma_{MAP}^2=\\frac{1}{N-1}\\sum_{n=1}^N(x_n-\\mu_{ML})^2 \\tag{3.97}$$\n",
    "We shall see in Section 10.1.3 that this result can be obtained from a Bayesian treatment in which we marginalize over the unknon mean. <font color='Red'>The factor of $N-1$ in the denominator of the Bayesian result takes account of the fact that one degree of feedom has been used in fitting the mean and removes the bias of maximum likelihood.</font>\n",
    "\n",
    "In the linear regression here, The mean of the target distribution is now given by the function $\\mathbf{w}^T\\phi(\\mathbf{x})$, which contains M parameters. However, not all of these parameters are tuned to the data. The effective number of parameters that are determined by the data is $\\gamma$, with the remaining $M-\\gamma$ parameters set to small values by the prior. This is reflected in the Bayesian result for the variance that has a factor $N-\\gamma$ in the denominator, thereby correcting for the bias of the maximum likelihood result so as to obtain $\\beta$.\n",
    "\n",
    "\n",
    "# Approximation for large training set\n",
    "If we consider the limit $N>>M$, all the parameters $\\mathbf{w}$ will be well determined by the data because $\\Phi^t\\Phi$ involves an implicit sum over all data points and so the eigenvalues $\\lambda_i$ increse with the size of the data set. In this case, $\\gamma = M$, and the <font color='Red'>re-estimation</font> equations for $\\alpha$ and $\\beta$ becomes\n",
    "$$\\begin{align*}\n",
    "\\alpha &= \\frac{M}{2E_W(\\mathbf{m}_N)} \\tag{3.98}\\\\\n",
    "\\beta &= \\frac{N}{2E_D(\\mathbf{m}_N)} \\tag{3.99} \\\\ \n",
    "where\\quad E_W(\\mathbf{m}_N) &= \\frac{1}{2}\\mathbf{m}_N^T\\mathbf{m}_N \\tag{3.25}\\\\\n",
    "E_D(\\mathbf{m}_N) &= \\frac{1}{2}\\sum_{n=1}^N\\big\\{t_n-\\mathbf{m}_N^T\\phi(\\mathbf{x}_n)\\big\\}^2 \\tag{3.26}\n",
    "\\end{align*}$$\n",
    "\n",
    "These results can be used as an easy-to-compute approximation to the full evidence re-estimation formulae, because they do not require evaluation of the eigen value spectrum of the Hessian."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

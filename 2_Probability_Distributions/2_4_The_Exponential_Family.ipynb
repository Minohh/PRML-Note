{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Exponential Family\n",
    "\n",
    "## General form\n",
    "$$p(\\mathbf{x}|\\mathbf{\\eta})=h(\\mathbf{x})g(\\mathbf{\\eta})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\} \\tag{2.194}$$\n",
    "where\n",
    "- $\\mathbf{x}$ may be scalar or vector, and may be discrete or continuous.\n",
    "- $\\mathbf{\\eta}$ are called the natural parameters of the distribution.\n",
    "- $\\mathbf{u}(\\mathbf{x})$ is some function of $\\mathbf{x}$.\n",
    "- $g(\\mathbf{\\eta})$ is the coefficient that ensures the distribution to be normalized and therefore satisfies\n",
    "$$g(\\mathbf{\\eta})\\int h(\\mathbf{x}) exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}d\\mathbf{x}=1 \\tag{2.195}$$\n",
    "\n",
    "Step that changes a distribution to exponential family\n",
    "1. If there is a exponential term in the distribution, go to step 3.\n",
    "2. Transform the distribution from $p(x|\\mu)$ to $exp\\{\\ln(p(x|\\mu))\\}$\n",
    "3. In the exponent, one part is relative to $x$, and the other part is relative to $\\mu$. Comparing with the general form of exponential family, extract the factor $\\eta$ from the part of $x$.\n",
    "4. The part of $\\mu$ will multiply by the coefficient outside the exponent to normalize the distribution and finally be a factor of $g(\\eta)$. Thus, we should use the parameter $\\eta$ to represent $\\mu$.\n",
    "5. Extract the $x$ relative factors $u(x)$ and $h(x)$.\n",
    "6. Extract the $\\mu$ relative factor $g(\\eta)$.  \n",
    "\n",
    "-----------------------\n",
    "## Bernoulli distribution\n",
    "$$p(x|\\mu)=Bern(x|\\mu)=\\mu^x(1-\\mu)^{1-x} \\tag{2.196}$$\n",
    "\n",
    "Transform to the exponential family\n",
    "$$\\begin{align*}\n",
    "p(x|\\mu)&=exp\\{x\\ln \\mu+(1-x)\\ln(1-\\mu)\\}\\\\\n",
    "&=exp\\{x\\ln \\mu+\\ln(1-\\mu)-x\\ln(1-\\mu)\\}\\\\\n",
    "&=(1-\\mu)exp\\{x\\ln \\mu-x\\ln(1-\\mu)\\}\\\\\n",
    "&=(1-\\mu)exp\\left\\{\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)x\\right\\} \\tag{2.197}\n",
    "\\end{align*}$$\n",
    "Comparison with $(2.194)$ allows us to indentify\n",
    "$$\\eta=\\ln\\left(\\frac{\\mu}{1-\\mu}\\right) \\tag{2.198}$$\n",
    "For solving the factor $(1-\\mu)$ outside the exponential, we also need to figure out what does $\\mu$ equal to. \n",
    "$$\\begin{align*}\n",
    "\\eta&=\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)\\\\\n",
    "\\Rightarrow exp(\\eta)&=\\frac{\\mu}{1-\\mu}\\\\\n",
    "\\Rightarrow \\frac{1}{exp(\\eta)}&=\\frac{1-\\mu}{\\mu}=\\frac{1}{\\mu}-1\\\\\n",
    "\\Rightarrow \\frac{1}{\\mu}&=\\frac{1}{exp(\\eta)}+1=\\frac{1+exp(\\eta)}{exp(\\eta)}\\\\\n",
    "\\Rightarrow \\mu&=\\frac{exp(\\eta)}{1+exp(\\eta)}=1-\\frac{1}{1+exp(\\eta)}\\\\\n",
    "\\Rightarrow 1-\\mu &= \\frac{1}{1+exp(\\eta)}\\\\\n",
    "\\Rightarrow p(x|\\eta) &= \\frac{1}{1+exp(\\eta)}exp\\left\\{\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)x\\right\\}\n",
    "\\end{align*}$$\n",
    "Then we can use the *logistic sigmoid* function which is denoted by\n",
    "$$\\sigma(\\eta)=\\frac{1}{1+exp(-\\eta)} \\tag{2.199}$$\n",
    "substitute this to the expression, gives that\n",
    "$$p(x|\\eta)=\\sigma(-\\eta)exp(\\eta x) \\tag{2.200}$$\n",
    "where\n",
    "- $\\eta=\\ln\\left(\\frac{\\mu}{1-\\mu}\\right)$\n",
    "- $u(x)=x$.\n",
    "- $h(x)=1$.\n",
    "- $g(\\eta)=\\sigma(-\\eta)$\n",
    "\n",
    "\n",
    "-----------------\n",
    "## ~~Multinomial distribution~~ (Multiple outcomes Bernoulli distribution)\n",
    "### First solution\n",
    "$$p(\\mathbf{x}|\\mathbf{\\mu})=\\prod_{k=1}^M\\mu_k^{x_k}=exp\\left\\{\\sum_{k=1}^Mx_k\\ln \\mu_k\\right\\}=exp(\\mathbf{\\eta}^T\\mathbf{x})$$\n",
    "where \n",
    "- $\\mathbf{x}=(x_1,\\cdots,x_M)^T$.\n",
    "- $\\mathbf{\\eta}=(\\eta_1,\\cdots,\\eta_M)^T=(\\ln \\mu_1,\\cdots,\\ln\\mu_M)^T$.\n",
    "- $\\mathbf{u}(\\mathbf{x})=\\mathbf{x}$.\n",
    "- $h(\\mathbf{x})=1$.\n",
    "- $g(\\mathbf{\\eta})=1$.\n",
    "\n",
    "\n",
    "### Another solution\n",
    "In some circumstances, it will be convinient to remove the probability of $x_M$ which is denoted by $\\mu_M$. Leaving only $M-1$ parameters to express the distribution.\n",
    "$$\\begin{align*}\n",
    "p(\\mathbf{x}|\\mathbf{\\mu})&=\\prod_{k=1}^M\\mu_k^{x_k}=exp\\left\\{\\sum_{k=1}^Mx_k\\ln \\mu_k\\right\\}\\\\\n",
    "&=exp\\left\\{\\sum_{k=1}^{M-1}x_k\\ln\\mu_k+\\underbrace{\\left(1-\\sum_{k=1}^{M-1}x_k\\right)}_{x_M}\\ln\\underbrace{\\left(1-\\sum_{k=1}^{M-1}\\mu_k\\right)}_{\\mu_M}\\right\\}\\\\\n",
    "&=exp\\left\\{\\sum_{k=1}^{M-1}x_k\\ln\\left(\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\right)+\\ln\\left(1-\\sum_{k=1}^{M-1}\\mu_k\\right)\\right\\} \\tag{2.211}\\\\\n",
    "&=\\left(1-\\sum_{k=1}^{M-1}\\mu_k\\right)exp\\left\\{\\sum_{k=1}^{M-1}x_k\\ln\\left(\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\right)\\right\\}\n",
    "\\end{align*}$$\n",
    "Then we extract the factor $\\mathbf{\\eta}=(\\eta_1,\\cdots,\\eta_{M-1})$, where $\\eta_k=\\ln\\left(\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\right)^T$, from which we solve the $\\mu_k$\n",
    "$$\\begin{align*} exp(\\eta_k)&=\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\\\\n",
    "\\Rightarrow \\sum_{k=1}^{M-1}exp(\\eta_k)&=\\frac{\\sum_{k=1}^{M-1}\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\\\\n",
    "\\Rightarrow 1+\\sum_{k=1}^{M-1}exp(\\eta_k)&=1+\\frac{\\sum_{k=1}^{M-1}\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}=\\frac{1}{1-\\sum_{j=1}^{M-1}\\mu_j}\\\\\n",
    "\\Rightarrow \\frac{1}{1+\\sum_{k=1}^{M-1}exp(\\eta_k)}&=1-\\sum_{j=1}^{M-1}\\mu_j\\\\\n",
    "\\Rightarrow \\frac{exp(\\eta_k)}{1+\\sum_{j=1}^{M-1}exp(\\eta_j)}&=\\left(1-\\sum_{j=1}^{M-1}\\mu_j\\right)\\cdot exp(\\eta_k)\\\\\n",
    "\\Rightarrow \\frac{exp(\\eta_k)}{1+\\sum_{j}exp(\\eta_j)}&=\\mu_k \\tag{2.213}\n",
    "\\end{align*}$$\n",
    "This is called the *softmax* function. Substituting softmax function to $(2.211)$ gives\n",
    "$$p(\\mathbf{x}|\\mathbf{\\eta})=\\left(1+\\sum_{k=1}^{M-1}exp(\\eta_k)\\right)^{-1}exp(\\mathbf{\\eta}^T\\mathbf{x})$$\n",
    "where\n",
    "- $\\mathbf{x}=(x_1,\\cdots,x_{M-1})^T$\n",
    "- $\\mathbf{\\eta}=(\\eta_1,\\cdots,\\eta_{M-1})$, where $\\eta_k=\\ln\\left(\\frac{\\mu_k}{1-\\sum_{j=1}^{M-1}\\mu_j}\\right)^T$.\n",
    "- $\\mathbf{u}(\\mathbf{x})=\\mathbf{x}$.\n",
    "- $h(\\mathbf{x})=1$.\n",
    "- $\\displaystyle{g(\\mathbf{\\eta})=\\left(1+\\sum_{k=1}^{M-1}exp(\\eta_k)\\right)^{-1}}$.\n",
    "\n",
    "-----------------\n",
    "\n",
    "## Gaussian\n",
    "$$\\begin{align*}\n",
    "p(x|\\mu,\\sigma^2)&=\\frac{1}{(2\\pi\\sigma^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\right\\}\\\\\n",
    "&=\\frac{1}{(2\\pi\\sigma^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma^2}x^2+\\frac{\\mu}{\\sigma^2}x-\\frac{1}{\\sigma^2}\\mu^2\\right\\}\n",
    "\\end{align*}$$\n",
    "where\n",
    "- $\\mathbf{\\eta}=\\begin{bmatrix}\\mu/\\sigma^2\\\\ -1/2\\sigma^2\\end{bmatrix}$.\n",
    "- $\\mathbf{u}(x)=\\begin{bmatrix}x\\\\ x^2\\end{bmatrix}$.\n",
    "- $h(x)=(2\\pi)^{1/2}$.\n",
    "- $g(\\mathbf{\\eta})=(-2\\eta_2)^{1/2}exp\\left(\\frac{\\eta_1^2}{4\\eta_2}\\right)$\n",
    "\n",
    "----------------\n",
    "---------------\n",
    "# The Properties of Exponential Family\n",
    "## Maximun likelihood and sufficient statistics\n",
    "\n",
    "### Gradient\n",
    "\n",
    "First order gradient of a function $f(\\mathbf{x})$ is denoted by a vector whose elements are the partial derivative in each dimensionality. And the second order gradient of the function $f(\\mathbf{x})$ is a $N\\times N$ matrix.\n",
    "$$\\nabla f(\\mathbf{x})=\\begin{bmatrix}\n",
    "\\partial f/\\partial x_1\\\\\n",
    "\\partial f/\\partial x_2\\\\\n",
    "\\vdots\\\\\n",
    "\\partial f/\\partial x_N\\\\\n",
    "\\end{bmatrix}\\qquad\n",
    "\\nabla^2 f(\\mathbf{x})=\n",
    "\\nabla\\big(\\nabla f(\\mathbf{x})\\big)^T\n",
    "=\\nabla(\n",
    "\\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} &\\frac{\\partial f}{\\partial x_2} &\\cdots &\\frac{\\partial f}{\\partial x_N} \\end{bmatrix}\n",
    ")\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_1} &\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &\\cdots &\\frac{\\partial^2 f}{\\partial x_1\\partial x_N} \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_2\\partial x_1} &\\frac{\\partial^2 f}{\\partial x_2 \\partial x_2} &\\cdots &\\frac{\\partial^2 f}{\\partial x_2\\partial x_N} \\\\\n",
    "\\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_N\\partial x_1} &\\frac{\\partial^2 f}{\\partial x_N \\partial x_2} &\\cdots &\\frac{\\partial^2 f}{\\partial x_N\\partial x_N} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Taking the gradient of both side of the integration of the exponential distribution with respect to $\\mathbf{\\eta}$, we have\n",
    "$$\\begin{align*}\n",
    "\\nabla \\left( \\int p(\\mathbf{x}|\\mathbf{\\eta})d\\mathbf{x}\\right ) &= \\nabla \\left(g(\\mathbf{\\eta})\\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\} d\\mathbf{x}\\right )\\\\\n",
    "&=\\nabla g(\\mathbf{\\eta}) \\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\} d\\mathbf{x}+g(\\mathbf{\\eta}) \\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}\\mathbf{u}(\\mathbf{x}) d\\mathbf{x}\\qquad (fg)'=f'g+fg' \\tag{2.224}\\\\\n",
    "&=\\nabla g(\\mathbf{\\eta})\\cdot\\frac{1}{g(\\mathbf{\\eta})}+\\mathbb{E}[\\mathbf{u}(\\mathbf{x})]\\\\\n",
    "&=0\\\\\n",
    "\\Rightarrow \\mathbb{E}[\\mathbf{u}(\\mathbf{x})]&=-\\nabla g(\\mathbf{\\eta})\\cdot\\frac{1}{g(\\mathbf{\\eta})} \\tag{2.225}\\\\\n",
    "&=-\\nabla \\ln g(\\mathbf{\\eta}) \\tag{2.226}\n",
    "\\end{align*}$$\n",
    "We can see that the first order moment of $\\mathbf{u}(\\mathbf{x})$ can be expressed in term of the derivatives of $-\\ln g(\\mathbf{\\eta})$. And the following derivation will show that the second order moment can also be expressed in term of the second derivatives of $-\\ln g(\\mathbf{\\eta})$.\n",
    "$$\\begin{align*}\n",
    "-\\nabla^2\\ln g(\\mathbf{\\eta)})&=\\nabla((-\\nabla\\ln g(\\mathbf{\\eta)})^T)\\\\\n",
    "&=\\nabla(\\mathbb{E}[\\mathbf{u}(\\mathbf{x})]^T)\\\\\n",
    "&=\\nabla(\\mathbb{E}[\\mathbf{u}^T(\\mathbf{x})])\\\\\n",
    "&=\\nabla\\left(g(\\mathbf{\\eta})\\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}\\mathbf{u}^T(\\mathbf{x})d\\mathbf{x} \\right )\\\\\n",
    "&=\\nabla g(\\mathbf{\\eta})\\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}\\mathbf{u}^T(\\mathbf{x})d\\mathbf{x} \n",
    "+\\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}\\mathbf{u}(\\mathbf{x})\\mathbf{u}^T(\\mathbf{x})d\\mathbf{x} \\\\\n",
    "&=\\frac{\\nabla g(\\mathbf{\\eta})}{g(\\mathbf{\\eta})}\\cdot g(\\mathbf{\\eta})\\int h(\\mathbf{x})exp\\{\\mathbf{\\eta}^T\\mathbf{u}(\\mathbf{x})\\}\\mathbf{u}^T(\\mathbf{x})d\\mathbf{x} \n",
    "+\\mathbb{E}[\\mathbf{u}(\\mathbf{x})\\mathbf{u}^T(\\mathbf{x})] \\\\\n",
    "&=-\\mathbb{E}[\\mathbf{u}(\\mathbf{x})]\\mathbb{E}[\\mathbf{u}(\\mathbf{x})]+\\mathbb{E}[\\mathbf{u}(\\mathbf{x})\\mathbf{u}^T(\\mathbf{x})]\\\\\n",
    "&=cov[\\mathbf{u}(\\mathbf{x})] \\tag{Exercise 2.58}\n",
    "\\end{align*}$$\n",
    "\n",
    "### Maximun likelihood and sufficient statistics\n",
    "Now consider a set of independent identically distributed data denoted by $\\mathbf{X}=\\{\\mathbf{x}_1,\\cdots,\\mathbf{x}_N\\}$, for which the likelihood function is given by\n",
    "$$p(\\mathbf{X}|\\mathbf{\\mu})=\\left(\\prod_{n=1}^Nh(\\mathbf{x}_n)\\right)g(\\mathbf{\\eta})^Nexp\\left\\{\\mathbf{\\eta}^T\\sum_{n=1}^N\\mathbf{u}(\\mathbf{x}_n)\\right\\}\\tag{2.227}$$\n",
    "Setting the gradient of $\\ln p(\\mathbf{X}|\\mathbf{\\eta})$ with respect to $\\eta$ to zero, we get the following condition to be satisfied bt the maximum likelihood estimator $\\mathbf{\\eta}_{ML}$\n",
    "$$-\\nabla \\ln g(\\mathbf{\\eta}_{ML})=\\frac{1}{N}\\sum_{n=1}^N\\mathbf{u}(\\mathbf{x}_n)\\tag{2.228}$$\n",
    "We see that the solution for te maximum likelihood estimator depends on the data only through $\\sum_n \\mathbf{u}(\\mathbf{x}_{n})$, which is therefore called the *sufficient statistic* of the distribution.\n",
    "\n",
    "------------------\n",
    "\n",
    "## Conjugate priors\n",
    "### Prior of exponential family\n",
    "$$p(\\mathbf{\\eta}|\\chi, v)=f(\\chi, v)g(\\mathbf{\\eta})^v exp\\{v\\mathbf{\\eta}^T\\chi\\}\\tag{2.229}$$\n",
    "where $f(\\chi, v)$ is a normalization coefficient, and $g(\\mathbf{\\eta})$ is the same function as appeares in the exponential family function form $(2.194)$.\n",
    "\n",
    "### Posterior of exponential family\n",
    "$$\\begin{align*}\n",
    "p(\\eta|\\mathbf{X}, \\chi,v) &\\propto p(\\mathbf{X}|\\mathbf{\\eta})p(\\mathbf{\\eta}|\\chi, v)\\\\\n",
    "&=\\left(\\prod_{n=1}^Nh(\\mathbf{x}_n)\\right)g(\\mathbf{\\eta})^Nexp\\left\\{\\mathbf{\\eta}^T\\sum_{n=1}^N\\mathbf{u}(\\mathbf{x}_n)\\right\\}\\cdot\n",
    "f(\\chi, v)g(\\mathbf{\\eta})^v exp\\{v\\mathbf{\\eta}^T\\chi\\}\\\\\n",
    "&\\propto g(\\mathbf{\\eta})^{v+N}exp\\left\\{ \\mathbf{\\eta}^T\\left( \\sum_{n=1}^N \\mathbf{u}(\\mathbf{x}_n)+v\\chi \\right) \\right\\} \\tag{2.230}\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "--------------------\n",
    "--------------------\n",
    "\n",
    "# Noninformative priors\n",
    "\n",
    "In some applications of probabilistic inference, we may have prior knowledge that can be conveniently expressed throught the prior distribution. But in many cases, however, we may have little idea of what form the distribution should take. We may then seek a form of prior distributon, called a *noninformative prior* , which is intended to have as little inluence on the posterior distribution as possible.\n",
    "\n",
    "Here, we shall introduce two kinds of informative priors.\n",
    "\n",
    "### Translation invariance prior\n",
    "Translation invariance means that the density mass is only relative to the length of a section we choose but the position. This is denoted by\n",
    "$$\\int_A^B p(\\mu)d\\mu=\\int_{A-c}^{B-c}p(\\mu)d\\mu=\\int_A^Bp(\\mu-c)d\\mu \\tag{2.234}$$\n",
    "Such expression must hold for all choices of $A$ and $B$, we have\n",
    "$$p(\\mu-c)=p(\\mu) \\tag{2.235}$$\n",
    "\n",
    "#### Gaussian translation invariance prior\n",
    "For a Gaussian ditribution, consider that we want to find a translation invariance prior with respect to $\\mu$. It's known that the prior of a Gaussian $\\mathcal{N}(x|\\mu,\\sigma^2)$ with respect to $\\mu$ is still a Gaussian $\\mathcal{N}(\\mu|\\mu_0, \\sigma_0^2)$. If this prior Gaussian satisfies the condition of translation invariance, which is equivilant to the expression\n",
    "$$\\begin{align*}\n",
    "\\mathcal{N}(\\mu|\\mu_0, \\sigma_0^2) &=\\mathcal{N}(\\mu|(\\mu_0+c), \\sigma_0^2)\\\\\n",
    "\\Rightarrow \\frac{1}{(2\\pi\\sigma_0^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma_0^2}(x-\\mu_0)^2\\right\\}\n",
    "&= \\frac{1}{(2\\pi\\sigma_0^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma_0^2}(x-\\mu_0-c)^2\\right\\}\\\\\n",
    "\\Rightarrow \\frac{1}{(2\\pi\\sigma_0^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma_0^2}(x-\\mu_0)^2\\right\\}\n",
    "&= \\frac{1}{(2\\pi\\sigma_0^2)^{1/2}}exp\\left\\{-\\frac{1}{2\\sigma_0^2}(x-\\mu_0)^2\\right\\}\\cdot exp\\left\\{-\\frac{1}{2\\sigma_0^2}(-c)^2\\right\\}\\\\\n",
    "\\Rightarrow exp\\left\\{-\\frac{1}{2\\sigma_0^2}(-c)^2\\right\\}&=1\\\\\n",
    "\\Rightarrow \\sigma_0^2 & \\to\\infty\\\\\n",
    "\\Rightarrow \\mathcal{N}(\\mu|\\mu_0, \\sigma_0^2) &=0\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "### Scale invariance prior\n",
    "Scale invariance means that the density mass of arbitrary interval, we say $[A,B]$, is the same as the interval of $[A/c, B/c]$ given any $c$. This is denoted by\n",
    "$$\\int_A^B p(\\sigma)d\\sigma=\\int_{A/c}^{B/c}p(\\sigma)d\\sigma=\\int_A^Bp(\\frac{1}{c}\\sigma)\\frac{1}{c}d\\sigma \\tag{2.238}$$\n",
    "Because this must hold for all choices of $A$ and $B$, we have\n",
    "$$\\begin{align*}\n",
    "p(\\sigma)&=p(\\frac{1}{c}\\sigma)\\frac{1}{c}\\tag{2.239}\\\\\n",
    "\\Rightarrow p(\\sigma)&=p(\\hat{\\sigma})\\frac{\\hat{\\sigma}}{\\sigma}\\qquad let\\ \\hat{\\sigma}=\\frac{\\sigma}{c}\\\\\n",
    "\\Rightarrow p(\\sigma)\\sigma &= p(\\hat{\\sigma})\\hat{\\sigma}\\\\\n",
    "\\Rightarrow p(\\sigma)&\\propto \\frac{1}{\\sigma}\n",
    "\\end{align*}$$\n",
    "\n",
    "#### Gaussian scale invariance prior\n",
    "For a Gaussian ditribution, consider that we want to find a scale invariance prior with respect to $\\lambda$. It's known that the prior of a Gaussian $\\mathcal{N}(x|\\mu,\\lambda^{-1})$ with respect to $\\lambda$ is a Gamma distribution $Gam(\\lambda|a_0, b_0)$. If this prior Gaussian satisfies the condition of translation invariance, then the Gamma distribution takes the form\n",
    "$$\\begin{align*}\n",
    "Gam(\\lambda|a_0, b_0) &\\propto \\lambda^{-1} \\\\\n",
    "\\Rightarrow \\frac{1}{\\Gamma(a_0)}b_0^{a_0}\\lambda^{a_0-1}exp(-b_0\\lambda) &\\propto \\lambda^{-1}\\\\\n",
    "\\Rightarrow a_0=b_0&=0\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
